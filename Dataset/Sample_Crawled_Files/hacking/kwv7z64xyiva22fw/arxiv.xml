<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">

<head>
  <title>[1903.11137] Hearing your touch: A new acoustic side channel on smartphones</title>
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/css/arXiv.css?v=20190307" />
  
  <!-- Piwik -->
  <script type="text/javascript">
    var _paq = _paq || [];
    _paq.push(["setDomains", ["*.arxiv.org"]]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
      var u = "//webanalytics.library.cornell.edu/";
      _paq.push(['setTrackerUrl', u + 'piwik.php']);
      _paq.push(['setSiteId', 538]);
      var d = document,
        g = d.createElement('script'),
        s = d.getElementsByTagName('script')[0];
      g.type = 'text/javascript';
      g.async = true;
      g.defer = true;
      g.src = u + 'piwik.js';
      s.parentNode.insertBefore(g, s);
    })();
  </script>
  <!-- End Piwik Code -->
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/zca7yc/b/13/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=7a8da419"></script>
<script type="text/javascript">window.ATL_JQ_PAGE_PROPS =  {
  "triggerFunction": function(showCollectorDialog) {
    //Requires that jQuery is available!
    jQuery("#feedback-button").click(function(e) {
      e.preventDefault();
      showCollectorDialog();
    });
  },
  fieldValues: {
    "components": ["15700"],  // Browse component.
    "versions": ["14132"],  // Release browse-0.1
    "customfield_11401": window.location.href
  }
  };
</script><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" media="screen" type="text/css" href="/bibex/bibex.css?20181010"/>
  <script src="//static.arxiv.org/js/mathjaxToggle.min.js" type="text/javascript"></script>
  <meta name="citation_title" content="Hearing your touch: A new acoustic side channel on smartphones"/>
  <meta name="citation_author" content="Shumailov, Ilia"/>
  <meta name="citation_author" content="Simon, Laurent"/>
  <meta name="citation_author" content="Yan, Jeff"/>
  <meta name="citation_author" content="Anderson, Ross"/>
  <meta name="citation_date" content="2019/03/26"/>
  <meta name="citation_online_date" content="2019/03/26"/>
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/1903.11137"/>
  <meta name="citation_arxiv_id" content="1903.11137"/><meta name="twitter:site" content="@arxiv"/>
    <meta property="twitter:title" content="Hearing your touch: A new acoustic side channel on smartphones"/>
    <meta property="twitter:description" content="We present the first acoustic side-channel attack that recovers what users
type on the virtual keyboard of their touch-screen smartphone or tablet. When a
user taps the screen with a finger, the..."/>
    <meta property="og:site_name" content="arXiv.org"/>
    <meta property="og:title" content="Hearing your touch: A new acoustic side channel on smartphones"/>
    <meta property="og:url" content="https://arxiv.org/abs/1903.11137v1"/>
    <meta property="og:description" content="We present the first acoustic side-channel attack that recovers what users
type on the virtual keyboard of their touch-screen smartphone or tablet. When a
user taps the screen with a finger, the tap generates a sound wave that
propagates on the screen surface and in the air. We found the device&#39;s
microphone(s) can recover this wave and &#34;hear&#34; the finger&#39;s touch, and the
wave&#39;s distortions are characteristic of the tap&#39;s location on the screen.
Hence, by recording audio through the built-in microphone(s), a malicious app
can infer text as the user enters it on their device. We evaluate the
effectiveness of the attack with 45 participants in a real-world environment on
an Android tablet and an Android smartphone. For the tablet, we recover 61% of
200 4-digit PIN-codes within 20 attempts, even if the model is not trained with
the victim&#39;s data. For the smartphone, we recover 9 words of size 7--13 letters
with 50 attempts in a common side-channel attack benchmark. Our results suggest
that it not always sufficient to rely on isolation mechanisms such as TrustZone
to protect user input. We propose and discuss hardware, operating-system and
application-level mechanisms to block this attack more effectively. Mobile
devices may need a richer capability model, a more user-friendly notification
system for sensor usage and a more thorough evaluation of the information
leaked by the underlying hardware."/>
</head>

<body  class="with-cu-identity">
  <noscript><img src="//webanalytics.library.cornell.edu/piwik.php?idsite=538&amp;rec=1" style="border:0;" alt="" /></noscript>
  <div id="cu-identity">
    <div id="cu-logo">
      <a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
    </div>
    <div id="support-ack">
      <a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br/>the Simons Foundation and member institutions.</a>
    </div>
  </div>

  <div id="header" >
    
  <h1><a href="/">arXiv.org</a> &gt; <a href="/list/cs/recent">cs</a> &gt; arXiv:1903.11137</h1>
  <div id="search">
    <form id="search-arxiv" method="get" action="https://arxiv.org/search">

      <div class="wrapper-search-arxiv">
        <input class="keyword-field" type="text" name="query" placeholder="Search or Article ID" />

        <div class="filter-field">
          <select name="searchtype">
            <option value="all">All fields</option>
            <option value="title">Title</option>
            <option value="author">Author(s)</option>
            <option value="abstract">Abstract</option>
            <option value="comments">Comments</option>
            <option value="journal_ref">Journal reference</option>
            <option value="acm_class">ACM classification</option>
            <option value="msc_class">MSC classification</option>
            <option value="report_num">Report number</option>
            <option value="paper_id">arXiv identifier</option>
            <option value="doi">DOI</option>
            <option value="orcid">ORCID</option>
            <option value="author_id">arXiv author ID</option>
            <option value="help">Help pages</option>
            <option value="full_text">Full text</option>
          </select>
        </div>
        <input class="btn-search-arxiv" value="" type="submit">
        <div class="links">(<a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced search</a>)</div>
      </div>
    </form>
  </div>

  </div>

  <div id="content">
    <!--
rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/1903.11137"
        dc:identifier="/abs/1903.11137"
        dc:title="Hearing your touch: A new acoustic side channel on smartphones"
        trackback:ping="/trackback/1903.11137" />
    </rdf:RDF>
-->
<div id="abs">
  <div class="extra-services">
    <div class="full-text">
      <span class="descriptor">Full-text links:</span>
      <h2>Download:</h2>
      <ul>
  <li><a href="/pdf/1903.11137" accesskey="f">PDF</a></li>
  <li><a href="/format/1903.11137">Other formats</a></li></ul>
      <div class="abs-license"><a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="Rights to this article"><img src="https://arxiv.org/icons/licenses/by-nc-sa-4.0.png"/></a></div>
    </div>
    <!--end full-text-->
    <div class="browse">
    <h3>Current browse context:</h3>
  <div class="current">cs.CR</div>

  <div class="prevnext">

  <span class="arrow">
    <a href="/prevnext?site=arxiv.org&amp;id=1903.11137&amp;function=prev&amp;context=cs.CR"
       accesskey="p" title="previous in cs.CR (accesskey p)">&lt;&nbsp;prev</a>
  </span>&nbsp;|&nbsp;

  
  <span class="arrow">
    <a href="/prevnext?site=arxiv.org&amp;id=1903.11137&amp;function=next&amp;context=cs.CR" accesskey="n"
       title="next in cs.CR (accesskey n)">next&nbsp;&gt;</a>
  </span><br/>
  </div><div class="list">
    <a href="/list/cs.CR/new">new</a>&nbsp;|
    <a href="/list/cs.CR/recent">recent</a>&nbsp;|
    <a href="/list/cs.CR/1903">1903</a>
  </div><h3>Change to browse by:</h3>
  <div class="switch">
    
      <a href="/abs/1903.11137?context=cs">cs</a>
      
    <br/>
    
      <span class="subclass"><a href="/abs/1903.11137?context=cs.AI">cs.AI</a></span>
      
    <br/>
    
  </div>
  
    </div>

    <div class="extra-ref-cite">
      <h3>References &amp; Citations</h3>
      <ul>
        
        <li><a href="https://ui.adsabs.harvard.edu/#abs/arXiv:1903.11137">NASA ADS</a></li>
      </ul>
    </div>

    <div class="bookmarks">
  <div class="what-is-this"><h3>Bookmark</h3> (<a href="https://arxiv.org/help/social_bookmarking">what is this?</a>)</div><a href="/ct?url=http%3A%2F%2Fwww.citeulike.org%2Fposturl%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1903.11137&amp;v=17c38719"
     title="Bookmark on CiteULike">
    <img src="//static.arxiv.org/icons/social/citeulike.png"
         alt="CiteULike logo" />
  </a>
  <a href="/ct?url=http%3A%2F%2Fwww.bibsonomy.org%2FBibtexHandler%3FrequTask%3Dupload%26url%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1903.11137%26description%3DHearing+your+touch%3A+A+new+acoustic+side+channel+on+smartphones&amp;v=eec3d4a7"
     title="Bookmark on BibSonomy">
    <img src="//static.arxiv.org/icons/social/bibsonomy.png"
         alt="BibSonomy logo"/>
  </a>
  <a href="/ct?url=https%3A%2F%2Fwww.mendeley.com%2Fimport%2F%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1903.11137&amp;v=2fa9c661"
     title="Bookmark on Mendeley">
    <img src="//static.arxiv.org/icons/social/mendeley.png"
         alt="Mendeley logo"/>
  </a>
  <a href="/ct?url=https%3A%2F%2Freddit.com%2Fsubmit%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1903.11137%26title%3DHearing+your+touch%3A+A+new+acoustic+side+channel+on+smartphones&amp;v=ec895bfc"
     title="Bookmark on Reddit">
    <img src="//static.arxiv.org/icons/social/reddit.png"
         alt="Reddit logo"/>
  </a>
  <a href="/ct?url=http%3A%2F%2Fsciencewise.info%2Fbookmarks%2Fadd%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1903.11137&amp;v=a5bf6599"
     title="Bookmark on ScienceWISE">
    <img src="//static.arxiv.org/icons/social/sciencewise.png"
         alt="ScienceWISE logo"/>
  </a>
</div>
  </div>
  <!--end extra-services-->

  <div class="leftcolumn">
    <div class="subheader">
      <h1>Computer Science > Cryptography and Security</h1>
    </div>
    <h1 class="title mathjax"><span class="descriptor">Title:</span>Hearing your touch: A new acoustic side channel on smartphones</h1>
    <div class="authors"><span class="descriptor">Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shumailov%2C+I">Ilia Shumailov</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Simon%2C+L">Laurent Simon</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yan%2C+J">Jeff Yan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Anderson%2C+R">Ross Anderson</a>
    </div>

    <div class="dateline">(Submitted on 26 Mar 2019)</div>

    
    <blockquote class="abstract mathjax"><span class="descriptor">Abstract:</span>  We present the first acoustic side-channel attack that recovers what users
type on the virtual keyboard of their touch-screen smartphone or tablet. When a
user taps the screen with a finger, the tap generates a sound wave that
propagates on the screen surface and in the air. We found the device&#39;s
microphone(s) can recover this wave and &#34;hear&#34; the finger&#39;s touch, and the
wave&#39;s distortions are characteristic of the tap&#39;s location on the screen.
Hence, by recording audio through the built-in microphone(s), a malicious app
can infer text as the user enters it on their device. We evaluate the
effectiveness of the attack with 45 participants in a real-world environment on
an Android tablet and an Android smartphone. For the tablet, we recover 61% of
200 4-digit PIN-codes within 20 attempts, even if the model is not trained with
the victim&#39;s data. For the smartphone, we recover 9 words of size 7--13 letters
with 50 attempts in a common side-channel attack benchmark. Our results suggest
that it not always sufficient to rely on isolation mechanisms such as TrustZone
to protect user input. We propose and discuss hardware, operating-system and
application-level mechanisms to block this attack more effectively. Mobile
devices may need a richer capability model, a more user-friendly notification
system for sensor usage and a more thorough evaluation of the information
leaked by the underlying hardware.
</blockquote>
    <!--CONTEXT-->
    <div class="metatable">
      <table summary="Additional metadata">
        <tr>
          <td class="tablecell label">Comments:</td>
          <td class="tablecell comments mathjax">Paper built on the MPhil thesis of Ilia Shumailov. 2017</td>
        </tr>
        <tr>
          <td class="tablecell label">Subjects:</td>
          <td class="tablecell subjects"><span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)</td>
        </tr><tr>
          <td class="tablecell label">Cite as:</td>
          <td class="tablecell arxivid"><a href="https://arxiv.org/abs/1903.11137">arXiv:1903.11137</a> [cs.CR]</td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxividv">(or <span class="arxivid">
              <a href="https://arxiv.org/abs/1903.11137v1">arXiv:1903.11137v1</a> [cs.CR]</span> for this version)
          </td>
        </tr>
      </table>
    </div>
    <div class="submission-history">
      <h2>Submission history</h2> From: Ilia Shumailov [<a href="/show-email/fe6e5f21/1903.11137">view email</a>]
      <br/><b>[v1]</b>
Tue, 26 Mar 2019 20:06:26 UTC (9,563 KB)<br/></div>
  </div>
  <!--end leftcolumn-->
  <div class="endorsers"><a href="/auth/show-endorsers/1903.11137">Which authors of this paper are endorsers?</a> | <a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="https://arxiv.org/help/mathjax">What is MathJax?</a>)
    <span class="help" style="display: inline-block; font-style: normal; float: right; margin-top: 0; margin-right: 1em;"><a href="https://confluence.cornell.edu/x/MjmLFQ">Browse v0.1 released 2018-10-22</a>&nbsp;&nbsp;<button class="button is-small" id="feedback-button">Feedback?</button></span>
  </div>
  <script type="text/javascript" language="javascript">mathjaxToggle();</script>
  <script src="/bibex/bibex.js?20181010" type="text/javascript" defer></script>
  
</div>

  </div>

  <footer style="clear: both;">
    <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
      <!-- Macro-Column 1 -->
      <div class="column" style="padding: 0;">
        <div class="columns">
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="https://arxiv.org/about">About arXiv</a></li>
              <li><a href="https://arxiv.org/about/people/leadership_team">Leadership Team</a></li>
            </ul>
          </div>
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><span class="icon"><i class="fa fa-envelope"></i></span><a href="https://arxiv.org/help/contact"> Contact Us</a></li>
              <li><span class="icon"><i class="fa fa-twitter"></i></span><a href="https://twitter.com/arxiv"> Follow us on Twitter</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- End Macro-Column 1 -->
      <!-- Macro-Column 2 -->
      <div class="column" style="padding: 0;">
        <div class="columns">
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="https://arxiv.org/help">Help</a></li>
              <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
            </ul>
          </div>
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="https://blogs.cornell.edu/arxiv">Blog</a></li>
              <li><a href="https://arxiv.org/help/subscribe"> Subscribe</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- End Macro-Column 2 -->
    </div>

    <div class="columns" style="border-top: 1px solid #979797; margin: -0.75em;">
      <div class="column">
        <p class="help" style="margin-bottom: 0;">arXiv&#174; is a registered trademark of Cornell University.</p>
      </div>
      <div class="column">
        <p class="help" style="margin-bottom: 0;">If you have a disability and are having trouble accessing information on this website or need materials in an alternate format,
        contact <a href="mailto:web-accessibility@cornell.edu">web-accessibility@cornell.edu</a> for assistance.</p>
      </div>
    </div>
  </footer>

</body>

</html>


#####EOF#####


<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">

<head>
  <title>[1808.10250] SonarSnoop: Active Acoustic Side-Channel Attacks</title>
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/css/arXiv.css?v=20190307" />
  
  <!-- Piwik -->
  <script type="text/javascript">
    var _paq = _paq || [];
    _paq.push(["setDomains", ["*.arxiv.org"]]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
      var u = "//webanalytics.library.cornell.edu/";
      _paq.push(['setTrackerUrl', u + 'piwik.php']);
      _paq.push(['setSiteId', 538]);
      var d = document,
        g = d.createElement('script'),
        s = d.getElementsByTagName('script')[0];
      g.type = 'text/javascript';
      g.async = true;
      g.defer = true;
      g.src = u + 'piwik.js';
      s.parentNode.insertBefore(g, s);
    })();
  </script>
  <!-- End Piwik Code -->
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/zca7yc/b/13/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=7a8da419"></script>
<script type="text/javascript">window.ATL_JQ_PAGE_PROPS =  {
  "triggerFunction": function(showCollectorDialog) {
    //Requires that jQuery is available!
    jQuery("#feedback-button").click(function(e) {
      e.preventDefault();
      showCollectorDialog();
    });
  },
  fieldValues: {
    "components": ["15700"],  // Browse component.
    "versions": ["14132"],  // Release browse-0.1
    "customfield_11401": window.location.href
  }
  };
</script><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" media="screen" type="text/css" href="/bibex/bibex.css?20181010"/>
  <script src="//static.arxiv.org/js/mathjaxToggle.min.js" type="text/javascript"></script>
  <meta name="citation_title" content="SonarSnoop: Active Acoustic Side-Channel Attacks"/>
  <meta name="citation_author" content="Cheng, Peng"/>
  <meta name="citation_author" content="Bagci, Ibrahim Ethem"/>
  <meta name="citation_author" content="Roedig, Utz"/>
  <meta name="citation_author" content="Yan, Jeff"/>
  <meta name="citation_date" content="2018/08/30"/>
  <meta name="citation_online_date" content="2018/08/30"/>
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/1808.10250"/>
  <meta name="citation_arxiv_id" content="1808.10250"/><meta name="twitter:site" content="@arxiv"/>
    <meta property="twitter:title" content="SonarSnoop: Active Acoustic Side-Channel Attacks"/>
    <meta property="twitter:description" content="We report the first active acoustic side-channel attack. Speakers are used to
emit human inaudible acoustic signals and the echo is recorded via microphones,
turning the acoustic system of a smart..."/>
    <meta property="og:site_name" content="arXiv.org"/>
    <meta property="og:title" content="SonarSnoop: Active Acoustic Side-Channel Attacks"/>
    <meta property="og:url" content="https://arxiv.org/abs/1808.10250v1"/>
    <meta property="og:description" content="We report the first active acoustic side-channel attack. Speakers are used to
emit human inaudible acoustic signals and the echo is recorded via microphones,
turning the acoustic system of a smart phone into a sonar system. The echo
signal can be used to profile user interaction with the device. For example, a
victim&#39;s finger movements can be inferred to steal Android phone unlock
patterns. In our empirical study, the number of candidate unlock patterns that
an attacker must try to authenticate herself to a Samsung S4 Android phone can
be reduced by up to 70% using this novel acoustic side-channel. Our approach
can be easily applied to other application scenarios and device types. Overall,
our work highlights a new family of security threats."/>
</head>

<body  class="with-cu-identity">
  <noscript><img src="//webanalytics.library.cornell.edu/piwik.php?idsite=538&amp;rec=1" style="border:0;" alt="" /></noscript>
  <div id="cu-identity">
    <div id="cu-logo">
      <a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
    </div>
    <div id="support-ack">
      <a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br/>the Simons Foundation and member institutions.</a>
    </div>
  </div>

  <div id="header" >
    
  <h1><a href="/">arXiv.org</a> &gt; <a href="/list/cs/recent">cs</a> &gt; arXiv:1808.10250</h1>
  <div id="search">
    <form id="search-arxiv" method="get" action="https://arxiv.org/search">

      <div class="wrapper-search-arxiv">
        <input class="keyword-field" type="text" name="query" placeholder="Search or Article ID" />

        <div class="filter-field">
          <select name="searchtype">
            <option value="all">All fields</option>
            <option value="title">Title</option>
            <option value="author">Author(s)</option>
            <option value="abstract">Abstract</option>
            <option value="comments">Comments</option>
            <option value="journal_ref">Journal reference</option>
            <option value="acm_class">ACM classification</option>
            <option value="msc_class">MSC classification</option>
            <option value="report_num">Report number</option>
            <option value="paper_id">arXiv identifier</option>
            <option value="doi">DOI</option>
            <option value="orcid">ORCID</option>
            <option value="author_id">arXiv author ID</option>
            <option value="help">Help pages</option>
            <option value="full_text">Full text</option>
          </select>
        </div>
        <input class="btn-search-arxiv" value="" type="submit">
        <div class="links">(<a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced search</a>)</div>
      </div>
    </form>
  </div>

  </div>

  <div id="content">
    <!--
rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/1808.10250"
        dc:identifier="/abs/1808.10250"
        dc:title="SonarSnoop: Active Acoustic Side-Channel Attacks"
        trackback:ping="/trackback/1808.10250" />
    </rdf:RDF>
-->
<div id="abs">
  <div class="extra-services">
    <div class="full-text">
      <span class="descriptor">Full-text links:</span>
      <h2>Download:</h2>
      <ul>
  <li><a href="/pdf/1808.10250" accesskey="f">PDF</a></li>
  <li><a href="/format/1808.10250">Other formats</a></li></ul>
      <div class="abs-license">(<a href="http://arxiv.org/licenses/nonexclusive-distrib/1.0/" title="Rights to this article">license</a>)</div>
    </div>
    <!--end full-text-->
    <div class="browse">
    <h3>Current browse context:</h3>
  <div class="current">cs.CR</div>

  <div class="prevnext">

  <span class="arrow">
    <a href="/prevnext?site=arxiv.org&amp;id=1808.10250&amp;function=prev&amp;context=cs.CR"
       accesskey="p" title="previous in cs.CR (accesskey p)">&lt;&nbsp;prev</a>
  </span>&nbsp;|&nbsp;

  
  <span class="arrow">
    <a href="/prevnext?site=arxiv.org&amp;id=1808.10250&amp;function=next&amp;context=cs.CR" accesskey="n"
       title="next in cs.CR (accesskey n)">next&nbsp;&gt;</a>
  </span><br/>
  </div><div class="list">
    <a href="/list/cs.CR/new">new</a>&nbsp;|
    <a href="/list/cs.CR/recent">recent</a>&nbsp;|
    <a href="/list/cs.CR/1808">1808</a>
  </div><h3>Change to browse by:</h3>
  <div class="switch">
    
      <a href="/abs/1808.10250?context=cs">cs</a>
      
    <br/>
    
  </div>
  
    </div>

    <div class="extra-ref-cite">
      <h3>References &amp; Citations</h3>
      <ul>
        
        <li><a href="https://ui.adsabs.harvard.edu/#abs/arXiv:1808.10250">NASA ADS</a></li>
      </ul>
    </div>

    <div class="dblp">
    <h3><a href="https://dblp.uni-trier.de">DBLP</a> - CS Bibliography</h3>
    <div class="list">
      <a href="https://dblp.uni-trier.de/db/journals/corr/corr1808.html#abs-1808-10250" title="listing on DBLP">listing</a> | <a href="https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1808-10250" title="DBLP bibtex record">bibtex</a>
    </div>
    
    <div class="list">
      <a href="https://dblp.uni-trier.de/search/author?author=Peng%20Cheng" title="DBLP author search">Peng Cheng</a><br/><a href="https://dblp.uni-trier.de/search/author?author=Ibrahim%20Ethem%20Bagci" title="DBLP author search">Ibrahim Ethem Bagci</a><br/><a href="https://dblp.uni-trier.de/search/author?author=Utz%20Roedig" title="DBLP author search">Utz Roedig</a><br/><a href="https://dblp.uni-trier.de/search/author?author=Jeff%20Yan" title="DBLP author search">Jeff Yan</a>
    
    </div>
    
  </div><div class="bookmarks">
  <div class="what-is-this"><h3>Bookmark</h3> (<a href="https://arxiv.org/help/social_bookmarking">what is this?</a>)</div><a href="/ct?url=http%3A%2F%2Fwww.citeulike.org%2Fposturl%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1808.10250&amp;v=a153eea3"
     title="Bookmark on CiteULike">
    <img src="//static.arxiv.org/icons/social/citeulike.png"
         alt="CiteULike logo" />
  </a>
  <a href="/ct?url=http%3A%2F%2Fwww.bibsonomy.org%2FBibtexHandler%3FrequTask%3Dupload%26url%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1808.10250%26description%3DSonarSnoop%3A+Active+Acoustic+Side-Channel+Attacks&amp;v=9a148ce5"
     title="Bookmark on BibSonomy">
    <img src="//static.arxiv.org/icons/social/bibsonomy.png"
         alt="BibSonomy logo"/>
  </a>
  <a href="/ct?url=https%3A%2F%2Fwww.mendeley.com%2Fimport%2F%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1808.10250&amp;v=fbbaf189"
     title="Bookmark on Mendeley">
    <img src="//static.arxiv.org/icons/social/mendeley.png"
         alt="Mendeley logo"/>
  </a>
  <a href="/ct?url=https%3A%2F%2Freddit.com%2Fsubmit%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1808.10250%26title%3DSonarSnoop%3A+Active+Acoustic+Side-Channel+Attacks&amp;v=217e1e52"
     title="Bookmark on Reddit">
    <img src="//static.arxiv.org/icons/social/reddit.png"
         alt="Reddit logo"/>
  </a>
  <a href="/ct?url=http%3A%2F%2Fsciencewise.info%2Fbookmarks%2Fadd%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1808.10250&amp;v=7b0493fd"
     title="Bookmark on ScienceWISE">
    <img src="//static.arxiv.org/icons/social/sciencewise.png"
         alt="ScienceWISE logo"/>
  </a>
</div>
  </div>
  <!--end extra-services-->

  <div class="leftcolumn">
    <div class="subheader">
      <h1>Computer Science > Cryptography and Security</h1>
    </div>
    <h1 class="title mathjax"><span class="descriptor">Title:</span>SonarSnoop: Active Acoustic Side-Channel Attacks</h1>
    <div class="authors"><span class="descriptor">Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng%2C+P">Peng Cheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bagci%2C+I+E">Ibrahim Ethem Bagci</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Roedig%2C+U">Utz Roedig</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yan%2C+J">Jeff Yan</a>
    </div>

    <div class="dateline">(Submitted on 30 Aug 2018)</div>

    
    <blockquote class="abstract mathjax"><span class="descriptor">Abstract:</span>  We report the first active acoustic side-channel attack. Speakers are used to
emit human inaudible acoustic signals and the echo is recorded via microphones,
turning the acoustic system of a smart phone into a sonar system. The echo
signal can be used to profile user interaction with the device. For example, a
victim&#39;s finger movements can be inferred to steal Android phone unlock
patterns. In our empirical study, the number of candidate unlock patterns that
an attacker must try to authenticate herself to a Samsung S4 Android phone can
be reduced by up to 70% using this novel acoustic side-channel. Our approach
can be easily applied to other application scenarios and device types. Overall,
our work highlights a new family of security threats.
</blockquote>
    <!--CONTEXT-->
    <div class="metatable">
      <table summary="Additional metadata"><tr>
          <td class="tablecell label">Subjects:</td>
          <td class="tablecell subjects"><span class="primary-subject">Cryptography and Security (cs.CR)</span></td>
        </tr><tr>
          <td class="tablecell label">Cite as:</td>
          <td class="tablecell arxivid"><a href="https://arxiv.org/abs/1808.10250">arXiv:1808.10250</a> [cs.CR]</td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxividv">(or <span class="arxivid">
              <a href="https://arxiv.org/abs/1808.10250v1">arXiv:1808.10250v1</a> [cs.CR]</span> for this version)
          </td>
        </tr>
      </table>
    </div>
    <div class="submission-history">
      <h2>Submission history</h2> From: Ibrahim Ethem Bagci [<a href="/show-email/701f9f22/1808.10250">view email</a>]
      <br/><b>[v1]</b>
Thu, 30 Aug 2018 12:42:20 UTC (4,232 KB)<br/></div>
  </div>
  <!--end leftcolumn-->
  <div class="endorsers"><a href="/auth/show-endorsers/1808.10250">Which authors of this paper are endorsers?</a> | <a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="https://arxiv.org/help/mathjax">What is MathJax?</a>)
    <span class="help" style="display: inline-block; font-style: normal; float: right; margin-top: 0; margin-right: 1em;"><a href="https://confluence.cornell.edu/x/MjmLFQ">Browse v0.1 released 2018-10-22</a>&nbsp;&nbsp;<button class="button is-small" id="feedback-button">Feedback?</button></span>
  </div>
  <script type="text/javascript" language="javascript">mathjaxToggle();</script>
  <script src="/bibex/bibex.js?20181010" type="text/javascript" defer></script>
  
</div>

  </div>

  <footer style="clear: both;">
    <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
      <!-- Macro-Column 1 -->
      <div class="column" style="padding: 0;">
        <div class="columns">
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="https://arxiv.org/about">About arXiv</a></li>
              <li><a href="https://arxiv.org/about/people/leadership_team">Leadership Team</a></li>
            </ul>
          </div>
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><span class="icon"><i class="fa fa-envelope"></i></span><a href="https://arxiv.org/help/contact"> Contact Us</a></li>
              <li><span class="icon"><i class="fa fa-twitter"></i></span><a href="https://twitter.com/arxiv"> Follow us on Twitter</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- End Macro-Column 1 -->
      <!-- Macro-Column 2 -->
      <div class="column" style="padding: 0;">
        <div class="columns">
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="https://arxiv.org/help">Help</a></li>
              <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
            </ul>
          </div>
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="https://blogs.cornell.edu/arxiv">Blog</a></li>
              <li><a href="https://arxiv.org/help/subscribe"> Subscribe</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- End Macro-Column 2 -->
    </div>

    <div class="columns" style="border-top: 1px solid #979797; margin: -0.75em;">
      <div class="column">
        <p class="help" style="margin-bottom: 0;">arXiv&#174; is a registered trademark of Cornell University.</p>
      </div>
      <div class="column">
        <p class="help" style="margin-bottom: 0;">If you have a disability and are having trouble accessing information on this website or need materials in an alternate format,
        contact <a href="mailto:web-accessibility@cornell.edu">web-accessibility@cornell.edu</a> for assistance.</p>
      </div>
    </div>
  </footer>

</body>

</html>


#####EOF#####





#####EOF#####


<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">

<head>
  <title>[1903.11137v1] Hearing your touch: A new acoustic side channel on smartphones</title>
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/css/arXiv.css?v=20190307" />
  
  <!-- Piwik -->
  <script type="text/javascript">
    var _paq = _paq || [];
    _paq.push(["setDomains", ["*.arxiv.org"]]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
      var u = "//webanalytics.library.cornell.edu/";
      _paq.push(['setTrackerUrl', u + 'piwik.php']);
      _paq.push(['setSiteId', 538]);
      var d = document,
        g = d.createElement('script'),
        s = d.getElementsByTagName('script')[0];
      g.type = 'text/javascript';
      g.async = true;
      g.defer = true;
      g.src = u + 'piwik.js';
      s.parentNode.insertBefore(g, s);
    })();
  </script>
  <!-- End Piwik Code -->
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/zca7yc/b/13/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=7a8da419"></script>
<script type="text/javascript">window.ATL_JQ_PAGE_PROPS =  {
  "triggerFunction": function(showCollectorDialog) {
    //Requires that jQuery is available!
    jQuery("#feedback-button").click(function(e) {
      e.preventDefault();
      showCollectorDialog();
    });
  },
  fieldValues: {
    "components": ["15700"],  // Browse component.
    "versions": ["14132"],  // Release browse-0.1
    "customfield_11401": window.location.href
  }
  };
</script><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" media="screen" type="text/css" href="/bibex/bibex.css?20181010"/>
  <script src="//static.arxiv.org/js/mathjaxToggle.min.js" type="text/javascript"></script>
  <meta name="citation_title" content="Hearing your touch: A new acoustic side channel on smartphones"/>
  <meta name="citation_author" content="Shumailov, Ilia"/>
  <meta name="citation_author" content="Simon, Laurent"/>
  <meta name="citation_author" content="Yan, Jeff"/>
  <meta name="citation_author" content="Anderson, Ross"/>
  <meta name="citation_date" content="2019/03/26"/>
  <meta name="citation_online_date" content="2019/03/26"/>
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/1903.11137"/>
  <meta name="citation_arxiv_id" content="1903.11137"/><meta name="twitter:site" content="@arxiv"/>
    <meta property="twitter:title" content="Hearing your touch: A new acoustic side channel on smartphones"/>
    <meta property="twitter:description" content="We present the first acoustic side-channel attack that recovers what users
type on the virtual keyboard of their touch-screen smartphone or tablet. When a
user taps the screen with a finger, the..."/>
    <meta property="og:site_name" content="arXiv.org"/>
    <meta property="og:title" content="Hearing your touch: A new acoustic side channel on smartphones"/>
    <meta property="og:url" content="https://arxiv.org/abs/1903.11137v1"/>
    <meta property="og:description" content="We present the first acoustic side-channel attack that recovers what users
type on the virtual keyboard of their touch-screen smartphone or tablet. When a
user taps the screen with a finger, the tap generates a sound wave that
propagates on the screen surface and in the air. We found the device&#39;s
microphone(s) can recover this wave and &#34;hear&#34; the finger&#39;s touch, and the
wave&#39;s distortions are characteristic of the tap&#39;s location on the screen.
Hence, by recording audio through the built-in microphone(s), a malicious app
can infer text as the user enters it on their device. We evaluate the
effectiveness of the attack with 45 participants in a real-world environment on
an Android tablet and an Android smartphone. For the tablet, we recover 61% of
200 4-digit PIN-codes within 20 attempts, even if the model is not trained with
the victim&#39;s data. For the smartphone, we recover 9 words of size 7--13 letters
with 50 attempts in a common side-channel attack benchmark. Our results suggest
that it not always sufficient to rely on isolation mechanisms such as TrustZone
to protect user input. We propose and discuss hardware, operating-system and
application-level mechanisms to block this attack more effectively. Mobile
devices may need a richer capability model, a more user-friendly notification
system for sensor usage and a more thorough evaluation of the information
leaked by the underlying hardware."/>
</head>

<body  class="with-cu-identity">
  <noscript><img src="//webanalytics.library.cornell.edu/piwik.php?idsite=538&amp;rec=1" style="border:0;" alt="" /></noscript>
  <div id="cu-identity">
    <div id="cu-logo">
      <a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
    </div>
    <div id="support-ack">
      <a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br/>the Simons Foundation and member institutions.</a>
    </div>
  </div>

  <div id="header" >
    
  <h1><a href="/">arXiv.org</a> &gt; <a href="/list/cs/recent">cs</a> &gt; arXiv:1903.11137v1</h1>
  <div id="search">
    <form id="search-arxiv" method="get" action="https://arxiv.org/search">

      <div class="wrapper-search-arxiv">
        <input class="keyword-field" type="text" name="query" placeholder="Search or Article ID" />

        <div class="filter-field">
          <select name="searchtype">
            <option value="all">All fields</option>
            <option value="title">Title</option>
            <option value="author">Author(s)</option>
            <option value="abstract">Abstract</option>
            <option value="comments">Comments</option>
            <option value="journal_ref">Journal reference</option>
            <option value="acm_class">ACM classification</option>
            <option value="msc_class">MSC classification</option>
            <option value="report_num">Report number</option>
            <option value="paper_id">arXiv identifier</option>
            <option value="doi">DOI</option>
            <option value="orcid">ORCID</option>
            <option value="author_id">arXiv author ID</option>
            <option value="help">Help pages</option>
            <option value="full_text">Full text</option>
          </select>
        </div>
        <input class="btn-search-arxiv" value="" type="submit">
        <div class="links">(<a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced search</a>)</div>
      </div>
    </form>
  </div>

  </div>

  <div id="content">
    <!--
rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/1903.11137"
        dc:identifier="/abs/1903.11137"
        dc:title="Hearing your touch: A new acoustic side channel on smartphones"
        trackback:ping="/trackback/1903.11137" />
    </rdf:RDF>
-->
<div id="abs">
  <div class="extra-services">
    <div class="full-text">
      <span class="descriptor">Full-text links:</span>
      <h2>Download:</h2>
      <ul>
  <li><a href="/pdf/1903.11137v1" accesskey="f">PDF</a></li>
  <li><a href="/format/1903.11137v1">Other formats</a></li></ul>
      <div class="abs-license"><a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="Rights to this article"><img src="https://arxiv.org/icons/licenses/by-nc-sa-4.0.png"/></a></div>
    </div>
    <!--end full-text-->
    <div class="browse">
    <h3>Current browse context:</h3>
  <div class="current">cs.CR</div>

  <div class="prevnext">

  <span class="arrow">
    <a href="/prevnext?site=arxiv.org&amp;id=1903.11137&amp;function=prev&amp;context=cs.CR"
       accesskey="p" title="previous in cs.CR (accesskey p)">&lt;&nbsp;prev</a>
  </span>&nbsp;|&nbsp;

  
  <span class="arrow">
    <a href="/prevnext?site=arxiv.org&amp;id=1903.11137&amp;function=next&amp;context=cs.CR" accesskey="n"
       title="next in cs.CR (accesskey n)">next&nbsp;&gt;</a>
  </span><br/>
  </div><div class="list">
    <a href="/list/cs.CR/new">new</a>&nbsp;|
    <a href="/list/cs.CR/recent">recent</a>&nbsp;|
    <a href="/list/cs.CR/1903">1903</a>
  </div><h3>Change to browse by:</h3>
  <div class="switch">
    
      <a href="/abs/1903.11137?context=cs">cs</a>
      
    <br/>
    
      <span class="subclass"><a href="/abs/1903.11137?context=cs.AI">cs.AI</a></span>
      
    <br/>
    
  </div>
  
    </div>

    <div class="extra-ref-cite">
      <h3>References &amp; Citations</h3>
      <ul>
        
        <li><a href="https://ui.adsabs.harvard.edu/#abs/arXiv:1903.11137">NASA ADS</a></li>
      </ul>
    </div>

    <div class="bookmarks">
  <div class="what-is-this"><h3>Bookmark</h3> (<a href="https://arxiv.org/help/social_bookmarking">what is this?</a>)</div><a href="/ct?url=http%3A%2F%2Fwww.citeulike.org%2Fposturl%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1903.11137&amp;v=17c38719"
     title="Bookmark on CiteULike">
    <img src="//static.arxiv.org/icons/social/citeulike.png"
         alt="CiteULike logo" />
  </a>
  <a href="/ct?url=http%3A%2F%2Fwww.bibsonomy.org%2FBibtexHandler%3FrequTask%3Dupload%26url%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1903.11137%26description%3DHearing+your+touch%3A+A+new+acoustic+side+channel+on+smartphones&amp;v=eec3d4a7"
     title="Bookmark on BibSonomy">
    <img src="//static.arxiv.org/icons/social/bibsonomy.png"
         alt="BibSonomy logo"/>
  </a>
  <a href="/ct?url=https%3A%2F%2Fwww.mendeley.com%2Fimport%2F%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1903.11137&amp;v=2fa9c661"
     title="Bookmark on Mendeley">
    <img src="//static.arxiv.org/icons/social/mendeley.png"
         alt="Mendeley logo"/>
  </a>
  <a href="/ct?url=https%3A%2F%2Freddit.com%2Fsubmit%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1903.11137%26title%3DHearing+your+touch%3A+A+new+acoustic+side+channel+on+smartphones&amp;v=ec895bfc"
     title="Bookmark on Reddit">
    <img src="//static.arxiv.org/icons/social/reddit.png"
         alt="Reddit logo"/>
  </a>
  <a href="/ct?url=http%3A%2F%2Fsciencewise.info%2Fbookmarks%2Fadd%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1903.11137&amp;v=a5bf6599"
     title="Bookmark on ScienceWISE">
    <img src="//static.arxiv.org/icons/social/sciencewise.png"
         alt="ScienceWISE logo"/>
  </a>
</div>
  </div>
  <!--end extra-services-->

  <div class="leftcolumn">
    <div class="subheader">
      <h1>Computer Science > Cryptography and Security</h1>
    </div>
    <h1 class="title mathjax"><span class="descriptor">Title:</span>Hearing your touch: A new acoustic side channel on smartphones</h1>
    <div class="authors"><span class="descriptor">Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shumailov%2C+I">Ilia Shumailov</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Simon%2C+L">Laurent Simon</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yan%2C+J">Jeff Yan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Anderson%2C+R">Ross Anderson</a>
    </div>

    <div class="dateline">(Submitted on 26 Mar 2019)</div>

    
    <blockquote class="abstract mathjax"><span class="descriptor">Abstract:</span>  We present the first acoustic side-channel attack that recovers what users
type on the virtual keyboard of their touch-screen smartphone or tablet. When a
user taps the screen with a finger, the tap generates a sound wave that
propagates on the screen surface and in the air. We found the device&#39;s
microphone(s) can recover this wave and &#34;hear&#34; the finger&#39;s touch, and the
wave&#39;s distortions are characteristic of the tap&#39;s location on the screen.
Hence, by recording audio through the built-in microphone(s), a malicious app
can infer text as the user enters it on their device. We evaluate the
effectiveness of the attack with 45 participants in a real-world environment on
an Android tablet and an Android smartphone. For the tablet, we recover 61% of
200 4-digit PIN-codes within 20 attempts, even if the model is not trained with
the victim&#39;s data. For the smartphone, we recover 9 words of size 7--13 letters
with 50 attempts in a common side-channel attack benchmark. Our results suggest
that it not always sufficient to rely on isolation mechanisms such as TrustZone
to protect user input. We propose and discuss hardware, operating-system and
application-level mechanisms to block this attack more effectively. Mobile
devices may need a richer capability model, a more user-friendly notification
system for sensor usage and a more thorough evaluation of the information
leaked by the underlying hardware.
</blockquote>
    <!--CONTEXT-->
    <div class="metatable">
      <table summary="Additional metadata">
        <tr>
          <td class="tablecell label">Comments:</td>
          <td class="tablecell comments mathjax">Paper built on the MPhil thesis of Ilia Shumailov. 2017</td>
        </tr>
        <tr>
          <td class="tablecell label">Subjects:</td>
          <td class="tablecell subjects"><span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)</td>
        </tr><tr>
          <td class="tablecell label">Cite as:</td>
          <td class="tablecell arxivid"><a href="https://arxiv.org/abs/1903.11137">arXiv:1903.11137</a> [cs.CR]</td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxividv">(or <span class="arxivid">
              <a href="https://arxiv.org/abs/1903.11137v1">arXiv:1903.11137v1</a> [cs.CR]</span> for this version)
          </td>
        </tr>
      </table>
    </div>
    <div class="submission-history">
      <h2>Submission history</h2> From: Ilia Shumailov [<a href="/show-email/fe6e5f21/1903.11137">view email</a>]
      <br/><b>[v1]</b>
Tue, 26 Mar 2019 20:06:26 UTC (9,563 KB)<br/></div>
  </div>
  <!--end leftcolumn-->
  <div class="endorsers"><a href="/auth/show-endorsers/1903.11137">Which authors of this paper are endorsers?</a> | <a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="https://arxiv.org/help/mathjax">What is MathJax?</a>)
    <span class="help" style="display: inline-block; font-style: normal; float: right; margin-top: 0; margin-right: 1em;"><a href="https://confluence.cornell.edu/x/MjmLFQ">Browse v0.1 released 2018-10-22</a>&nbsp;&nbsp;<button class="button is-small" id="feedback-button">Feedback?</button></span>
  </div>
  <script type="text/javascript" language="javascript">mathjaxToggle();</script>
  <script src="/bibex/bibex.js?20181010" type="text/javascript" defer></script>
  
</div>

  </div>

  <footer style="clear: both;">
    <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
      <!-- Macro-Column 1 -->
      <div class="column" style="padding: 0;">
        <div class="columns">
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="https://arxiv.org/about">About arXiv</a></li>
              <li><a href="https://arxiv.org/about/people/leadership_team">Leadership Team</a></li>
            </ul>
          </div>
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><span class="icon"><i class="fa fa-envelope"></i></span><a href="https://arxiv.org/help/contact"> Contact Us</a></li>
              <li><span class="icon"><i class="fa fa-twitter"></i></span><a href="https://twitter.com/arxiv"> Follow us on Twitter</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- End Macro-Column 1 -->
      <!-- Macro-Column 2 -->
      <div class="column" style="padding: 0;">
        <div class="columns">
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="https://arxiv.org/help">Help</a></li>
              <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
            </ul>
          </div>
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="https://blogs.cornell.edu/arxiv">Blog</a></li>
              <li><a href="https://arxiv.org/help/subscribe"> Subscribe</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- End Macro-Column 2 -->
    </div>

    <div class="columns" style="border-top: 1px solid #979797; margin: -0.75em;">
      <div class="column">
        <p class="help" style="margin-bottom: 0;">arXiv&#174; is a registered trademark of Cornell University.</p>
      </div>
      <div class="column">
        <p class="help" style="margin-bottom: 0;">If you have a disability and are having trouble accessing information on this website or need materials in an alternate format,
        contact <a href="mailto:web-accessibility@cornell.edu">web-accessibility@cornell.edu</a> for assistance.</p>
      </div>
    </div>
  </footer>

</body>

</html>


#####EOF#####





#####EOF#####


<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">

<head>
  <title>[1707.08945] Robust Physical-World Attacks on Deep Learning Models</title>
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/css/arXiv.css?v=20190307" />
  
  <!-- Piwik -->
  <script type="text/javascript">
    var _paq = _paq || [];
    _paq.push(["setDomains", ["*.arxiv.org"]]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
      var u = "//webanalytics.library.cornell.edu/";
      _paq.push(['setTrackerUrl', u + 'piwik.php']);
      _paq.push(['setSiteId', 538]);
      var d = document,
        g = d.createElement('script'),
        s = d.getElementsByTagName('script')[0];
      g.type = 'text/javascript';
      g.async = true;
      g.defer = true;
      g.src = u + 'piwik.js';
      s.parentNode.insertBefore(g, s);
    })();
  </script>
  <!-- End Piwik Code -->
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/zca7yc/b/13/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=7a8da419"></script>
<script type="text/javascript">window.ATL_JQ_PAGE_PROPS =  {
  "triggerFunction": function(showCollectorDialog) {
    //Requires that jQuery is available!
    jQuery("#feedback-button").click(function(e) {
      e.preventDefault();
      showCollectorDialog();
    });
  },
  fieldValues: {
    "components": ["15700"],  // Browse component.
    "versions": ["14132"],  // Release browse-0.1
    "customfield_11401": window.location.href
  }
  };
</script><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" media="screen" type="text/css" href="/bibex/bibex.css?20181010"/>
  <script src="//static.arxiv.org/js/mathjaxToggle.min.js" type="text/javascript"></script>
  <meta name="citation_title" content="Robust Physical-World Attacks on Deep Learning Models"/>
  <meta name="citation_author" content="Eykholt, Kevin"/>
  <meta name="citation_author" content="Evtimov, Ivan"/>
  <meta name="citation_author" content="Fernandes, Earlence"/>
  <meta name="citation_author" content="Li, Bo"/>
  <meta name="citation_author" content="Rahmati, Amir"/>
  <meta name="citation_author" content="Xiao, Chaowei"/>
  <meta name="citation_author" content="Prakash, Atul"/>
  <meta name="citation_author" content="Kohno, Tadayoshi"/>
  <meta name="citation_author" content="Song, Dawn"/>
  <meta name="citation_date" content="2017/07/27"/>
  <meta name="citation_online_date" content="2018/04/10"/>
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/1707.08945"/>
  <meta name="citation_arxiv_id" content="1707.08945"/><meta name="twitter:site" content="@arxiv"/>
    <meta property="twitter:title" content="Robust Physical-World Attacks on Deep Learning Models"/>
    <meta property="twitter:description" content="Recent studies show that the state-of-the-art deep neural networks (DNNs) are
vulnerable to adversarial examples, resulting from small-magnitude
perturbations added to the input. Given that that..."/>
    <meta property="og:site_name" content="arXiv.org"/>
    <meta property="og:title" content="Robust Physical-World Attacks on Deep Learning Models"/>
    <meta property="og:url" content="https://arxiv.org/abs/1707.08945v5"/>
    <meta property="og:description" content="Recent studies show that the state-of-the-art deep neural networks (DNNs) are
vulnerable to adversarial examples, resulting from small-magnitude
perturbations added to the input. Given that that emerging physical systems are
using DNNs in safety-critical situations, adversarial examples could mislead
these systems and cause dangerous situations.Therefore, understanding
adversarial examples in the physical world is an important step towards
developing resilient learning algorithms. We propose a general attack
algorithm,Robust Physical Perturbations (RP2), to generate robust visual
adversarial perturbations under different physical conditions. Using the
real-world case of road sign classification, we show that adversarial examples
generated using RP2 achieve high targeted misclassification rates against
standard-architecture road sign classifiers in the physical world under various
environmental conditions, including viewpoints. Due to the current lack of a
standardized testing method, we propose a two-stage evaluation methodology for
robust physical adversarial examples consisting of lab and field tests. Using
this methodology, we evaluate the efficacy of physical adversarial
manipulations on real objects. Witha perturbation in the form of only black and
white stickers,we attack a real stop sign, causing targeted misclassification
in 100% of the images obtained in lab settings, and in 84.8%of the captured
video frames obtained on a moving vehicle(field test) for the target
classifier."/>
</head>

<body  class="with-cu-identity">
  <noscript><img src="//webanalytics.library.cornell.edu/piwik.php?idsite=538&amp;rec=1" style="border:0;" alt="" /></noscript>
  <div id="cu-identity">
    <div id="cu-logo">
      <a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
    </div>
    <div id="support-ack">
      <a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br/>the Simons Foundation and member institutions.</a>
    </div>
  </div>

  <div id="header" >
    
  <h1><a href="/">arXiv.org</a> &gt; <a href="/list/cs/recent">cs</a> &gt; arXiv:1707.08945</h1>
  <div id="search">
    <form id="search-arxiv" method="get" action="https://arxiv.org/search">

      <div class="wrapper-search-arxiv">
        <input class="keyword-field" type="text" name="query" placeholder="Search or Article ID" />

        <div class="filter-field">
          <select name="searchtype">
            <option value="all">All fields</option>
            <option value="title">Title</option>
            <option value="author">Author(s)</option>
            <option value="abstract">Abstract</option>
            <option value="comments">Comments</option>
            <option value="journal_ref">Journal reference</option>
            <option value="acm_class">ACM classification</option>
            <option value="msc_class">MSC classification</option>
            <option value="report_num">Report number</option>
            <option value="paper_id">arXiv identifier</option>
            <option value="doi">DOI</option>
            <option value="orcid">ORCID</option>
            <option value="author_id">arXiv author ID</option>
            <option value="help">Help pages</option>
            <option value="full_text">Full text</option>
          </select>
        </div>
        <input class="btn-search-arxiv" value="" type="submit">
        <div class="links">(<a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced search</a>)</div>
      </div>
    </form>
  </div>

  </div>

  <div id="content">
    <!--
rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/1707.08945"
        dc:identifier="/abs/1707.08945"
        dc:title="Robust Physical-World Attacks on Deep Learning Models"
        trackback:ping="/trackback/1707.08945" />
    </rdf:RDF>
-->
<div id="abs">
  <div class="extra-services">
    <div class="full-text">
      <span class="descriptor">Full-text links:</span>
      <h2>Download:</h2>
      <ul>
  <li><a href="/pdf/1707.08945" accesskey="f">PDF</a></li>
  <li><a href="/format/1707.08945">Other formats</a></li></ul>
      <div class="abs-license">(<a href="http://arxiv.org/licenses/nonexclusive-distrib/1.0/" title="Rights to this article">license</a>)</div>
    </div>
    <!--end full-text-->
    <div class="browse">
    <h3>Current browse context:</h3>
  <div class="current">cs.CR</div>

  <div class="prevnext">

  <span class="arrow">
    <a href="/prevnext?site=arxiv.org&amp;id=1707.08945&amp;function=prev&amp;context=cs.CR"
       accesskey="p" title="previous in cs.CR (accesskey p)">&lt;&nbsp;prev</a>
  </span>&nbsp;|&nbsp;

  
  <span class="arrow">
    <a href="/prevnext?site=arxiv.org&amp;id=1707.08945&amp;function=next&amp;context=cs.CR" accesskey="n"
       title="next in cs.CR (accesskey n)">next&nbsp;&gt;</a>
  </span><br/>
  </div><div class="list">
    <a href="/list/cs.CR/new">new</a>&nbsp;|
    <a href="/list/cs.CR/recent">recent</a>&nbsp;|
    <a href="/list/cs.CR/1707">1707</a>
  </div><h3>Change to browse by:</h3>
  <div class="switch">
    
      <a href="/abs/1707.08945?context=cs">cs</a>
      
    <br/>
    
      <span class="subclass"><a href="/abs/1707.08945?context=cs.LG">cs.LG</a></span>
      
    <br/>
    
  </div>
  
    </div>

    <div class="extra-ref-cite">
      <h3>References &amp; Citations</h3>
      <ul>
        
        <li><a href="https://ui.adsabs.harvard.edu/#abs/arXiv:1707.08945">NASA ADS</a></li>
      </ul>
    </div>

    
    <div class="extra-general">
        <div class="what-is-this">
            <h3><a href="/tb/1707.08945"> 3 blog links</a></h3> (<a href="https://arxiv.org/help/trackback">what is this?</a>)
        </div>
    </div>
    <div class="dblp">
    <h3><a href="https://dblp.uni-trier.de">DBLP</a> - CS Bibliography</h3>
    <div class="list">
      <a href="https://dblp.uni-trier.de/db/journals/corr/corr1707.html#EvtimovEFKLPRS17" title="listing on DBLP">listing</a> | <a href="https://dblp.uni-trier.de/rec/bibtex/journals/corr/EvtimovEFKLPRS17" title="DBLP bibtex record">bibtex</a>
    </div>
    
    <div class="list">
      <a href="https://dblp.uni-trier.de/search/author?author=Ivan%20Evtimov" title="DBLP author search">Ivan Evtimov</a><br/><a href="https://dblp.uni-trier.de/search/author?author=Kevin%20Eykholt" title="DBLP author search">Kevin Eykholt</a><br/><a href="https://dblp.uni-trier.de/search/author?author=Earlence%20Fernandes" title="DBLP author search">Earlence Fernandes</a><br/><a href="https://dblp.uni-trier.de/search/author?author=Tadayoshi%20Kohno" title="DBLP author search">Tadayoshi Kohno</a><br/><a href="https://dblp.uni-trier.de/search/author?author=Bo%20Li" title="DBLP author search">Bo Li</a>
    
      <div class="list">&hellip;</div>
    
    </div>
    
  </div><div class="bookmarks">
  <div class="what-is-this"><h3>Bookmark</h3> (<a href="https://arxiv.org/help/social_bookmarking">what is this?</a>)</div><a href="/ct?url=http%3A%2F%2Fwww.citeulike.org%2Fposturl%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1707.08945&amp;v=3ba997dd"
     title="Bookmark on CiteULike">
    <img src="//static.arxiv.org/icons/social/citeulike.png"
         alt="CiteULike logo" />
  </a>
  <a href="/ct?url=http%3A%2F%2Fwww.bibsonomy.org%2FBibtexHandler%3FrequTask%3Dupload%26url%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1707.08945%26description%3DRobust+Physical-World+Attacks+on+Deep+Learning+Models&amp;v=db88bec1"
     title="Bookmark on BibSonomy">
    <img src="//static.arxiv.org/icons/social/bibsonomy.png"
         alt="BibSonomy logo"/>
  </a>
  <a href="/ct?url=https%3A%2F%2Fwww.mendeley.com%2Fimport%2F%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1707.08945&amp;v=794e054a"
     title="Bookmark on Mendeley">
    <img src="//static.arxiv.org/icons/social/mendeley.png"
         alt="Mendeley logo"/>
  </a>
  <a href="/ct?url=https%3A%2F%2Freddit.com%2Fsubmit%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1707.08945%26title%3DRobust+Physical-World+Attacks+on+Deep+Learning+Models&amp;v=f3c66267"
     title="Bookmark on Reddit">
    <img src="//static.arxiv.org/icons/social/reddit.png"
         alt="Reddit logo"/>
  </a>
  <a href="/ct?url=http%3A%2F%2Fsciencewise.info%2Fbookmarks%2Fadd%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1707.08945&amp;v=0dd1bbe4"
     title="Bookmark on ScienceWISE">
    <img src="//static.arxiv.org/icons/social/sciencewise.png"
         alt="ScienceWISE logo"/>
  </a>
</div>
  </div>
  <!--end extra-services-->

  <div class="leftcolumn">
    <div class="subheader">
      <h1>Computer Science > Cryptography and Security</h1>
    </div>
    <h1 class="title mathjax"><span class="descriptor">Title:</span>Robust Physical-World Attacks on Deep Learning Models</h1>
    <div class="authors"><span class="descriptor">Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Eykholt%2C+K">Kevin Eykholt</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Evtimov%2C+I">Ivan Evtimov</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fernandes%2C+E">Earlence Fernandes</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+B">Bo Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rahmati%2C+A">Amir Rahmati</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiao%2C+C">Chaowei Xiao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Prakash%2C+A">Atul Prakash</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kohno%2C+T">Tadayoshi Kohno</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song%2C+D">Dawn Song</a>
    </div>

    <div class="dateline">(Submitted on 27 Jul 2017 (<a href="/abs/1707.08945v1">v1</a>), last revised 10 Apr 2018 (this version, v5))</div>

    
    <blockquote class="abstract mathjax"><span class="descriptor">Abstract:</span>  Recent studies show that the state-of-the-art deep neural networks (DNNs) are
vulnerable to adversarial examples, resulting from small-magnitude
perturbations added to the input. Given that that emerging physical systems are
using DNNs in safety-critical situations, adversarial examples could mislead
these systems and cause dangerous situations.Therefore, understanding
adversarial examples in the physical world is an important step towards
developing resilient learning algorithms. We propose a general attack
algorithm,Robust Physical Perturbations (RP2), to generate robust visual
adversarial perturbations under different physical conditions. Using the
real-world case of road sign classification, we show that adversarial examples
generated using RP2 achieve high targeted misclassification rates against
standard-architecture road sign classifiers in the physical world under various
environmental conditions, including viewpoints. Due to the current lack of a
standardized testing method, we propose a two-stage evaluation methodology for
robust physical adversarial examples consisting of lab and field tests. Using
this methodology, we evaluate the efficacy of physical adversarial
manipulations on real objects. Witha perturbation in the form of only black and
white stickers,we attack a real stop sign, causing targeted misclassification
in 100% of the images obtained in lab settings, and in 84.8%of the captured
video frames obtained on a moving vehicle(field test) for the target
classifier.
</blockquote>
    <!--CONTEXT-->
    <div class="metatable">
      <table summary="Additional metadata">
        <tr>
          <td class="tablecell label">Comments:</td>
          <td class="tablecell comments mathjax">Accepted to CVPR 2018</td>
        </tr>
        <tr>
          <td class="tablecell label">Subjects:</td>
          <td class="tablecell subjects"><span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)</td>
        </tr><tr>
          <td class="tablecell label">Cite as:</td>
          <td class="tablecell arxivid"><a href="https://arxiv.org/abs/1707.08945">arXiv:1707.08945</a> [cs.CR]</td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxividv">(or <span class="arxivid">
              <a href="https://arxiv.org/abs/1707.08945v5">arXiv:1707.08945v5</a> [cs.CR]</span> for this version)
          </td>
        </tr>
      </table>
    </div>
    <div class="submission-history">
      <h2>Submission history</h2> From: Kevin Eykholt [<a href="/show-email/fb73fbcb/1707.08945">view email</a>]
      <br/>
  <b><a href="/abs/1707.08945v1">[v1]</a></b>
  Thu, 27 Jul 2017 17:37:22 UTC (9,143 KB)<br/>
  <b><a href="/abs/1707.08945v2">[v2]</a></b>
  Sun, 30 Jul 2017 15:58:21 UTC (9,128 KB)<br/>
  <b><a href="/abs/1707.08945v3">[v3]</a></b>
  Mon, 7 Aug 2017 23:52:10 UTC (9,128 KB)<br/>
  <b><a href="/abs/1707.08945v4">[v4]</a></b>
  Wed, 13 Sep 2017 03:59:56 UTC (8,067 KB)<br/><b>[v5]</b>
Tue, 10 Apr 2018 16:22:47 UTC (1,867 KB)<br/></div>
  </div>
  <!--end leftcolumn-->
  <div class="endorsers"><a href="/auth/show-endorsers/1707.08945">Which authors of this paper are endorsers?</a> | <a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="https://arxiv.org/help/mathjax">What is MathJax?</a>)
    <span class="help" style="display: inline-block; font-style: normal; float: right; margin-top: 0; margin-right: 1em;"><a href="https://confluence.cornell.edu/x/MjmLFQ">Browse v0.1 released 2018-10-22</a>&nbsp;&nbsp;<button class="button is-small" id="feedback-button">Feedback?</button></span>
  </div>
  <script type="text/javascript" language="javascript">mathjaxToggle();</script>
  <script src="/bibex/bibex.js?20181010" type="text/javascript" defer></script>
  
</div>

  </div>

  <footer style="clear: both;">
    <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
      <!-- Macro-Column 1 -->
      <div class="column" style="padding: 0;">
        <div class="columns">
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="https://arxiv.org/about">About arXiv</a></li>
              <li><a href="https://arxiv.org/about/people/leadership_team">Leadership Team</a></li>
            </ul>
          </div>
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><span class="icon"><i class="fa fa-envelope"></i></span><a href="https://arxiv.org/help/contact"> Contact Us</a></li>
              <li><span class="icon"><i class="fa fa-twitter"></i></span><a href="https://twitter.com/arxiv"> Follow us on Twitter</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- End Macro-Column 1 -->
      <!-- Macro-Column 2 -->
      <div class="column" style="padding: 0;">
        <div class="columns">
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="https://arxiv.org/help">Help</a></li>
              <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
            </ul>
          </div>
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="https://blogs.cornell.edu/arxiv">Blog</a></li>
              <li><a href="https://arxiv.org/help/subscribe"> Subscribe</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- End Macro-Column 2 -->
    </div>

    <div class="columns" style="border-top: 1px solid #979797; margin: -0.75em;">
      <div class="column">
        <p class="help" style="margin-bottom: 0;">arXiv&#174; is a registered trademark of Cornell University.</p>
      </div>
      <div class="column">
        <p class="help" style="margin-bottom: 0;">If you have a disability and are having trouble accessing information on this website or need materials in an alternate format,
        contact <a href="mailto:web-accessibility@cornell.edu">web-accessibility@cornell.edu</a> for assistance.</p>
      </div>
    </div>
  </footer>

</body>

</html>


#####EOF#####


<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.15.2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.15.2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.15.2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>arXiv Privacy Policy | arXiv e-print repository</title>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.15.2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>

<script src="https://static.arxiv.org/static/base/0.15.2/js/notification.js"></script>
<!-- Piwik -->
<script type="text/javascript">
var _paq = _paq || [];
_paq.push(["setDomains", ["*.arxiv.org"]]);
_paq.push(['trackPageView']);
_paq.push(['enableLinkTracking']);
(function()
{ var u="//webanalytics.library.cornell.edu/"; _paq.push(['setTrackerUrl', u+'piwik.php']); _paq.push(['setSiteId', 538]); var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0]; g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s); }
)();
</script>
<!-- End Piwik Code -->
    <!-- Custom style sheets or other head includes -->
    
  <link rel="stylesheet" href="https://static.arxiv.org/_marxdown/static/arxiv.marxdown/0.1/docs/css/docs.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/_marxdown/static/arxiv.marxdown/0.1/docs/css/codehilite.css" />
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-2rfjj7/b/29/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&amp;collectorId=9f4c36dd"></script>

  <script type="text/javascript">
  window.ATL_JQ_PAGE_PROPS =  {
    "triggerFunction": function(showCollectorDialog) {
      //Requires that jQuery is available!
      $("#feedback-button").click(function(e) {
        e.preventDefault();
        showCollectorDialog();
      });
    },
    fieldValues: {
      "components": ["12154"],
      "versions": ["14219"],
      "customfield_11401": window.location.href
    }
  };
  </script>


  </head>
  <body>
  
  <noscript><img src="//webanalytics.library.cornell.edu/piwik.php?idsite=538&rec=1" style="border:0;" alt="" /></noscript>
  <header>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.15.2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo"><img src="https://static.arxiv.org/static/base/0.15.2/images/arxiv-logo-web.svg" alt="arXiv" aria-label="logo" width="85" /></a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>

  <main role="main" class="container">
    

    


  
  <form action="/help/search" method="GET" class="form">
    <div class="field has-addons" style="justify-content: flex-end;">
      <div class="control" style="z-index: 1000">
        <label for="q" class="is-sr-only">Search Help</label>
        <input
          class="input is-small"
          type="text"
          name="q"
          placeholder="Search Help"
          
        />
      </div>
      <button class="button is-link is-small">Search</button>
    </div>
  </form>
  


<div class="search-shift-up">
  

<nav class="breadcrumb" aria-label="breadcrumbs">
  <ul>
    <li>
      <a href="https://arxiv.org/">
        arXiv
      </a>
    </li>
    <li>
      <a href="/help/">
        Help
      </a>
    </li>
    
      
      <li>
        <a href="/help/policies">
          arXiv Policies
        </a>
      </li>
      
    
    <li class="is-active">
      <a href="/help/policies/privacy_policy" aria-current="page">
        arXiv Privacy Policy
      </a>
    </li>
  </ul>
</nav>

</div>
<div class="search-shift-up">
  
  
<h1 id="arxiv-privacy-policy">arXiv Privacy Policy</h1>
<p>The following privacy notice describes what information we collect from you when you visit arXiv.org and how we use this information. Please read this privacy notice carefully so that you understand our privacy practices.</p>
<h2 id="effective-date-of-privacy-notice">Effective Date of Privacy Notice</h2>
<p>The privacy notice was last revised on 2019-02-08.</p>
<h2 id="what-information-we-gather-during-your-visit">What information we gather during your visit</h2>
<p><em>Information we gather for anonymous browsing:</em></p>
<p>We automatically collect certain information from you when you visit arXiv. This data is used to gather metrics on site usage including geographic location of visitors (IP address), pathways navigated through the website, what type of internet browser is used (user-agent). No attempt is made to deanonymize information we collect. </p>
<p><em>Information you may provide as registered user:</em></p>
<p>We may also collect other information as described in this notice or other specific Cornell University privacy notices. In order to take full advantage of some of our services, including article submission, we may request name, email, username, ORCID, past emails, affiliation, country, additional contact information and/or online identifiers. We may also request a record of affiliation and publication record be provided via email. When a user submits an article to arXiv we also collect author names, affiliation of co-authors (if provided), submitter IP address and submitter remote host.</p>
<p>Information associated with your user account can be viewed on your <a href="/user/">user dashboard</a>.</p>
<p><em>Information you may provide when submitting feedback or help request:</em></p>
<p>Users who email <a href="/help/contact">arXiv administrators</a> for technical help or moderation requests should include information relevant to the request.</p>
<p>Users who choose to fill out a feedback form on the website can do so anonymously or by providing personal information, such as name, email, and information about your operating environment.</p>
<p><em>Information provided by others:</em></p>
<p>Users who submit articles or other content to arXiv must ensure that they have the consent to share personal data contained within the content or the submission may be removed. </p>
<h2 id="how-we-use-your-information">How we use your information</h2>
<p>We use gathered information to track user trends and site usage with the goal of improving our visitors experience and optimizing our websites. We also use the information to administer this website and prevent abuse.</p>
<p><em>Customer support:</em></p>
<p>We use your information to resolve technical and moderation issues you encounter, to respond to your requests for assistance, to analyze crash information, and to repair and improve the services.</p>
<p><em>For safety and security:</em></p>
<p>We use information about you and your service use to verify accounts and activity, to monitor suspicious or fraudulent activity and to identify violations of service policies.</p>
<p><em>Daily mailings:</em></p>
<p>User-controlled subscriptions for announcements of daily papers include instructions to unsubscribe. More information at: <a href="https://arxiv.org/help/subscribe">https://arxiv.org/help/subscribe</a></p>
<p><em>Article Submission:</em></p>
<p>In the content submission and moderation process we link user accounts, authorship, and submitted content to track and process submissions. More information is at: <a href="https://arxiv.org/help/submit">https://arxiv.org/help/submit</a>; <a href="https://arxiv.org/help/registerhelp">https://arxiv.org/help/registerhelp</a>; <a href="https://arxiv.org/help/terms_of_submission">https://arxiv.org/help/terms_of_submission</a>; <a href="https://arxiv.org/help/moderation">https://arxiv.org/help/moderation</a> </p>
<p>Article metadata provided by submitter, including author list, author affiliation (if provided), submitter name is displayed with the article posting, email announcement of articles, and bulk metadata access via API. </p>
<p>The email address that was used to upload each submission is viewable by registered arXiv users to give users the ability to send feedback on papers to the submitter and to help users <a href="https://arxiv.org/help/endorsement#request">contact eligible arXiv endorsers</a> in order to make their own submissions.</p>
<p><em>Fundraising:</em> </p>
<p>Should you decide to donate to arXiv through the donate page you will be directed to the Cornell giving website and the information you provide will be used to process your donation. Please also see the <a href="https://giving.cornell.edu/privacy-policy/">Cornell giving privacy policy</a>.</p>
<p><em>Membership:</em></p>
<p>For users who visit arXiv from the IP range of an arXiv member institution, you will see the institution acknowledged on the arXiv.org website. arXiv member institutions also receive anonymous statistics on usage from within their IP range.</p>
<h2 id="with-whom-we-share-your-information">With whom we share your information</h2>
<p>Information we gather is shared with vendors we use to support the infrastructure, submission, moderation, and archival requirements of arXiv. However, we will not sell this data to third-parties for their use in direct marketing, advertising, or for the promotion of their products and services.</p>
<p><em>Info we share with third party services:</em></p>
<p>Atlassian Jira is used for collecting feedback about development features that are sent in voluntarily by users. Users can optionally provide name, email, and information about their current operating environment.</p>
<p>iThenticate is used by arXiv administrators for text analysis on submissions. The name of the submitter is included in the iThenticate report for tracking purposes.</p>
<p>Fastmail is used for email communication with arXiv volunteer moderators that may contain submission metadata, including author names, affiliations, submitter name and email.</p>
<p>GMail is used for email communication of submission and moderator emails that may contain submission metadata, including author names, affiliations, submitter name and email.</p>
<p>Slack is used for email communication of submission and moderator emails that may contain submission metadata, including author names, affiliations, submitter name and email.</p>
<p>Amazon Web Services is used for server infrastructure for components of the arXiv system.</p>
<p><a href="http://ads.harvard.edu/">Astrophysics Data System</a> (ADS) is a trusted partner with arXiv who we share anonymized browsing logs associated with arXiv papers for use in ADS search and discovery tools. </p>
<p><em>Info we share with moderators as part of submission and moderation:</em></p>
<p>As part of the article submission and moderation process arXiv moderators can view author list, submitter name, submitter email, and if the submitter was endorsed by another user moderators can view the name of the endorser.</p>
<p><em>Info we share publicly:</em></p>
<p>Users with registered arXiv accounts can view the email address that was used to upload each submission to arXiv. Please see the <a href="https://arxiv.org/help/email-protection">email protection page</a> for more information.</p>
<p>The metadata for arXiv articles is freely accessible and available to any third-party use.</p>
<h2 id="cookies">Cookies</h2>
<p>Cookies are text files stored on your computer and accessible only to the websites which create them.</p>
<p>Our websites may use cookies to keep you logged into secure areas of the website and/or to keep track of your preferences as you interact with certain services. It is not necessary to use/permit cookies to read articles on arXiv.org.</p>
<p>To manage cookies in arXiv please see: <a href="https://arxiv.org/cookies">https://arxiv.org/cookies</a></p>
<h2 id="protecting-your-information">Protecting your information</h2>
<p>No method of transmitting over the internet or storing electronic data is 100% secure, but arXiv follows standard practices to protect against the loss, misuse, or alteration of the information that is under our control.</p>
<h2 id="fundraising-and-update-emails">Fundraising and Update Emails</h2>
<p>Donors to arXiv may receive future fundraising emails from Cornell University. Those emails do contain options to unsubscribe.</p>
<p>arXiv members will receive annual update emails and direct correspondence relating to membership. Those emails will contain options to unsubscribe.</p>
<p>Your information is never shared with any third party marketing services.</p>
<h2 id="social-media-presence">Social Media Presence</h2>
<p>If you share our content through social media, such as tweeting about us on Twitter, those social networks will record that you have done so and may set a cookie for this purpose.</p>
<p>If you wish to opt-out of any of these social interactions please refer to the specific social media platform for instructions on how to do so.</p>
<h2 id="external-website-links">External Website Links</h2>
<p>From time-to-time, we will link to external websites that we neither own nor control. Cornell University is not responsible for the content, privacy practices, or web accessibility needs on these websites.</p>
<p>Contributors to the site, including article submissions and metadata may contain links as well. We are not responsible for the content, privacy practices, or web accessibility needs on these websites.</p>
<h2 id="special-notice-for-eu-residents">Special Notice for EU Residents</h2>
<p>If you are located within the European Economic Area (European Union, Norway, Liechtenstein, and Iceland), we acknowledge the rights granted to you under the General Data Protection Regulation (GDPR).</p>
<p>These rights may include:</p>
<ol>
<li>Right to access your information held by us</li>
<li>Right to correct inaccurate or incorrect information about you</li>
<li>Right to the erasure of your information when it is no longer necessary for us to retain it</li>
<li>Right to restrict processing of your personal information in specific situations</li>
<li>Right to object to processing your information, including sending you communications that may be considered direct-marketing materials</li>
<li>Right to object to automated decision-making and profiling, where applicable</li>
<li>Right to complain to a supervisory authority in your jurisdiction within the EU</li>
</ol>
<p>Please contact us with any questions, concerns, or if you wish to exercise any of these rights: <a href="mailto:gdpr@cornell.edu">gdpr@cornell.edu</a></p>
<h2 id="contact-information">Contact Information</h2>
<p>For questions about arXiv privacy policies or technical support please contact: <a href="mailto:help@arxiv.org">help@arxiv.org</a> </p>
<p>If you have general questions about Cornell University privacy practices, please contact: <a href="mailto:privacy@cornell.edu">privacy@cornell.edu</a> or visit <a href="https://privacy.cornell.edu">https://privacy.cornell.edu</a>.</p>


</div>

<aside class="has-text-centered" style="padding-top: 3rem;">
  
  <p class="is-size-7">"arXiv Privacy Policy" revision <a href="https://github.com/arXiv/arxiv-docs/releases/tag/0.2.8">0.2.8</a>. Last modified <a href="https://github.com/arXiv/arxiv-docs/tree/b54e00df/help/policies/privacy_policy.md">2019-03-13</a>.</p> <button class="button is-small" id="feedback-button">Feedback?</button>
  
</aside>


  </main>

  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About arXiv</a></li>
          <li><a href="https://arxiv.org/about/people/leadership_team">Leadership Team</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li><span class="icon"><i class="fa fa-envelope"></i></span><a href="https://arxiv.org/help/contact"> Contact</a></li>
          <li><span class="icon"><i class="fa fa-twitter"></i></span><a href="https://twitter.com/arxiv"> Follow us on Twitter</a></li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help">Help</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://blogs.cornell.edu/arxiv">Blog</a></li>
          <li><a href="https://arxiv.org/help/subscribe"> Subscribe</a></li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>

<div class="columns" style="border-top: 1px solid #979797">
  <div class="column">
    <p class="help">arXiv&#174; is a registered trademark of Cornell University.</p>
  </div>
  <div class="column">
    <p class="help">If you have a disability and are having trouble accessing information
      on this website or need materials in an alternate format, contact
      <a href="mailto:web-accessibility@cornell.edu">web-accessibility@cornell.edu</a> for
       assistance.</p>
  </div>
</div>
    
  </footer>
  </body>
</html>


#####EOF#####


<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.15.2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.15.2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.15.2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Contacting arXiv | arXiv e-print repository</title>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.15.2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>

<script src="https://static.arxiv.org/static/base/0.15.2/js/notification.js"></script>
<!-- Piwik -->
<script type="text/javascript">
var _paq = _paq || [];
_paq.push(["setDomains", ["*.arxiv.org"]]);
_paq.push(['trackPageView']);
_paq.push(['enableLinkTracking']);
(function()
{ var u="//webanalytics.library.cornell.edu/"; _paq.push(['setTrackerUrl', u+'piwik.php']); _paq.push(['setSiteId', 538]); var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0]; g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s); }
)();
</script>
<!-- End Piwik Code -->
    <!-- Custom style sheets or other head includes -->
    
  <link rel="stylesheet" href="https://static.arxiv.org/_marxdown/static/arxiv.marxdown/0.1/docs/css/docs.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/_marxdown/static/arxiv.marxdown/0.1/docs/css/codehilite.css" />
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-2rfjj7/b/29/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&amp;collectorId=9f4c36dd"></script>

  <script type="text/javascript">
  window.ATL_JQ_PAGE_PROPS =  {
    "triggerFunction": function(showCollectorDialog) {
      //Requires that jQuery is available!
      $("#feedback-button").click(function(e) {
        e.preventDefault();
        showCollectorDialog();
      });
    },
    fieldValues: {
      "components": ["12154"],
      "versions": ["14219"],
      "customfield_11401": window.location.href
    }
  };
  </script>


  </head>
  <body>
  
  <noscript><img src="//webanalytics.library.cornell.edu/piwik.php?idsite=538&rec=1" style="border:0;" alt="" /></noscript>
  <header>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.15.2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo"><img src="https://static.arxiv.org/static/base/0.15.2/images/arxiv-logo-web.svg" alt="arXiv" aria-label="logo" width="85" /></a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>

  <main role="main" class="container">
    

    


  
  <form action="/help/search" method="GET" class="form">
    <div class="field has-addons" style="justify-content: flex-end;">
      <div class="control" style="z-index: 1000">
        <label for="q" class="is-sr-only">Search Help</label>
        <input
          class="input is-small"
          type="text"
          name="q"
          placeholder="Search Help"
          
        />
      </div>
      <button class="button is-link is-small">Search</button>
    </div>
  </form>
  


<div class="search-shift-up">
  

<nav class="breadcrumb" aria-label="breadcrumbs">
  <ul>
    <li>
      <a href="https://arxiv.org/">
        arXiv
      </a>
    </li>
    <li>
      <a href="/help/">
        Help
      </a>
    </li>
    
    <li class="is-active">
      <a href="/help/contact" aria-current="page">
        Contacting arXiv
      </a>
    </li>
  </ul>
</nav>

</div>
<div class="search-shift-up">
  
  
<h1 id="contacting-arxiv">Contacting arXiv</h1>
<h2 id="membership-and-giving">Membership and Giving</h2>
<p>Information about institutional membership or contributions to arXiv can be found at <a href="/help/support">arXiv funding support</a>, or contact <a href="mailto:support@arxiv.org">support@arxiv.org</a>.</p>
<h2 id="technical-queries">Technical queries</h2>
<p>If you have questions about paper submission, accounts, and use of arXiv, review <a href="/help">help</a> and <a href="/help/faq">FAQ</a>. If you require further assistance, contact us at <a href="mailto:help@arxiv.org">help@arxiv.org</a>. </p>
<p>Note that arXiv help and moderation email addresses are monitored between 0900-1700 EST/EDT, Monday through Friday, subject to administrator availability and the holiday schedule.</p>
<p><strong>To avoid a long delay in receiving a response:</strong> </p>
<ul>
<li>Send a precise email including all relevant details (especially submission or paper ids, rejection ids, URLs, etc.).</li>
<li>If you receive an automated response to your email, please read all instructions carefully.<ul>
<li>If there is no other identifying information in your message
    (e.g., paper id or rejection id), you may need to address your
    email with <strong>"Dear arXiv,"</strong> to bypass our SPAM filter.</li>
</ul>
</li>
</ul>
<h2 id="moderation-queries">Moderation queries</h2>
<p>If you have questions about the status of your submission, contact us at <a href="mailto:moderation@arxiv.org">moderation@arxiv.org</a>.</p>
<ul>
<li>Send a precise email, including all relevant details (especially paper ids, former correspondence, submission summaries, etc.)</li>
<li>If you receive an automated response to your email, please read all instructions carefully.<ul>
<li>If there is no other identifying information in your message
    (e.g., paper id or rejection id), you may need to address your
    email with <strong>"Dear arXiv-moderation,"</strong> to bypass our SPAM
    filter.</li>
</ul>
</li>
<li>Please be <em>patient</em>, as moderation appeals are complicated and may
    take some time.</li>
</ul>
<h2 id="copyright-infringement">Copyright infringement</h2>
<p>To contact Cornell University's designated agent for alleged copyright
infringement, in accordance with the Digital Millennium Copyright Act,
see <a href="http://www.cornell.edu/copyright-infringement.cfm">Copyright
Infringements</a>.</p>


</div>

<aside class="has-text-centered" style="padding-top: 3rem;">
  
  <p class="is-size-7">"Contacting arXiv" revision <a href="https://github.com/arXiv/arxiv-docs/releases/tag/0.2.8">0.2.8</a>. Last modified <a href="https://github.com/arXiv/arxiv-docs/tree/2e903516/help/contact.md">2019-03-13</a>.</p> <button class="button is-small" id="feedback-button">Feedback?</button>
  
</aside>


  </main>

  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About arXiv</a></li>
          <li><a href="https://arxiv.org/about/people/leadership_team">Leadership Team</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li><span class="icon"><i class="fa fa-envelope"></i></span><a href="https://arxiv.org/help/contact"> Contact</a></li>
          <li><span class="icon"><i class="fa fa-twitter"></i></span><a href="https://twitter.com/arxiv"> Follow us on Twitter</a></li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help">Help</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://blogs.cornell.edu/arxiv">Blog</a></li>
          <li><a href="https://arxiv.org/help/subscribe"> Subscribe</a></li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>

<div class="columns" style="border-top: 1px solid #979797">
  <div class="column">
    <p class="help">arXiv&#174; is a registered trademark of Cornell University.</p>
  </div>
  <div class="column">
    <p class="help">If you have a disability and are having trouble accessing information
      on this website or need materials in an alternate format, contact
      <a href="mailto:web-accessibility@cornell.edu">web-accessibility@cornell.edu</a> for
       assistance.</p>
  </div>
</div>
    
  </footer>
  </body>
</html>


#####EOF#####


<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">

<head>
  <title>[1803.04497] Automated software vulnerability detection with machine learning</title>
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/css/arXiv.css?v=20190307" />
  
  <!-- Piwik -->
  <script type="text/javascript">
    var _paq = _paq || [];
    _paq.push(["setDomains", ["*.arxiv.org"]]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
      var u = "//webanalytics.library.cornell.edu/";
      _paq.push(['setTrackerUrl', u + 'piwik.php']);
      _paq.push(['setSiteId', 538]);
      var d = document,
        g = d.createElement('script'),
        s = d.getElementsByTagName('script')[0];
      g.type = 'text/javascript';
      g.async = true;
      g.defer = true;
      g.src = u + 'piwik.js';
      s.parentNode.insertBefore(g, s);
    })();
  </script>
  <!-- End Piwik Code -->
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/zca7yc/b/13/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=7a8da419"></script>
<script type="text/javascript">window.ATL_JQ_PAGE_PROPS =  {
  "triggerFunction": function(showCollectorDialog) {
    //Requires that jQuery is available!
    jQuery("#feedback-button").click(function(e) {
      e.preventDefault();
      showCollectorDialog();
    });
  },
  fieldValues: {
    "components": ["15700"],  // Browse component.
    "versions": ["14132"],  // Release browse-0.1
    "customfield_11401": window.location.href
  }
  };
</script><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" media="screen" type="text/css" href="/bibex/bibex.css?20181010"/>
  <script src="//static.arxiv.org/js/mathjaxToggle.min.js" type="text/javascript"></script>
  <meta name="citation_title" content="Automated software vulnerability detection with machine learning"/>
  <meta name="citation_author" content="Harer, Jacob A."/>
  <meta name="citation_author" content="Kim, Louis Y."/>
  <meta name="citation_author" content="Russell, Rebecca L."/>
  <meta name="citation_author" content="Ozdemir, Onur"/>
  <meta name="citation_author" content="Kosta, Leonard R."/>
  <meta name="citation_author" content="Rangamani, Akshay"/>
  <meta name="citation_author" content="Hamilton, Lei H."/>
  <meta name="citation_author" content="Centeno, Gabriel I."/>
  <meta name="citation_author" content="Key, Jonathan R."/>
  <meta name="citation_author" content="Ellingwood, Paul M."/>
  <meta name="citation_author" content="Antelman, Erik"/>
  <meta name="citation_author" content="Mackay, Alan"/>
  <meta name="citation_author" content="McConley, Marc W."/>
  <meta name="citation_author" content="Opper, Jeffrey M."/>
  <meta name="citation_author" content="Chin, Peter"/>
  <meta name="citation_author" content="Lazovich, Tomo"/>
  <meta name="citation_date" content="2018/02/14"/>
  <meta name="citation_online_date" content="2018/08/02"/>
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/1803.04497"/>
  <meta name="citation_arxiv_id" content="1803.04497"/><meta name="twitter:site" content="@arxiv"/>
    <meta property="twitter:title" content="Automated software vulnerability detection with machine learning"/>
    <meta property="twitter:description" content="Thousands of security vulnerabilities are discovered in production software
each year, either reported publicly to the Common Vulnerabilities and Exposures
database or discovered internally in..."/>
    <meta property="og:site_name" content="arXiv.org"/>
    <meta property="og:title" content="Automated software vulnerability detection with machine learning"/>
    <meta property="og:url" content="https://arxiv.org/abs/1803.04497v2"/>
    <meta property="og:description" content="Thousands of security vulnerabilities are discovered in production software
each year, either reported publicly to the Common Vulnerabilities and Exposures
database or discovered internally in proprietary code. Vulnerabilities often
manifest themselves in subtle ways that are not obvious to code reviewers or
the developers themselves. With the wealth of open source code available for
analysis, there is an opportunity to learn the patterns of bugs that can lead
to security vulnerabilities directly from data. In this paper, we present a
data-driven approach to vulnerability detection using machine learning,
specifically applied to C and C++ programs. We first compile a large dataset of
hundreds of thousands of open-source functions labeled with the outputs of a
static analyzer. We then compare methods applied directly to source code with
methods applied to artifacts extracted from the build process, finding that
source-based models perform better. We also compare the application of deep
neural network models with more traditional models such as random forests and
find the best performance comes from combining features learned by deep models
with tree-based models. Ultimately, our highest performing model achieves an
area under the precision-recall curve of 0.49 and an area under the ROC curve
of 0.87."/>
</head>

<body  class="with-cu-identity">
  <noscript><img src="//webanalytics.library.cornell.edu/piwik.php?idsite=538&amp;rec=1" style="border:0;" alt="" /></noscript>
  <div id="cu-identity">
    <div id="cu-logo">
      <a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
    </div>
    <div id="support-ack">
      <a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br/>the Simons Foundation and member institutions.</a>
    </div>
  </div>

  <div id="header" >
    
  <h1><a href="/">arXiv.org</a> &gt; <a href="/list/cs/recent">cs</a> &gt; arXiv:1803.04497</h1>
  <div id="search">
    <form id="search-arxiv" method="get" action="https://arxiv.org/search">

      <div class="wrapper-search-arxiv">
        <input class="keyword-field" type="text" name="query" placeholder="Search or Article ID" />

        <div class="filter-field">
          <select name="searchtype">
            <option value="all">All fields</option>
            <option value="title">Title</option>
            <option value="author">Author(s)</option>
            <option value="abstract">Abstract</option>
            <option value="comments">Comments</option>
            <option value="journal_ref">Journal reference</option>
            <option value="acm_class">ACM classification</option>
            <option value="msc_class">MSC classification</option>
            <option value="report_num">Report number</option>
            <option value="paper_id">arXiv identifier</option>
            <option value="doi">DOI</option>
            <option value="orcid">ORCID</option>
            <option value="author_id">arXiv author ID</option>
            <option value="help">Help pages</option>
            <option value="full_text">Full text</option>
          </select>
        </div>
        <input class="btn-search-arxiv" value="" type="submit">
        <div class="links">(<a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced search</a>)</div>
      </div>
    </form>
  </div>

  </div>

  <div id="content">
    <!--
rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/1803.04497"
        dc:identifier="/abs/1803.04497"
        dc:title="Automated software vulnerability detection with machine learning"
        trackback:ping="/trackback/1803.04497" />
    </rdf:RDF>
-->
<div id="abs">
  <div class="extra-services">
    <div class="full-text">
      <span class="descriptor">Full-text links:</span>
      <h2>Download:</h2>
      <ul>
  <li><a href="/pdf/1803.04497" accesskey="f">PDF</a></li>
  <li><a href="/format/1803.04497">Other formats</a></li></ul>
      <div class="abs-license">(<a href="http://arxiv.org/licenses/nonexclusive-distrib/1.0/" title="Rights to this article">license</a>)</div>
    </div>
    <!--end full-text-->
    <div class="browse">
    <h3>Current browse context:</h3>
  <div class="current">cs.SE</div>

  <div class="prevnext">

  <span class="arrow">
    <a href="/prevnext?site=arxiv.org&amp;id=1803.04497&amp;function=prev&amp;context=cs.SE"
       accesskey="p" title="previous in cs.SE (accesskey p)">&lt;&nbsp;prev</a>
  </span>&nbsp;|&nbsp;

  
  <span class="arrow">
    <a href="/prevnext?site=arxiv.org&amp;id=1803.04497&amp;function=next&amp;context=cs.SE" accesskey="n"
       title="next in cs.SE (accesskey n)">next&nbsp;&gt;</a>
  </span><br/>
  </div><div class="list">
    <a href="/list/cs.SE/new">new</a>&nbsp;|
    <a href="/list/cs.SE/recent">recent</a>&nbsp;|
    <a href="/list/cs.SE/1803">1803</a>
  </div><h3>Change to browse by:</h3>
  <div class="switch">
    
      <a href="/abs/1803.04497?context=cs">cs</a>
      
    <br/>
    
      <span class="subclass"><a href="/abs/1803.04497?context=cs.LG">cs.LG</a></span>
      
    <br/>
    
      <a href="/abs/1803.04497?context=stat">stat</a>
      
    <br/>
    
      <span class="subclass"><a href="/abs/1803.04497?context=stat.ML">stat.ML</a></span>
      
    <br/>
    
  </div>
  
    </div>

    <div class="extra-ref-cite">
      <h3>References &amp; Citations</h3>
      <ul>
        
        <li><a href="https://ui.adsabs.harvard.edu/#abs/arXiv:1803.04497">NASA ADS</a></li>
      </ul>
    </div>

    <div class="dblp">
    <h3><a href="https://dblp.uni-trier.de">DBLP</a> - CS Bibliography</h3>
    <div class="list">
      <a href="https://dblp.uni-trier.de/db/journals/corr/corr1803.html#abs-1803-04497" title="listing on DBLP">listing</a> | <a href="https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1803-04497" title="DBLP bibtex record">bibtex</a>
    </div>
    
    <div class="list">
      <a href="https://dblp.uni-trier.de/search/author?author=Jacob%20A.%20Harer" title="DBLP author search">Jacob A. Harer</a><br/><a href="https://dblp.uni-trier.de/search/author?author=Louis%20Y.%20Kim" title="DBLP author search">Louis Y. Kim</a><br/><a href="https://dblp.uni-trier.de/search/author?author=Rebecca%20L.%20Russell" title="DBLP author search">Rebecca L. Russell</a><br/><a href="https://dblp.uni-trier.de/search/author?author=Onur%20Ozdemir" title="DBLP author search">Onur Ozdemir</a><br/><a href="https://dblp.uni-trier.de/search/author?author=Leonard%20R.%20Kosta" title="DBLP author search">Leonard R. Kosta</a>
    
      <div class="list">&hellip;</div>
    
    </div>
    
  </div><div class="bookmarks">
  <div class="what-is-this"><h3>Bookmark</h3> (<a href="https://arxiv.org/help/social_bookmarking">what is this?</a>)</div><a href="/ct?url=http%3A%2F%2Fwww.citeulike.org%2Fposturl%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1803.04497&amp;v=7f9a2a9f"
     title="Bookmark on CiteULike">
    <img src="//static.arxiv.org/icons/social/citeulike.png"
         alt="CiteULike logo" />
  </a>
  <a href="/ct?url=http%3A%2F%2Fwww.bibsonomy.org%2FBibtexHandler%3FrequTask%3Dupload%26url%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1803.04497%26description%3DAutomated+software+vulnerability+detection+with+machine+learning&amp;v=250bc3eb"
     title="Bookmark on BibSonomy">
    <img src="//static.arxiv.org/icons/social/bibsonomy.png"
         alt="BibSonomy logo"/>
  </a>
  <a href="/ct?url=https%3A%2F%2Fwww.mendeley.com%2Fimport%2F%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1803.04497&amp;v=d0e63059"
     title="Bookmark on Mendeley">
    <img src="//static.arxiv.org/icons/social/mendeley.png"
         alt="Mendeley logo"/>
  </a>
  <a href="/ct?url=https%3A%2F%2Freddit.com%2Fsubmit%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1803.04497%26title%3DAutomated+software+vulnerability+detection+with+machine+learning&amp;v=bd8192d7"
     title="Bookmark on Reddit">
    <img src="//static.arxiv.org/icons/social/reddit.png"
         alt="Reddit logo"/>
  </a>
  <a href="/ct?url=http%3A%2F%2Fsciencewise.info%2Fbookmarks%2Fadd%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1803.04497&amp;v=4531f931"
     title="Bookmark on ScienceWISE">
    <img src="//static.arxiv.org/icons/social/sciencewise.png"
         alt="ScienceWISE logo"/>
  </a>
</div>
  </div>
  <!--end extra-services-->

  <div class="leftcolumn">
    <div class="subheader">
      <h1>Computer Science > Software Engineering</h1>
    </div>
    <h1 class="title mathjax"><span class="descriptor">Title:</span>Automated software vulnerability detection with machine learning</h1>
    <div class="authors"><span class="descriptor">Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Harer%2C+J+A">Jacob A. Harer</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kim%2C+L+Y">Louis Y. Kim</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Russell%2C+R+L">Rebecca L. Russell</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ozdemir%2C+O">Onur Ozdemir</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kosta%2C+L+R">Leonard R. Kosta</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rangamani%2C+A">Akshay Rangamani</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hamilton%2C+L+H">Lei H. Hamilton</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Centeno%2C+G+I">Gabriel I. Centeno</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Key%2C+J+R">Jonathan R. Key</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ellingwood%2C+P+M">Paul M. Ellingwood</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Antelman%2C+E">Erik Antelman</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mackay%2C+A">Alan Mackay</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=McConley%2C+M+W">Marc W. McConley</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Opper%2C+J+M">Jeffrey M. Opper</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chin%2C+P">Peter Chin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lazovich%2C+T">Tomo Lazovich</a>
    </div>

    <div class="dateline">(Submitted on 14 Feb 2018 (<a href="/abs/1803.04497v1">v1</a>), last revised 2 Aug 2018 (this version, v2))</div>

    
    <blockquote class="abstract mathjax"><span class="descriptor">Abstract:</span>  Thousands of security vulnerabilities are discovered in production software
each year, either reported publicly to the Common Vulnerabilities and Exposures
database or discovered internally in proprietary code. Vulnerabilities often
manifest themselves in subtle ways that are not obvious to code reviewers or
the developers themselves. With the wealth of open source code available for
analysis, there is an opportunity to learn the patterns of bugs that can lead
to security vulnerabilities directly from data. In this paper, we present a
data-driven approach to vulnerability detection using machine learning,
specifically applied to C and C++ programs. We first compile a large dataset of
hundreds of thousands of open-source functions labeled with the outputs of a
static analyzer. We then compare methods applied directly to source code with
methods applied to artifacts extracted from the build process, finding that
source-based models perform better. We also compare the application of deep
neural network models with more traditional models such as random forests and
find the best performance comes from combining features learned by deep models
with tree-based models. Ultimately, our highest performing model achieves an
area under the precision-recall curve of 0.49 and an area under the ROC curve
of 0.87.
</blockquote>
    <!--CONTEXT-->
    <div class="metatable">
      <table summary="Additional metadata"><tr>
          <td class="tablecell label">Subjects:</td>
          <td class="tablecell subjects"><span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)</td>
        </tr><tr>
          <td class="tablecell label">Cite as:</td>
          <td class="tablecell arxivid"><a href="https://arxiv.org/abs/1803.04497">arXiv:1803.04497</a> [cs.SE]</td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxividv">(or <span class="arxivid">
              <a href="https://arxiv.org/abs/1803.04497v2">arXiv:1803.04497v2</a> [cs.SE]</span> for this version)
          </td>
        </tr>
      </table>
    </div>
    <div class="submission-history">
      <h2>Submission history</h2> From: Onur Ozdemir [<a href="/show-email/d10345b7/1803.04497">view email</a>]
      <br/>
  <b><a href="/abs/1803.04497v1">[v1]</a></b>
  Wed, 14 Feb 2018 13:00:05 UTC (1,526 KB)<br/><b>[v2]</b>
Thu, 2 Aug 2018 13:27:12 UTC (1,526 KB)<br/></div>
  </div>
  <!--end leftcolumn-->
  <div class="endorsers"><a href="/auth/show-endorsers/1803.04497">Which authors of this paper are endorsers?</a> | <a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="https://arxiv.org/help/mathjax">What is MathJax?</a>)
    <span class="help" style="display: inline-block; font-style: normal; float: right; margin-top: 0; margin-right: 1em;"><a href="https://confluence.cornell.edu/x/MjmLFQ">Browse v0.1 released 2018-10-22</a>&nbsp;&nbsp;<button class="button is-small" id="feedback-button">Feedback?</button></span>
  </div>
  <script type="text/javascript" language="javascript">mathjaxToggle();</script>
  <script src="/bibex/bibex.js?20181010" type="text/javascript" defer></script>
  
</div>

  </div>

  <footer style="clear: both;">
    <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
      <!-- Macro-Column 1 -->
      <div class="column" style="padding: 0;">
        <div class="columns">
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="https://arxiv.org/about">About arXiv</a></li>
              <li><a href="https://arxiv.org/about/people/leadership_team">Leadership Team</a></li>
            </ul>
          </div>
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><span class="icon"><i class="fa fa-envelope"></i></span><a href="https://arxiv.org/help/contact"> Contact Us</a></li>
              <li><span class="icon"><i class="fa fa-twitter"></i></span><a href="https://twitter.com/arxiv"> Follow us on Twitter</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- End Macro-Column 1 -->
      <!-- Macro-Column 2 -->
      <div class="column" style="padding: 0;">
        <div class="columns">
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="https://arxiv.org/help">Help</a></li>
              <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
            </ul>
          </div>
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="https://blogs.cornell.edu/arxiv">Blog</a></li>
              <li><a href="https://arxiv.org/help/subscribe"> Subscribe</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- End Macro-Column 2 -->
    </div>

    <div class="columns" style="border-top: 1px solid #979797; margin: -0.75em;">
      <div class="column">
        <p class="help" style="margin-bottom: 0;">arXiv&#174; is a registered trademark of Cornell University.</p>
      </div>
      <div class="column">
        <p class="help" style="margin-bottom: 0;">If you have a disability and are having trouble accessing information on this website or need materials in an alternate format,
        contact <a href="mailto:web-accessibility@cornell.edu">web-accessibility@cornell.edu</a> for assistance.</p>
      </div>
    </div>
  </footer>

</body>

</html>


#####EOF#####


<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.15.2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.15.2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.15.2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.15.2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>

<script src="https://static.arxiv.org/static/base/0.15.2/js/notification.js"></script>
<!-- Piwik -->
<script type="text/javascript">
var _paq = _paq || [];
_paq.push(["setDomains", ["*.arxiv.org"]]);
_paq.push(['trackPageView']);
_paq.push(['enableLinkTracking']);
(function()
{ var u="//webanalytics.library.cornell.edu/"; _paq.push(['setTrackerUrl', u+'piwik.php']); _paq.push(['setSiteId', 538]); var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0]; g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s); }
)();
</script>
<!-- End Piwik Code -->
    <!-- Custom style sheets or other head includes -->
    
  <link rel="stylesheet" href="https://static.arxiv.org/static/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/css/search.css" />

  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14157"],  // Release search-0.5
        "customfield_11401": window.location.href
      }
    };
    </script>


  </head>
  <body>
  
  <noscript><img src="//webanalytics.library.cornell.edu/piwik.php?idsite=538&rec=1" style="border:0;" alt="" /></noscript>
  <header>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.15.2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo"><img src="https://static.arxiv.org/static/base/0.15.2/images/arxiv-logo-web.svg" alt="arXiv" aria-label="logo" width="85" /></a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>

  <main role="main" class="container">
    

    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;6 of 6 results for author: <span class="mathjax">Shumailov, I</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://confluence.cornell.edu/x/giazFQ">Search v0.5 released 2018-12-20</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Shumailov%2C+I">Search in all archives.</a>
    
    
    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Shumailov, I">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Shumailov%2C+I&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Shumailov, I">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      




<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.11137">arXiv:1903.11137</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.11137">pdf</a>, <a href="https://arxiv.org/format/1903.11137">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hearing your touch: A new acoustic side channel on smartphones
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shumailov%2C+I">Ilia Shumailov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Simon%2C+L">Laurent Simon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jeff Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+R">Ross Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.11137v1-abstract-short" style="display: inline;">
        We present the first acoustic side-channel attack that recovers what users type on the virtual keyboard of their touch-screen smartphone or tablet. When a user taps the screen with a finger, the tap generates a sound wave that propagates on the screen surface and in the air. We found the device's microphone(s) can recover this wave and "hear" the finger's touch, and the wave's distortions are char&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.11137v1-abstract-full').style.display = 'inline'; document.getElementById('1903.11137v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.11137v1-abstract-full" style="display: none;">
        We present the first acoustic side-channel attack that recovers what users type on the virtual keyboard of their touch-screen smartphone or tablet. When a user taps the screen with a finger, the tap generates a sound wave that propagates on the screen surface and in the air. We found the device&#39;s microphone(s) can recover this wave and &#34;hear&#34; the finger&#39;s touch, and the wave&#39;s distortions are characteristic of the tap&#39;s location on the screen. Hence, by recording audio through the built-in microphone(s), a malicious app can infer text as the user enters it on their device. We evaluate the effectiveness of the attack with 45 participants in a real-world environment on an Android tablet and an Android smartphone. For the tablet, we recover 61% of 200 4-digit PIN-codes within 20 attempts, even if the model is not trained with the victim&#39;s data. For the smartphone, we recover 9 words of size 7--13 letters with 50 attempts in a common side-channel attack benchmark. Our results suggest that it not always sufficient to rely on isolation mechanisms such as TrustZone to protect user input. We propose and discuss hardware, operating-system and application-level mechanisms to block this attack more effectively. Mobile devices may need a richer capability model, a more user-friendly notification system for sensor usage and a more thorough evaluation of the information leaked by the underlying hardware.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.11137v1-abstract-full').style.display = 'none'; document.getElementById('1903.11137v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 March, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Paper built on the MPhil thesis of Ilia Shumailov. 2017</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1901.08121">arXiv:1901.08121</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1901.08121">pdf</a>, <a href="https://arxiv.org/format/1901.08121">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sitatapatra: Blocking the Transfer of Adversarial Samples
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shumailov%2C+I">Ilia Shumailov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+X">Xitong Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yiren Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mullins%2C+R">Robert Mullins</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+R">Ross Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+C">Cheng-Zhong Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1901.08121v1-abstract-short" style="display: inline;">
        Convolutional Neural Networks (CNNs) are widely used to solve classification tasks in computer vision. However, they can be tricked into misclassifying specially crafted `adversarial' samples -- and samples built to trick one model often work alarmingly well against other models trained on the same task. In this paper we introduce Sitatapatra, a system designed to block the transfer of adversarial&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.08121v1-abstract-full').style.display = 'inline'; document.getElementById('1901.08121v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1901.08121v1-abstract-full" style="display: none;">
        Convolutional Neural Networks (CNNs) are widely used to solve classification tasks in computer vision. However, they can be tricked into misclassifying specially crafted `adversarial&#39; samples -- and samples built to trick one model often work alarmingly well against other models trained on the same task. In this paper we introduce Sitatapatra, a system designed to block the transfer of adversarial samples. It diversifies neural networks using a key, as in cryptography, and provides a mechanism for detecting attacks. What&#39;s more, when adversarial samples are detected they can typically be traced back to the individual device that was used to develop them. The run-time overheads are minimal permitting the use of Sitatapatra on constrained systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.08121v1-abstract-full').style.display = 'none'; document.getElementById('1901.08121v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 January, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1901.01769">arXiv:1901.01769</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1901.01769">pdf</a>, <a href="https://arxiv.org/format/1901.01769">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Tendrils of Crime: Visualizing the Diffusion of Stolen Bitcoins
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ahmed%2C+M">Mansoor Ahmed</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shumailov%2C+I">Ilia Shumailov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+R">Ross Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1901.01769v1-abstract-short" style="display: inline;">
        The first six months of 2018 have seen cryptocurrency thefts of $761 million, and the technology is also the latest and greatest tool for money laundering. This increase in crime has caused both researchers and law enforcement to look for ways to trace criminal proceeds. Although tracing algorithms have improved recently, they still yield an enormous amount of data of which very few datapoints are&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.01769v1-abstract-full').style.display = 'inline'; document.getElementById('1901.01769v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1901.01769v1-abstract-full" style="display: none;">
        The first six months of 2018 have seen cryptocurrency thefts of $761 million, and the technology is also the latest and greatest tool for money laundering. This increase in crime has caused both researchers and law enforcement to look for ways to trace criminal proceeds. Although tracing algorithms have improved recently, they still yield an enormous amount of data of which very few datapoints are relevant or interesting to investigators, let alone ordinary bitcoin owners interested in provenance. In this work we describe efforts to visualize relevant data on a blockchain. To accomplish this we come up with a graphical model to represent the stolen coins and then implement this using a variety of visualization techniques.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.01769v1-abstract-full').style.display = 'none'; document.getElementById('1901.01769v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 January, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at The Fifth International Workshop on Graphical Models for Security, hosted at FLoC 2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1812.00381">arXiv:1812.00381</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1812.00381">pdf</a>, <a href="https://arxiv.org/format/1812.00381">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Automatic Discovery of Cybercrime Supply Chains
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bhalerao%2C+R">Rasika Bhalerao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Aliapoulios%2C+M">Maxwell Aliapoulios</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shumailov%2C+I">Ilia Shumailov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Afroz%2C+S">Sadia Afroz</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=McCoy%2C+D">Damon McCoy</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1812.00381v2-abstract-short" style="display: inline;">
        Cybercrime forums enable modern criminal entrepreneurs to collaborate with other criminals into increasingly efficient and sophisticated criminal endeavors. Understanding the connections between different products and services can often illuminate effective interventions. However, generating this understanding of supply chains currently requires time-consuming manual effort.
  In this paper, we pr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1812.00381v2-abstract-full').style.display = 'inline'; document.getElementById('1812.00381v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1812.00381v2-abstract-full" style="display: none;">
        Cybercrime forums enable modern criminal entrepreneurs to collaborate with other criminals into increasingly efficient and sophisticated criminal endeavors. Understanding the connections between different products and services can often illuminate effective interventions. However, generating this understanding of supply chains currently requires time-consuming manual effort.
  In this paper, we propose a language-agnostic method to automatically extract supply chains from cybercrime forum posts and replies. Our supply chain detection algorithm can identify 36% and 58% relevant chains within major English and Russian forums, respectively, showing improvements over the baselines of 13% and 36%, respectively. Our analysis of the automatically generated supply chains demonstrates underlying connections between products and services within these forums. For example, the extracted supply chain illuminated the connection between hack-for-hire services and the selling of rare and valuable `OG&#39; accounts, which has only recently been reported. The understanding of connections between products and services exposes potentially effective intervention points.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1812.00381v2-abstract-full').style.display = 'none'; document.getElementById('1812.00381v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 December, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 December, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.07375">arXiv:1811.07375</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.07375">pdf</a>, <a href="https://arxiv.org/format/1811.07375">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Taboo Trap: Behavioural Detection of Adversarial Samples
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shumailov%2C+I">Ilia Shumailov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yiren Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mullins%2C+R">Robert Mullins</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+R">Ross Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.07375v1-abstract-short" style="display: inline;">
        Deep Neural Networks (DNNs) have become a powerful tool for a wide range of problems. Yet recent work has shown an increasing variety of adversarial samples that can fool them. Most existing detection mechanisms impose significant costs, either by using additional classifiers to spot adversarial samples, or by requiring the DNN to be restructured. In this paper, we introduce a novel defence. We tr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.07375v1-abstract-full').style.display = 'inline'; document.getElementById('1811.07375v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.07375v1-abstract-full" style="display: none;">
        Deep Neural Networks (DNNs) have become a powerful tool for a wide range of problems. Yet recent work has shown an increasing variety of adversarial samples that can fool them. Most existing detection mechanisms impose significant costs, either by using additional classifiers to spot adversarial samples, or by requiring the DNN to be restructured. In this paper, we introduce a novel defence. We train our DNN so that, as long as it is working as intended on the kind of inputs we expect, its behavior is constrained, in that a set of behaviors are taboo. If it is exposed to adversarial samples, they will often cause a taboo behavior, which we can detect. As an analogy, we can imagine that we are teaching our robot good manners; if it&#39;s ever rude, we know it&#39;s come under some bad influence. This defence mechanism is very simple and, although it involves a modest increase in training, has almost zero computation overhead at runtime -- making it particularly suitable for use in embedded systems. Taboos can be both subtle and diverse. Just as humans&#39; choice of language can convey a lot of information about location, affiliation, class and much else that can be opaque to outsiders but that enables members of the same group to recognise each other, so also taboo choice can encode and hide information. We can use this to make adversarial attacks much harder. It is a well-established design principle that the security of a system should not depend on the obscurity of its design, but of some variable (the key) which can differ between implementations and be changed as necessary. We explain how taboos can be used to equip a classifier with just such a key, and to tune the keying mechanism to adversaries of various capabilities. We evaluate the performance of a prototype against a wide range of attacks and show how our simple defense can work well in practice.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.07375v1-abstract-full').style.display = 'none'; document.getElementById('1811.07375v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 November, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.00208">arXiv:1810.00208</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.00208">pdf</a>, <a href="https://arxiv.org/format/1810.00208">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        To compress or not to compress: Understanding the Interactions between Adversarial Attacks and Neural Network Compression
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yiren Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shumailov%2C+I">Ilia Shumailov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mullins%2C+R">Robert Mullins</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+R">Ross Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.00208v1-abstract-short" style="display: inline;">
        As deep neural networks (DNNs) become widely used, pruned and quantised models are becoming ubiquitous on edge devices; such compressed DNNs are popular for lowering computational requirements. Meanwhile, recent studies show that adversarial samples can be effective at making DNNs misclassify. We, therefore, investigate the extent to which adversarial samples are transferable between uncompressed&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.00208v1-abstract-full').style.display = 'inline'; document.getElementById('1810.00208v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.00208v1-abstract-full" style="display: none;">
        As deep neural networks (DNNs) become widely used, pruned and quantised models are becoming ubiquitous on edge devices; such compressed DNNs are popular for lowering computational requirements. Meanwhile, recent studies show that adversarial samples can be effective at making DNNs misclassify. We, therefore, investigate the extent to which adversarial samples are transferable between uncompressed and compressed DNNs. We find that adversarial samples remain transferable for both pruned and quantised models. For pruning, the adversarial samples generated from heavily pruned models remain effective on uncompressed models. For quantisation, we find the transferability of adversarial samples is highly sensitive to integer precision.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.00208v1-abstract-full').style.display = 'none'; document.getElementById('1810.00208v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 September, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to SysML 2019</span>
    </p>
    

    

    
  </li>

</ol>


  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://confluence.cornell.edu/x/giazFQ">Search v0.5 released 2018-12-20</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>

  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About arXiv</a></li>
          <li><a href="https://arxiv.org/about/people/leadership_team">Leadership Team</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li><span class="icon"><i class="fa fa-envelope"></i></span><a href="https://arxiv.org/help/contact"> Contact</a></li>
          <li><span class="icon"><i class="fa fa-twitter"></i></span><a href="https://twitter.com/arxiv"> Follow us on Twitter</a></li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help">Help</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://blogs.cornell.edu/arxiv">Blog</a></li>
          <li><a href="https://arxiv.org/help/subscribe"> Subscribe</a></li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>

<div class="columns" style="border-top: 1px solid #979797">
  <div class="column">
    <p class="help">arXiv&#174; is a registered trademark of Cornell University.</p>
  </div>
  <div class="column">
    <p class="help">If you have a disability and are having trouble accessing information
      on this website or need materials in an alternate format, contact
      <a href="mailto:web-accessibility@cornell.edu">web-accessibility@cornell.edu</a> for
       assistance.</p>
  </div>
</div>
    
  </footer>
  </body>
</html>


#####EOF#####


<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.15.2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.15.2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.15.2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Advanced Search | arXiv e-print repository</title>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.15.2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>

<script src="https://static.arxiv.org/static/base/0.15.2/js/notification.js"></script>
<!-- Piwik -->
<script type="text/javascript">
var _paq = _paq || [];
_paq.push(["setDomains", ["*.arxiv.org"]]);
_paq.push(['trackPageView']);
_paq.push(['enableLinkTracking']);
(function()
{ var u="//webanalytics.library.cornell.edu/"; _paq.push(['setTrackerUrl', u+'piwik.php']); _paq.push(['setSiteId', 538]); var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0]; g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s); }
)();
</script>
<!-- End Piwik Code -->
    <!-- Custom style sheets or other head includes -->
    
  <link rel="stylesheet" href="https://static.arxiv.org/static/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/css/search.css" />

  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14157"],  // Release search-0.5
        "customfield_11401": window.location.href
      }
    };
    </script>


  </head>
  <body>
  
  <noscript><img src="//webanalytics.library.cornell.edu/piwik.php?idsite=538&rec=1" style="border:0;" alt="" /></noscript>
  <header>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.15.2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo"><img src="https://static.arxiv.org/static/base/0.15.2/images/arxiv-logo-web.svg" alt="arXiv" aria-label="logo" width="85" /></a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>

  <main role="main" class="container">
    

    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Advanced Search
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://confluence.cornell.edu/x/giazFQ">Search v0.5 released 2018-12-20</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/?order=-announced_date_first&amp;size=50">Simple Search</a>
        
      </div>
    </div>
    

<div class="columns is-desktop">
  <div class="column is-three-fifths-desktop">
    
    <div class="box">
      <form method="GET" action="/search/advanced">
        <input id="advanced" name="advanced" type="hidden" value="1">
        
        <section data-toggle="fieldset" id="terms-fieldset">
          
            <div class="field has-addons-tablet" data-toggle="fieldset-entry">
              <div class="control fieldset-hidden-on-first-row" style="visibility:hidden;">
                <span class="select">
                  <label for="terms-0-operator" class="hidden-label">Boolean operator</label>
                  <select default="AND" id="terms-0-operator" name="terms-0-operator"><option selected value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select>
                </span>
              </div>
              <div class="control is-expanded">
                <label for="terms-0-term" class="hidden-label">Search term</label>
                
                  <input class="input" id="terms-0-term" name="terms-0-term" placeholder="Search term..." type="text" value="">
                
                

              </div>
              <div class="control">
                <label for="terms-0-field" class="hidden-label">Field to search</label>
                <span class="select">
                  <select default="title" id="terms-0-field" name="terms-0-field"><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option value="all">All fields</option></select>
                </span>
              </div>
              <div class="control">
                <button type="button"
                  data-toggle="fieldset-remove-row"
                  id="term-0-remove"
                  class="button is-monospace fieldset-hidden-on-first-row"
                  aria-label="Remove this search term"
                  style="visibility:hidden;"
                  >
                </button>
              </div>
            </div>
          
          <div class="field is-clearfix">
            <div class="control is-pulled-right">
              <span style="line-height: 2em; padding-right: 0.25em;">Add another term</span>
              <button type="button" class="button is-monospace"
                data-toggle="fieldset-add-row"
                data-target="#terms-fieldset">+
              </button>
            </div>
          </div>
        </section>

        <section>
          <fieldset class="fieldset ">
            <legend class="legend">Subject</legend>
            
            <div class="help has-text-grey">All classifications will be included by default.</div>
            <div class="columns is-baseline">
              <div class="column">
                
<div class="field is-horizontal">
  <div class="control">
    <div class="checkbox">
      <input id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y">
      <label for="classification-computer_science">Computer Science (cs)</label>
    </div>
    
  </div>
</div>

                
<div class="field is-horizontal">
  <div class="control">
    <div class="checkbox">
      <input id="classification-economics" name="classification-economics" type="checkbox" value="y">
      <label for="classification-economics">Economics (econ)</label>
    </div>
    
  </div>
</div>

                
<div class="field is-horizontal">
  <div class="control">
    <div class="checkbox">
      <input id="classification-eess" name="classification-eess" type="checkbox" value="y">
      <label for="classification-eess">Electrical Engineering and Systems Science (eess)</label>
    </div>
    
  </div>
</div>

                
<div class="field is-horizontal">
  <div class="control">
    <div class="checkbox">
      <input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y">
      <label for="classification-mathematics">Mathematics (math)</label>
    </div>
    
  </div>
</div>

              </div>
              <div class="column">
                
                <div class="field is-horizontal is-grouped">
                  <div class="control">
                    <div class="checkbox">
                      <input id="classification-physics" name="classification-physics" type="checkbox" value="y">
                      <label for="classification-physics">Physics</label>
                    </div>
                    
                  </div>
                  <div class="control">
                    <div class="select is-small">
                      <label for="classification-physics_archives" class="hidden-label">Subtopic within physics</label>
                      <select id="classification-physics_archives" name="classification-physics_archives"><option selected value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select>
                    </div>
                    
                  </div>
                </div>
                <script>
                  $('select#classification-physics_archives').change(function() {
                    $('input#classification-physics').attr('checked', true);
                  });
                </script>
                
<div class="field is-horizontal">
  <div class="control">
    <div class="checkbox">
      <input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y">
      <label for="classification-q_biology">Quantitative Biology (q-bio)</label>
    </div>
    
  </div>
</div>

                
<div class="field is-horizontal">
  <div class="control">
    <div class="checkbox">
      <input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y">
      <label for="classification-q_finance">Quantitative Finance (q-fin)</label>
    </div>
    
  </div>
</div>

                
<div class="field is-horizontal">
  <div class="control">
    <div class="checkbox">
      <input id="classification-statistics" name="classification-statistics" type="checkbox" value="y">
      <label for="classification-statistics">Statistics (stat)</label>
    </div>
    
  </div>
</div>

              </div>
            </div>
            <div class="field">
              <div class="control">
                
                <label class="radio">
                  <input checked id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"> Include cross-listed papers
                </label>
                
                <label class="radio">
                  <input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"> Exclude cross-listed papers
                </label>
                
              </div>
            </div>
          </fieldset>
          <fieldset class="fieldset ">
            <legend class="legend">Date</legend>

                
                
                <div class="field is-grouped">
                  <div class="control">
                    <div class="radio"> 
                      <input checked id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates">
                      <label for="date-filter_by-0">All dates</label>
                    </div>
                  </div>
                  
                  
                </div>
                
                <div class="field is-grouped">
                  <div class="control">
                    <div class="radio"> 
                      <input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12">
                      <label for="date-filter_by-1">Past 12 months</label>
                    </div>
                  </div>
                  
                  
                </div>
                
                <div class="field is-grouped">
                  <div class="control">
                    <div class="radio"> 
                      <input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year">
                      <label for="date-filter_by-2">Specific year</label>
                    </div>
                  </div>
                  
                  
                  <div class="control is-datefield">
                    <label for="date-year" class="hidden-label">Enter four digit year</label>
                    
                      <input class="input is-small" id="date-year" name="date-year" placeholder="YYYY" type="text" value="">
                    
                    <script>
                      $('#date-year').focus(function() {
                        $('input[name="date-filter_by"]').attr('checked', false);
                        $('input[value="specific_year"]').attr('checked', true);
                      });
                    </script>
                    
                  </div>
                  
                </div>
                
                <div class="field is-grouped">
                  <div class="control">
                    <div class="radio"> 
                      <input id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range">
                      <label for="date-filter_by-3">Date range</label>
                    </div>
                  </div>
                  
                  
                </div>
                
                <div class="field is-grouped">
                  <div class="control">
                    <div class="control"><label for="date-from_date">From</label></div>
                    <div class="control is-datefield">
                      
                      <input class="input is-small" id="date-from_date" name="date-from_date" placeholder="YYYY[-MM[-DD]]" type="text" value="">
                      
                    </div>
                    
                  </div>
                  <div class="control">
                    <div class="control"><label for="date-to_date">to</label></div>
                    <div class="control is-datefield">
                      
                      <input class="input is-small" id="date-to_date" name="date-to_date" placeholder="YYYY[-MM[-DD]]" type="text" value="">
                      
                    </div>
                    
                  </div>
                </div>
                <div class="help has-text-grey is-size-7">
                  When limiting by date range, the lower bound of the "from" date
                  and the upper bound of the "to" date are used.<br/>For example,
                  searching with <code>From: 2012-02</code> and <code>To: 2013</code>
                  will search for papers submitted from
                  <code>2012-02-01</code> to <code>2013-12-31</code>.
                </div>
                <script>
                  $('#date-to_date').focus(function() {
                    $('input[name="date-filter_by"]').attr('checked', false);
                    $('input[value="date_range"]').attr('checked', true);
                  });
                  $('#date-from_date').focus(function() {
                    $('input[name="date-filter_by"]').attr('checked', false);
                    $('input[value="date_range"]').attr('checked', true);
                  });
                </script>

                <hr />

                <div class="field">
                  <div class="control">
                    
                    <label class="radio">
                      <input checked id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"> Submission date (most recent)
                    </label>
                    
                    <label class="radio">
                      <input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"> Submission date (original)
                    </label>
                    
                    <label class="radio">
                      <input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"> Announcement date
                    </label>
                    
                    <p class="help has-text-grey">You may filter on either submission date or announcement date. Note that announcement date supports only year and month granularity.</p>
                  </div>
                </div>

          </fieldset>
        </section>
        <section>
          <div class="field">
            <div class="control">
              
              <label class="radio">
                <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
              </label>
              
              <label class="radio">
                <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
              </label>
              
            </div>
          </div>
          <div class="level">
            <div class="level-left">
              <div class="level-item">
                <div class="field is-horizontal is-grouped">
                  <div class="control">
                    <span class="select is-small">
                      <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
                    </span>
                  </div>
                  <div class="control">
                   <label for="size">results per page</label>
                  </div>
                </div>
              </div>
              <div class="level-item">
                <div class="field is-horizontal is-grouped">
                  <div class="control">
                    <div class="checkbox">
                      <input id="include_older_versions" name="include_older_versions" type="checkbox" value="y">
                      <label for="include_older_versions">Include older versions of papers</label>
                    </div>
                  </div>
                </div>
              </div>
          </div>
            <div class="level-right">
              <button class="button is-link is-medium">Search</button>
            </div>
          </div>
        </section>
        <input type="hidden" name="order" value="-announced_date_first">
        </form>
      </div>
    </div>
    <div class="column">
      
  <div class="box">
    <article class="message is-link">
      <div class="message-header">
        <h4 class="has-text-white is-marginless is-bold">Searching by Author Name</h4>
      </div>
      <div class="message-body">
        <ul>
          <li>Using the <strong>Author(s) field</strong> produces best results for author name searches.</li>
          <li>For the most precise name search, follow <strong>surname(s), forename(s)</strong> or <strong>surname(s), initial(s)</strong> pattern: example Hawking, S or Hawking, Stephen</li>
          <li>For best results on multiple author names, <strong>separate individuals with a ;</strong> (semicolon). Example: Jin, D S; Ye, J</li>
          <li>Author names enclosed in quotes will return only <strong>exact matches</strong>. For example, "Stephen Hawking" will not return matches for Stephen W. Hawking.</li>
          <li>Diacritic character variants are automatically searched in the Author(s) field.</li>
          <li>Queries with no punctuation will treat each term independently.</li>
        </ul>
      </div>
    </article>
    <h2 class="title is-5">Tips</h2>
    <p class="title is-6">Wildcards:</p>
      <ul>
       <li>Use ? to replace a single character or * to replace any number of characters.</li>
       <li>Can be used in any field, but not in the first character position. See Journal References tips for exceptions.</li>
      </ul>
    <p class="title is-6">Expressions:</p>
      <ul>
         <li>TeX expressions can be searched, enclosed in single $ characters.</li>
      </ul>
    <p class="title is-6">Phrases:</p>
      <ul>
        <li>Enclose phrases in double quotes for exact matches in title, abstract, and comments.</li>
      </ul>
    <p class="title is-6">Dates:</p>
      <ul>
        <li>Sorting by announcement date will use the year and month the <em>original version</em> (v1) of the paper was announced.</li>
        <li>Sorting by submission date will use the year, month and day the <em>latest version</em> of the paper was submitted.</li>
      </ul>
    <p class="title is-6">Journal References:</p>
      <ul>
        <li>If a journal reference search contains a wildcard, matches will be made using wildcard matching as expected. For example, <strong>math*</strong> will match <em>math</em>, <em>maths</em>, <em>mathematics</em>.</li>
        <li>If a journal reference search does <strong>not</strong> contain a wildcard, only exact phrases entered will be matched. For example, <strong>math</strong> would match <em>math</em> or <em>math and science</em> but not <em>maths</em> or <em>mathematics</em>.</li>
        <li>All journal reference searches that do not contain a wildcard are literal searches: a search for <strong>Physica A</strong> will match all papers with journal references containing <em>Physica A</em>, but a search for <strong>Physica A, 245 (1997) 181</strong> will only return the paper with journal reference <em>Physica A, 245 (1997) 181</em>.</li>
      </ul>
  </div>

    </div>
  </div>
  

      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://confluence.cornell.edu/x/giazFQ">Search v0.5 released 2018-12-20</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>

  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About arXiv</a></li>
          <li><a href="https://arxiv.org/about/people/leadership_team">Leadership Team</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li><span class="icon"><i class="fa fa-envelope"></i></span><a href="https://arxiv.org/help/contact"> Contact</a></li>
          <li><span class="icon"><i class="fa fa-twitter"></i></span><a href="https://twitter.com/arxiv"> Follow us on Twitter</a></li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help">Help</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://blogs.cornell.edu/arxiv">Blog</a></li>
          <li><a href="https://arxiv.org/help/subscribe"> Subscribe</a></li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>

<div class="columns" style="border-top: 1px solid #979797">
  <div class="column">
    <p class="help">arXiv&#174; is a registered trademark of Cornell University.</p>
  </div>
  <div class="column">
    <p class="help">If you have a disability and are having trouble accessing information
      on this website or need materials in an alternate format, contact
      <a href="mailto:web-accessibility@cornell.edu">web-accessibility@cornell.edu</a> for
       assistance.</p>
  </div>
</div>
    
  </footer>
  </body>
</html>


#####EOF#####


<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.15.2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.15.2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.15.2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.15.2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>

<script src="https://static.arxiv.org/static/base/0.15.2/js/notification.js"></script>
<!-- Piwik -->
<script type="text/javascript">
var _paq = _paq || [];
_paq.push(["setDomains", ["*.arxiv.org"]]);
_paq.push(['trackPageView']);
_paq.push(['enableLinkTracking']);
(function()
{ var u="//webanalytics.library.cornell.edu/"; _paq.push(['setTrackerUrl', u+'piwik.php']); _paq.push(['setSiteId', 538]); var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0]; g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s); }
)();
</script>
<!-- End Piwik Code -->
    <!-- Custom style sheets or other head includes -->
    
  <link rel="stylesheet" href="https://static.arxiv.org/static/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/css/search.css" />

  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14157"],  // Release search-0.5
        "customfield_11401": window.location.href
      }
    };
    </script>


  </head>
  <body>
  
  <noscript><img src="//webanalytics.library.cornell.edu/piwik.php?idsite=538&rec=1" style="border:0;" alt="" /></noscript>
  <header>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.15.2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo"><img src="https://static.arxiv.org/static/base/0.15.2/images/arxiv-logo-web.svg" alt="arXiv" aria-label="logo" width="85" /></a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>

  <main role="main" class="container">
    

    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 113 results for author: <span class="mathjax">Yan, J</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://confluence.cornell.edu/x/giazFQ">Search v0.5 released 2018-12-20</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Yan%2C+J">Search in all archives.</a>
    
    
    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Yan, J">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Yan%2C+J&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Yan, J">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Yan%2C+J&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Yan%2C+J&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Yan%2C+J&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Yan%2C+J&amp;start=100"
            class="pagination-link "
            aria-label="Page 3"
            aria-current="page">3
          </a>
        </li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1904.00597">arXiv:1904.00597</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1904.00597">pdf</a>, <a href="https://arxiv.org/format/1904.00597">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Combinatorial Embedding Networks for Deep Graph Matching
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+R">Runzhong Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junchi Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+X">Xiaokang Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1904.00597v1-abstract-short" style="display: inline;">
        Graph matching refers to finding node correspondence between graphs, such that the corresponding node and edge's affinity can be maximized. In addition with its NP-completeness nature, another important challenge is effective modeling of the node-wise and structure-wise affinity across graphs and the resulting objective, to guide the matching procedure effectively finding the true matching against&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.00597v1-abstract-full').style.display = 'inline'; document.getElementById('1904.00597v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1904.00597v1-abstract-full" style="display: none;">
        Graph matching refers to finding node correspondence between graphs, such that the corresponding node and edge&#39;s affinity can be maximized. In addition with its NP-completeness nature, another important challenge is effective modeling of the node-wise and structure-wise affinity across graphs and the resulting objective, to guide the matching procedure effectively finding the true matching against noises. To this end, this paper devises an end-to-end differentiable deep network pipeline to learn the affinity for graph matching. It involves a supervised permutation loss regarding with node correspondence to capture the combinatorial nature for graph matching. Meanwhile deep graph embedding models are adopted to parameterize both intra-graph and cross-graph affinity functions, instead of the traditional shallow and simple parametric forms e.g. a Gaussian kernel. The embedding can also effectively capture the higher-order structure beyond second-order edges. The permutation loss model is agnostic to the number of nodes, and the embedding model is shared among nodes such that the network allows for varying numbers of nodes in graphs for training and inference. Moreover, our network is class-agnostic with some generalization capability across different categories. All these features are welcomed for real-world applications. Experiments show its superiority against state-of-the-art graph matching learning methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.00597v1-abstract-full').style.display = 'none'; document.getElementById('1904.00597v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 April, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.11137">arXiv:1903.11137</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.11137">pdf</a>, <a href="https://arxiv.org/format/1903.11137">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hearing your touch: A new acoustic side channel on smartphones
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shumailov%2C+I">Ilia Shumailov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Simon%2C+L">Laurent Simon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jeff Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+R">Ross Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.11137v1-abstract-short" style="display: inline;">
        We present the first acoustic side-channel attack that recovers what users type on the virtual keyboard of their touch-screen smartphone or tablet. When a user taps the screen with a finger, the tap generates a sound wave that propagates on the screen surface and in the air. We found the device's microphone(s) can recover this wave and "hear" the finger's touch, and the wave's distortions are char&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.11137v1-abstract-full').style.display = 'inline'; document.getElementById('1903.11137v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.11137v1-abstract-full" style="display: none;">
        We present the first acoustic side-channel attack that recovers what users type on the virtual keyboard of their touch-screen smartphone or tablet. When a user taps the screen with a finger, the tap generates a sound wave that propagates on the screen surface and in the air. We found the device&#39;s microphone(s) can recover this wave and &#34;hear&#34; the finger&#39;s touch, and the wave&#39;s distortions are characteristic of the tap&#39;s location on the screen. Hence, by recording audio through the built-in microphone(s), a malicious app can infer text as the user enters it on their device. We evaluate the effectiveness of the attack with 45 participants in a real-world environment on an Android tablet and an Android smartphone. For the tablet, we recover 61% of 200 4-digit PIN-codes within 20 attempts, even if the model is not trained with the victim&#39;s data. For the smartphone, we recover 9 words of size 7--13 letters with 50 attempts in a common side-channel attack benchmark. Our results suggest that it not always sufficient to rely on isolation mechanisms such as TrustZone to protect user input. We propose and discuss hardware, operating-system and application-level mechanisms to block this attack more effectively. Mobile devices may need a richer capability model, a more user-friendly notification system for sensor usage and a more thorough evaluation of the information leaked by the underlying hardware.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.11137v1-abstract-full').style.display = 'none'; document.getElementById('1903.11137v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 March, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Paper built on the MPhil thesis of Ilia Shumailov. 2017</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.04480">arXiv:1903.04480</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.04480">pdf</a>, <a href="https://arxiv.org/format/1903.04480">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Video Generation from Single Semantic Label Map
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Pan%2C+J">Junting Pan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+C">Chengyu Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jia%2C+X">Xu Jia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shao%2C+J">Jing Shao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sheng%2C+L">Lu Sheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xiaogang Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.04480v1-abstract-short" style="display: inline;">
        This paper proposes the novel task of video generation conditioned on a SINGLE semantic label map, which provides a good balance between flexibility and quality in the generation process. Different from typical end-to-end approaches, which model both scene content and dynamics in a single step, we propose to decompose this difficult task into two sub-problems. As current image generation methods d&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.04480v1-abstract-full').style.display = 'inline'; document.getElementById('1903.04480v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.04480v1-abstract-full" style="display: none;">
        This paper proposes the novel task of video generation conditioned on a SINGLE semantic label map, which provides a good balance between flexibility and quality in the generation process. Different from typical end-to-end approaches, which model both scene content and dynamics in a single step, we propose to decompose this difficult task into two sub-problems. As current image generation methods do better than video generation in terms of detail, we synthesize high quality content by only generating the first frame. Then we animate the scene based on its semantic meaning to obtain the temporally coherent video, giving us excellent results overall. We employ a cVAE for predicting optical flow as a beneficial intermediate step to generate a video sequence conditioned on the initial single frame. A semantic label map is integrated into the flow prediction module to achieve major improvements in the image-to-video generation process. Extensive experiments on the Cityscapes dataset show that our method outperforms all competing methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.04480v1-abstract-full').style.display = 'none'; document.getElementById('1903.04480v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 March, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Paper accepted at CVPR 2019. Source code and models available at https://github.com/junting/seg2vid/tree/master</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1902.10949">arXiv:1902.10949</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1902.10949">pdf</a>, <a href="https://arxiv.org/format/1902.10949">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamic Multi-path Neural Network
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Su%2C+Y">Yingcheng Su</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+S">Shunfeng Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Y">Yichao Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+X">Xuebo Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Su%2C+T">Tian Su</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+D">Ding Liang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1902.10949v2-abstract-short" style="display: inline;">
        Although deeper and larger neural networks have achieved better performance, the complex network structure and increasing computational cost cannot meet the demands of many resource-constrained applications. An effective way to address this problem is to make use of dynamic inference mechanism. Existing methods usually choose to execute or skip an entire specific layer, which can only alter the de&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.10949v2-abstract-full').style.display = 'inline'; document.getElementById('1902.10949v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1902.10949v2-abstract-full" style="display: none;">
        Although deeper and larger neural networks have achieved better performance, the complex network structure and increasing computational cost cannot meet the demands of many resource-constrained applications. An effective way to address this problem is to make use of dynamic inference mechanism. Existing methods usually choose to execute or skip an entire specific layer, which can only alter the depth of the network. In this paper, we propose a novel method called Dynamic Multi-path Neural Network (DMNN), which provides more path selection choices in terms of network width and depth during inference. The inference path of the network is determined by a controller, which takes into account both historical and object category information. The proposed method can be easily incorporated into most modern network architectures. Experimental results on ImageNet and CIFAR-100 demonstrate the superiority of our method on both efficiency and overall classification accuracy. To be specific, we integrate DMNN into ResNet-101 and find that our method significantly outperforms its counterparts with an encouraging 45.1% FLOPs reduction.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.10949v2-abstract-full').style.display = 'none'; document.getElementById('1902.10949v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 February, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1902.09711">arXiv:1902.09711</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1902.09711">pdf</a>, <a href="https://arxiv.org/ps/1902.09711">ps</a>, <a href="https://arxiv.org/format/1902.09711">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Detecting Data Errors with Statistical Constraints
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J+N">Jing Nathan Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schulte%2C+O">Oliver Schulte</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+J">Jiannan Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cheng%2C+R">Reynold Cheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1902.09711v1-abstract-short" style="display: inline;">
        A powerful approach to detecting erroneous data is to check which potentially dirty data records are incompatible with a user's domain knowledge. Previous approaches allow the user to specify domain knowledge in the form of logical constraints (e.g., functional dependency and denial constraints). We extend the constraint-based approach by introducing a novel class of statistical constraints (SCs).&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.09711v1-abstract-full').style.display = 'inline'; document.getElementById('1902.09711v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1902.09711v1-abstract-full" style="display: none;">
        A powerful approach to detecting erroneous data is to check which potentially dirty data records are incompatible with a user&#39;s domain knowledge. Previous approaches allow the user to specify domain knowledge in the form of logical constraints (e.g., functional dependency and denial constraints). We extend the constraint-based approach by introducing a novel class of statistical constraints (SCs). An SC treats each column as a random variable, and enforces an independence or dependence relationship between two (or a few) random variables. Statistical constraints are expressive, allowing the user to specify a wide range of domain knowledge, beyond traditional integrity constraints. Furthermore, they work harmoniously with downstream statistical modeling. We develop CODED, an SC-Oriented Data Error Detection system that supports three key tasks: (1) Checking whether an SC is violated or not on a given dataset, (2) Identify the top-k records that contribute the most to the violation of an SC, and (3) Checking whether a set of input SCs have conflicts or not. We present effective solutions for each task. Experiments on synthetic and real-world data illustrate how SCs apply to error detection, and provide evidence that CODED performs better than state-of-the-art approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.09711v1-abstract-full').style.display = 'none'; document.getElementById('1902.09711v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 February, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1902.08546">arXiv:1902.08546</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1902.08546">pdf</a>, <a href="https://arxiv.org/format/1902.08546">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/ICIP.2018.8451133">10.1109/ICIP.2018.8451133 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Image Aesthetics Assessment Using Composite Features from off-the-Shelf Deep Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+X">Xin Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jia Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fan%2C+C">Cien Fan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1902.08546v1-abstract-short" style="display: inline;">
        Deep convolutional neural networks have recently achieved great success on image aesthetics assessment task. In this paper, we propose an efficient method which takes the global, local and scene-aware information of images into consideration and exploits the composite features extracted from corresponding pretrained deep learning models to classify the derived features with support vector machine.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.08546v1-abstract-full').style.display = 'inline'; document.getElementById('1902.08546v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1902.08546v1-abstract-full" style="display: none;">
        Deep convolutional neural networks have recently achieved great success on image aesthetics assessment task. In this paper, we propose an efficient method which takes the global, local and scene-aware information of images into consideration and exploits the composite features extracted from corresponding pretrained deep learning models to classify the derived features with support vector machine. Contrary to popular methods that require fine-tuning or training a new model from scratch, our training-free method directly takes the deep features generated by off-the-shelf models for image classification and scene recognition. Also, we analyzed the factors that could influence the performance from two aspects: the architecture of the deep neural network and the contribution of local and scene-aware information. It turns out that deep residual network could produce more aesthetics-aware image representation and composite features lead to the improvement of overall performance. Experiments on common large-scale aesthetics assessment benchmarks demonstrate that our method outperforms the state-of-the-art results in photo aesthetics assessment.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.08546v1-abstract-full').style.display = 'none'; document.getElementById('1902.08546v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 February, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ICIP 2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1902.07814">arXiv:1902.07814</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1902.07814">pdf</a>, <a href="https://arxiv.org/format/1902.07814">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Dual Retrieval Module for Semi-supervised Relation Extraction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+H">Hongtao Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jun Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qu%2C+M">Meng Qu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+X">Xiang Ren</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1902.07814v2-abstract-short" style="display: inline;">
        Relation extraction is an important task in structuring content of text data, and becomes especially challenging when learning with weak supervision---where only a limited number of labeled sentences are given and a large number of unlabeled sentences are available. Most existing work exploits unlabeled data based on the ideas of self-training (i.e., bootstrapping a model) and multi-view learning&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.07814v2-abstract-full').style.display = 'inline'; document.getElementById('1902.07814v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1902.07814v2-abstract-full" style="display: none;">
        Relation extraction is an important task in structuring content of text data, and becomes especially challenging when learning with weak supervision---where only a limited number of labeled sentences are given and a large number of unlabeled sentences are available. Most existing work exploits unlabeled data based on the ideas of self-training (i.e., bootstrapping a model) and multi-view learning (e.g., ensembling multiple model variants). However, these methods either suffer from the issue of semantic drift, or do not fully capture the problem characteristics of relation extraction. In this paper, we leverage a key insight that retrieving sentences expressing a relation is a dual task of predicting relation label for a given sentence---two tasks are complementary to each other and can be optimized jointly for mutual enhancement. To model this intuition, we propose DualRE, a principled framework that introduces a retrieval module which is jointly trained with the original relation prediction module. In this way, high-quality samples selected by retrieval module from unlabeled data can be used to improve prediction module, and vice versa. Experimental results\footnote{\small Code and data can be found at \url{https://github.com/INK-USC/DualRE}.} on two public datasets as well as case studies demonstrate the effectiveness of the DualRE approach.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.07814v2-abstract-full').style.display = 'none'; document.getElementById('1902.07814v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 February, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 February, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 2-page references. Accepted to The Web Conference 2019</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1902.06854">arXiv:1902.06854</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1902.06854">pdf</a>, <a href="https://arxiv.org/format/1902.06854">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        WIDER Face and Pedestrian Challenge 2018: Methods and Results
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Loy%2C+C+C">Chen Change Loy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+D">Dahua Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ouyang%2C+W">Wanli Ouyang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xiong%2C+Y">Yuanjun Xiong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+S">Shuo Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Q">Qingqiu Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+D">Dongzhan Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xia%2C+W">Wei Xia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Q">Quanquan Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+P">Ping Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+J">Jianfeng Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Z">Zuoxin Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yuan%2C+Y">Ye Yuan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+B">Boxun Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shao%2C+S">Shuai Shao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+G">Gang Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+F">Fangyun Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ming%2C+X">Xiang Ming</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+D">Dong Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+S">Shifeng Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chi%2C+C">Cheng Chi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lei%2C+Z">Zhen Lei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+S+Z">Stan Z. Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+H">Hongkai Zhang</a>
      , et al. (27 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1902.06854v1-abstract-short" style="display: inline;">
        This paper presents a review of the 2018 WIDER Challenge on Face and Pedestrian. The challenge focuses on the problem of precise localization of human faces and bodies, and accurate association of identities. It comprises of three tracks: (i) WIDER Face which aims at soliciting new approaches to advance the state-of-the-art in face detection, (ii) WIDER Pedestrian which aims to find effective and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.06854v1-abstract-full').style.display = 'inline'; document.getElementById('1902.06854v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1902.06854v1-abstract-full" style="display: none;">
        This paper presents a review of the 2018 WIDER Challenge on Face and Pedestrian. The challenge focuses on the problem of precise localization of human faces and bodies, and accurate association of identities. It comprises of three tracks: (i) WIDER Face which aims at soliciting new approaches to advance the state-of-the-art in face detection, (ii) WIDER Pedestrian which aims to find effective and efficient approaches to address the problem of pedestrian detection in unconstrained environments, and (iii) WIDER Person Search which presents an exciting challenge of searching persons across 192 movies. In total, 73 teams made valid submissions to the challenge tracks. We summarize the winning solutions for all three tracks. and present discussions on open problems and potential research directions in these topics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.06854v1-abstract-full').style.display = 'none'; document.getElementById('1902.06854v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 February, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Report of ECCV 2018 workshop: WIDER Face and Pedestrian Challenge</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1902.03098">arXiv:1902.03098</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1902.03098">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Measuring Long-term Impact of Ads on LinkedIn Feed
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jinyun Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tiwana%2C+B">Birjodh Tiwana</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ghosh%2C+S">Souvik Ghosh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+H">Haishan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chatterjee%2C+S">Shaunak Chatterjee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1902.03098v1-abstract-short" style="display: inline;">
        Organic updates (from a member's network) and sponsored updates (or ads, from advertisers) together form the newsfeed on LinkedIn. The newsfeed, the default homepage for members, attracts them to engage, brings them value and helps LinkedIn grow. Engagement and Revenue on feed are two critical, yet often conflicting objectives. Hence, it is important to design a good Revenue-Engagement Tradeoff (R&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.03098v1-abstract-full').style.display = 'inline'; document.getElementById('1902.03098v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1902.03098v1-abstract-full" style="display: none;">
        Organic updates (from a member&#39;s network) and sponsored updates (or ads, from advertisers) together form the newsfeed on LinkedIn. The newsfeed, the default homepage for members, attracts them to engage, brings them value and helps LinkedIn grow. Engagement and Revenue on feed are two critical, yet often conflicting objectives. Hence, it is important to design a good Revenue-Engagement Tradeoff (RENT) mechanism to blend ads in the feed. In this paper, we design experiments to understand how members&#39; behavior evolve over time given different ads experiences. These experiences vary on ads density, while the quality of ads (ensured by relevance models) is held constant. Our experiments have been conducted on randomized member buckets and we use two experimental designs to measure the short term and long term effects of the various treatments. Based on the first three months&#39; data, we observe that the long term impact is at a much smaller scale than the short term impact in our application. Furthermore, we observe different member cohorts (based on user activity level) adapt and react differently over time.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.03098v1-abstract-full').style.display = 'none'; document.getElementById('1902.03098v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 January, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2019.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        2018 Conference on Digital Experimentation (CODE)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1901.10550">arXiv:1901.10550</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1901.10550">pdf</a>, <a href="https://arxiv.org/format/1901.10550">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Methodology">stat.ME</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Personalization and Optimization of Decision Parameters via Heterogenous Causal Effects
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tu%2C+Y">Ye Tu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Basu%2C+K">Kinjal Basu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jinyun Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tiwana%2C+B">Birjodh Tiwana</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chatterjee%2C+S">Shaunak Chatterjee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1901.10550v2-abstract-short" style="display: inline;">
        Randomized experimentation (also known as A/B testing or bucket testing) is very commonly used in the internet industry to measure the effect of a new treatment. Often, the decision on the basis of such A/B testing is to ramp the treatment variant that did best for the entire population. However, the effect of any given treatment varies across experimental units, and choosing a single variant to r&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.10550v2-abstract-full').style.display = 'inline'; document.getElementById('1901.10550v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1901.10550v2-abstract-full" style="display: none;">
        Randomized experimentation (also known as A/B testing or bucket testing) is very commonly used in the internet industry to measure the effect of a new treatment. Often, the decision on the basis of such A/B testing is to ramp the treatment variant that did best for the entire population. However, the effect of any given treatment varies across experimental units, and choosing a single variant to ramp to the whole population can be quite suboptimal. In this work, we propose a method which automatically identifies the collection of cohorts exhibiting heterogeneous treatment effect (using causal trees). We then use stochastic optimization to identify the optimal treatment variant in each cohort. We use two real-life examples - one related to serving notifications and the other related to modulating ads density on feed. In both examples, using offline simulation and online experimentation, we demonstrate the benefits of our approach. At the time of writing this paper, the method described has been deployed on the LinkedIn Ads and Notifications system.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.10550v2-abstract-full').style.display = 'none'; document.getElementById('1901.10550v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 February, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 January, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 Pages, 5 Figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1901.06783">arXiv:1901.06783</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1901.06783">pdf</a>, <a href="https://arxiv.org/format/1901.06783">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamic Curriculum Learning for Imbalanced Data Classification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yiru Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gan%2C+W">Weihao Gan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+W">Wei Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1901.06783v1-abstract-short" style="display: inline;">
        Human attribute analysis is a challenging task in the field of computer vision, since the data is largely imbalance-distributed. Common techniques such as re-sampling and cost-sensitive learning require prior-knowledge to train the system. To address this problem, we propose a unified framework called Dynamic Curriculum Learning (DCL) to online adaptively adjust the sampling strategy and loss lear&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.06783v1-abstract-full').style.display = 'inline'; document.getElementById('1901.06783v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1901.06783v1-abstract-full" style="display: none;">
        Human attribute analysis is a challenging task in the field of computer vision, since the data is largely imbalance-distributed. Common techniques such as re-sampling and cost-sensitive learning require prior-knowledge to train the system. To address this problem, we propose a unified framework called Dynamic Curriculum Learning (DCL) to online adaptively adjust the sampling strategy and loss learning in single batch, which resulting in better generalization and discrimination. Inspired by the curriculum learning, DCL consists of two level curriculum schedulers: (1) sampling scheduler not only manages the data distribution from imbalanced to balanced but also from easy to hard; (2) loss scheduler controls the learning importance between classification and metric learning loss. Learning from these two schedulers, we demonstrate our DCL framework with the new state-of-the-art performance on the widely used face attribute dataset CelebA and pedestrian attribute dataset RAP.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.06783v1-abstract-full').style.display = 'none'; document.getElementById('1901.06783v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 January, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1901.06129">arXiv:1901.06129</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1901.06129">pdf</a>, <a href="https://arxiv.org/format/1901.06129">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Object Tracking with Multiple Cues and Switcher-Aware Classification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Feng%2C+W">Weitao Feng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+Z">Zhihao Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+W">Wei Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ouyang%2C+W">Wanli Ouyang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1901.06129v1-abstract-short" style="display: inline;">
        In this paper, we propose a unified Multi-Object Tracking (MOT) framework learning to make full use of long term and short term cues for handling complex cases in MOT scenes. Besides, for better association, we propose switcher-aware classification (SAC), which takes the potential identity-switch causer (switcher) into consideration. Specifically, the proposed framework includes a Single Object Tr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.06129v1-abstract-full').style.display = 'inline'; document.getElementById('1901.06129v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1901.06129v1-abstract-full" style="display: none;">
        In this paper, we propose a unified Multi-Object Tracking (MOT) framework learning to make full use of long term and short term cues for handling complex cases in MOT scenes. Besides, for better association, we propose switcher-aware classification (SAC), which takes the potential identity-switch causer (switcher) into consideration. Specifically, the proposed framework includes a Single Object Tracking (SOT) sub-net to capture short term cues, a re-identification (ReID) sub-net to extract long term cues and a switcher-aware classifier to make matching decisions using extracted features from the main target and the switcher. Short term cues help to find false negatives, while long term cues avoid critical mistakes when occlusion happens, and the SAC learns to combine multiple cues in an effective way and improves robustness. The method is evaluated on the challenging MOT benchmarks and achieves the state-of-the-art results.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.06129v1-abstract-full').style.display = 'none'; document.getElementById('1901.06129v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 January, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 7 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1812.11703">arXiv:1812.11703</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1812.11703">pdf</a>, <a href="https://arxiv.org/format/1812.11703">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SiamRPN++: Evolution of Siamese Visual Tracking with Very Deep Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+B">Bo Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+W">Wei Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Q">Qiang Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+F">Fangyi Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xing%2C+J">Junliang Xing</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1812.11703v1-abstract-short" style="display: inline;">
        Siamese network based trackers formulate tracking as convolutional feature cross-correlation between target template and searching region. However, Siamese trackers still have accuracy gap compared with state-of-the-art algorithms and they cannot take advantage of feature from deep networks, such as ResNet-50 or deeper. In this work we prove the core reason comes from the lack of strict translatio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1812.11703v1-abstract-full').style.display = 'inline'; document.getElementById('1812.11703v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1812.11703v1-abstract-full" style="display: none;">
        Siamese network based trackers formulate tracking as convolutional feature cross-correlation between target template and searching region. However, Siamese trackers still have accuracy gap compared with state-of-the-art algorithms and they cannot take advantage of feature from deep networks, such as ResNet-50 or deeper. In this work we prove the core reason comes from the lack of strict translation invariance. By comprehensive theoretical analysis and experimental validations, we break this restriction through a simple yet effective spatial aware sampling strategy and successfully train a ResNet-driven Siamese tracker with significant performance gain. Moreover, we propose a new model architecture to perform depth-wise and layer-wise aggregations, which not only further improves the accuracy but also reduces the model size. We conduct extensive ablation studies to demonstrate the effectiveness of the proposed tracker, which obtains currently the best results on four large tracking benchmarks, including OTB2015, VOT2018, UAV123, and LaSOT. Our model will be released to facilitate further studies based on this problem.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1812.11703v1-abstract-full').style.display = 'none'; document.getElementById('1812.11703v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 December, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1812.05285">arXiv:1812.05285</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1812.05285">pdf</a>, <a href="https://arxiv.org/format/1812.05285">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        IRLAS: Inverse Reinforcement Learning for Architecture Search
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+M">Minghao Guo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhong%2C+Z">Zhao Zhong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+W">Wei Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+D">Dahua Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1812.05285v3-abstract-short" style="display: inline;">
        In this paper, we propose an inverse reinforcement learning method for architecture search (IRLAS), which trains an agent to learn to search network structures that are topologically inspired by human-designed network. Most existing architecture search approaches totally neglect the topological characteristics of architectures, which results in complicated architecture with a high inference latenc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1812.05285v3-abstract-full').style.display = 'inline'; document.getElementById('1812.05285v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1812.05285v3-abstract-full" style="display: none;">
        In this paper, we propose an inverse reinforcement learning method for architecture search (IRLAS), which trains an agent to learn to search network structures that are topologically inspired by human-designed network. Most existing architecture search approaches totally neglect the topological characteristics of architectures, which results in complicated architecture with a high inference latency. Motivated by the fact that human-designed networks are elegant in topology with a fast inference speed, we propose a mirror stimuli function inspired by biological cognition theory to extract the abstract topological knowledge of an expert human-design network (ResNeXt). To avoid raising a too strong prior over the search space, we introduce inverse reinforcement learning to train the mirror stimuli function and exploit it as a heuristic guidance for architecture search, easily generalized to different architecture search algorithms. On CIFAR-10, the best architecture searched by our proposed IRLAS achieves 2.60% error rate. For ImageNet mobile setting, our model achieves a state-of-the-art top-1 accuracy 75.28%, while being 2~4x faster than most auto-generated architectures. A fast version of this model achieves 10% faster than MobileNetV2, while maintaining a higher accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1812.05285v3-abstract-full').style.display = 'none'; document.getElementById('1812.05285v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 March, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 December, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1812.01243">arXiv:1812.01243</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1812.01243">pdf</a>, <a href="https://arxiv.org/format/1812.01243">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Decomposed Attention: Self-Attention with Linear Complexities
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Z">Zhuoran Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+M">Mingyuan Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yi%2C+S">Shuai Yi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+H">Haiyu Zhao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1812.01243v4-abstract-short" style="display: inline;">
        Recent works have been applying self-attention to various fields in computer vision and natural language processing. However, the memory and computational demands of existing self-attention operations grow quadratically with the spatiotemporal size of the input. This prohibits the application of self-attention on large inputs, e.g., long sequences, high-definition images, or large videos. To remed&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1812.01243v4-abstract-full').style.display = 'inline'; document.getElementById('1812.01243v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1812.01243v4-abstract-full" style="display: none;">
        Recent works have been applying self-attention to various fields in computer vision and natural language processing. However, the memory and computational demands of existing self-attention operations grow quadratically with the spatiotemporal size of the input. This prohibits the application of self-attention on large inputs, e.g., long sequences, high-definition images, or large videos. To remedy this drawback, this paper proposes a novel decomposed attention (DA) module with substantially less memory and computational consumption. The resource-efficiency allows more widespread and flexible application. Empirical evaluations on object recognition demonstrated the effectiveness of these advantages. DA-augmented models achieved state-of-the-art performance for object recognition on MS-COCO 2017 and significant improvement for image classification on ImageNet. Further, the resource-efficiency of DA democratizes self-attention to fields where the prohibitively high costs have been preventing its application. The state-of-the-art result for stereo depth estimation on the Scene Flow dataset exemplified this.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1812.01243v4-abstract-full').style.display = 'none'; document.getElementById('1812.01243v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 March, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 December, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.12030">arXiv:1811.12030</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.12030">pdf</a>, <a href="https://arxiv.org/format/1811.12030">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Grid R-CNN
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+X">Xin Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+B">Buyu Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yue%2C+Y">Yuxin Yue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Q">Quanquan Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.12030v1-abstract-short" style="display: inline;">
        This paper proposes a novel object detection framework named Grid R-CNN, which adopts a grid guided localization mechanism for accurate object detection. Different from the traditional regression based methods, the Grid R-CNN captures the spatial information explicitly and enjoys the position sensitive property of fully convolutional architecture. Instead of using only two independent points, we d&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.12030v1-abstract-full').style.display = 'inline'; document.getElementById('1811.12030v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.12030v1-abstract-full" style="display: none;">
        This paper proposes a novel object detection framework named Grid R-CNN, which adopts a grid guided localization mechanism for accurate object detection. Different from the traditional regression based methods, the Grid R-CNN captures the spatial information explicitly and enjoys the position sensitive property of fully convolutional architecture. Instead of using only two independent points, we design a multi-point supervision formulation to encode more clues in order to reduce the impact of inaccurate prediction of specific points. To take the full advantage of the correlation of points in a grid, we propose a two-stage information fusion strategy to fuse feature maps of neighbor grid points. The grid guided localization approach is easy to be extended to different state-of-the-art detection frameworks. Grid R-CNN leads to high quality object localization, and experiments demonstrate that it achieves a 4.1% AP gain at IoU=0.8 and a 10.0% AP gain at IoU=0.9 on COCO benchmark compared to Faster R-CNN with Res50 backbone and FPN architecture.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.12030v1-abstract-full').style.display = 'none'; document.getElementById('1811.12030v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 November, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.02454">arXiv:1811.02454</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.02454">pdf</a>, <a href="https://arxiv.org/format/1811.02454">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Synaptic Strength For Convolutional Neural Network
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+C">Chen Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhong%2C+Z">Zhao Zhong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+W">Wei Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.02454v1-abstract-short" style="display: inline;">
        Convolutional Neural Networks(CNNs) are both computation and memory intensive which hindered their deployment in mobile devices. Inspired by the relevant concept in neural science literature, we propose Synaptic Pruning: a data-driven method to prune connections between input and output feature maps with a newly proposed class of parameters called Synaptic Strength. Synaptic Strength is designed t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.02454v1-abstract-full').style.display = 'inline'; document.getElementById('1811.02454v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.02454v1-abstract-full" style="display: none;">
        Convolutional Neural Networks(CNNs) are both computation and memory intensive which hindered their deployment in mobile devices. Inspired by the relevant concept in neural science literature, we propose Synaptic Pruning: a data-driven method to prune connections between input and output feature maps with a newly proposed class of parameters called Synaptic Strength. Synaptic Strength is designed to capture the importance of a connection based on the amount of information it transports. Experiment results show the effectiveness of our approach. On CIFAR-10, we prune connections for various CNN models with up to 96% , which results in significant size reduction and computation saving. Further evaluation on ImageNet demonstrates that synaptic pruning is able to discover efficient models which is competitive to state-of-the-art compact CNNs such as MobileNet-V2 and NasNet-Mobile. Our contribution is summarized as following: (1) We introduce Synaptic Strength, a new class of parameters for CNNs to indicate the importance of each connections. (2) Our approach can prune various CNNs with high compression without compromising accuracy. (3) Further investigation shows, the proposed Synaptic Strength is a better indicator for kernel pruning compared with the previous approach in both empirical result and theoretical analysis.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.02454v1-abstract-full').style.display = 'none'; document.getElementById('1811.02454v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 November, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by NIPS 2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.02198">arXiv:1811.02198</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.02198">pdf</a>, <a href="https://arxiv.org/format/1811.02198">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Collaborative Filtering with Stability
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+D">Dongsheng Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+C">Chao Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lv%2C+Q">Qin Lv</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junchi Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shang%2C+L">Li Shang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chu%2C+S+M">Stephen M. Chu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.02198v1-abstract-short" style="display: inline;">
        Collaborative filtering (CF) is a popular technique in today's recommender systems, and matrix approximation-based CF methods have achieved great success in both rating prediction and top-N recommendation tasks. However, real-world user-item rating matrices are typically sparse, incomplete and noisy, which introduce challenges to the algorithm stability of matrix approximation, i.e., small changes&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.02198v1-abstract-full').style.display = 'inline'; document.getElementById('1811.02198v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.02198v1-abstract-full" style="display: none;">
        Collaborative filtering (CF) is a popular technique in today&#39;s recommender systems, and matrix approximation-based CF methods have achieved great success in both rating prediction and top-N recommendation tasks. However, real-world user-item rating matrices are typically sparse, incomplete and noisy, which introduce challenges to the algorithm stability of matrix approximation, i.e., small changes in the training data may significantly change the models. As a result, existing matrix approximation solutions yield low generalization performance, exhibiting high error variance on the training data, and minimizing the training error may not guarantee error reduction on the test data. This paper investigates the algorithm stability problem of matrix approximation methods and how to achieve stable collaborative filtering via stable matrix approximation. We present a new algorithm design framework, which (1) introduces new optimization objectives to guide stable matrix approximation algorithm design, and (2) solves the optimization problem to obtain stable approximation solutions with good generalization performance. Experimental results on real-world datasets demonstrate that the proposed method can achieve better accuracy compared with state-of-the-art matrix approximation methods and ensemble methods in both rating prediction and top-N recommendation tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.02198v1-abstract-full').style.display = 'none'; document.getElementById('1811.02198v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 November, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.12387">arXiv:1810.12387</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.12387">pdf</a>, <a href="https://arxiv.org/format/1810.12387">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Language Modeling with Sparse Product of Sememe Experts
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+Y">Yihong Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jun Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+H">Hao Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zhiyuan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+R">Ruobing Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+M">Maosong Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+F">Fen Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+L">Leyu Lin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.12387v1-abstract-short" style="display: inline;">
        Most language modeling methods rely on large-scale data to statistically learn the sequential patterns of words. In this paper, we argue that words are atomic language units but not necessarily atomic semantic units. Inspired by HowNet, we use sememes, the minimum semantic units in human languages, to represent the implicit semantics behind words for language modeling, named Sememe-Driven Language&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.12387v1-abstract-full').style.display = 'inline'; document.getElementById('1810.12387v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.12387v1-abstract-full" style="display: none;">
        Most language modeling methods rely on large-scale data to statistically learn the sequential patterns of words. In this paper, we argue that words are atomic language units but not necessarily atomic semantic units. Inspired by HowNet, we use sememes, the minimum semantic units in human languages, to represent the implicit semantics behind words for language modeling, named Sememe-Driven Language Model (SDLM). More specifically, to predict the next word, SDLM first estimates the sememe distribution gave textual context. Afterward, it regards each sememe as a distinct semantic expert, and these experts jointly identify the most probable senses and the corresponding word. In this way, SDLM enables language models to work beyond word-level manipulation to fine-grained sememe-level semantics and offers us more powerful tools to fine-tune language models and improve the interpretability as well as the robustness of language models. Experiments on language modeling and the downstream application of headline gener- ation demonstrate the significant effect of SDLM. Source code and data used in the experiments can be accessed at https:// github.com/thunlp/SDLM-pytorch.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.12387v1-abstract-full').style.display = 'none'; document.getElementById('1810.12387v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 October, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">EMNLP 2018. The first three authors contribute equally</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.11199">arXiv:1810.11199</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.11199">pdf</a>, <a href="https://arxiv.org/format/1810.11199">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Optimal Offloading and Resource Allocation in Mobile-Edge Computing with Inter-user Task Dependency
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jia Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bi%2C+S">Suzhi Bi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y+A">Ying-Jun Angela Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tao%2C+M">Meixia Tao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.11199v1-abstract-short" style="display: inline;">
        Mobile-edge computing (MEC) has recently emerged as a cost-effective paradigm to enhance the computing capability of hardware-constrained wireless devices (WDs). In this paper, we consider a two-user MEC network, where each WD has a sequence of tasks to execute. In particular, we consider task dependency between the two WDs, where the input of a task at one WD requires the final task output at the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.11199v1-abstract-full').style.display = 'inline'; document.getElementById('1810.11199v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.11199v1-abstract-full" style="display: none;">
        Mobile-edge computing (MEC) has recently emerged as a cost-effective paradigm to enhance the computing capability of hardware-constrained wireless devices (WDs). In this paper, we consider a two-user MEC network, where each WD has a sequence of tasks to execute. In particular, we consider task dependency between the two WDs, where the input of a task at one WD requires the final task output at the other WD. Under the considered task-dependency model, we study the optimal task offloading policy and resource allocation (e.g., on offloading transmit power and local CPU frequencies) that minimize the weighted sum of the WDs&#39; energy consumption and task execution time. The problem is challenging due to the combinatorial nature of the offloading decisions among all tasks and the strong coupling with resource allocation. To tackle this problem, we first assume that the offloading decisions are given and derive the closed-form expressions of the optimal offloading transmit power and local CPU frequencies. Then, an efficient bi-section search method is proposed to obtain the optimal solutions. Furthermore, we prove that the optimal offloading decisions follow an one-climb policy, based on which a reduced-complexity Gibbs Sampling algorithm is proposed to obtain the optimal offloading decisions. Numerical results show that the proposed method can significantly outperform the other representative benchmarks and efficiently achieve low complexity with respect to the call graph size.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.11199v1-abstract-full').style.display = 'none'; document.getElementById('1810.11199v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 October, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper has been submitted for publication in IEEE Transactions on Wireless Communications</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.02954">arXiv:1810.02954</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.02954">pdf</a>, <a href="https://arxiv.org/format/1810.02954">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Statistics Theory">math.ST</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Methodology">stat.ME</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adapting to Unknown Noise Distribution in Matrix Denoising
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Montanari%2C+A">Andrea Montanari</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ruan%2C+F">Feng Ruan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jun Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.02954v3-abstract-short" style="display: inline;">
        We consider the problem of estimating an unknown matrix $\boldsymbol{X}\in {\mathbb R}^{m\times n}$, from observations $\boldsymbol{Y} = \boldsymbol{X}+\boldsymbol{W}$ where $\boldsymbol{W}$ is a noise matrix with independent and identically distributed entries, as to minimize estimation error measured in operator norm. Assuming that the underlying signal $\boldsymbol{X}$ is low-rank and incoheren&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.02954v3-abstract-full').style.display = 'inline'; document.getElementById('1810.02954v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.02954v3-abstract-full" style="display: none;">
        We consider the problem of estimating an unknown matrix $\boldsymbol{X}\in {\mathbb R}^{m\times n}$, from observations $\boldsymbol{Y} = \boldsymbol{X}+\boldsymbol{W}$ where $\boldsymbol{W}$ is a noise matrix with independent and identically distributed entries, as to minimize estimation error measured in operator norm. Assuming that the underlying signal $\boldsymbol{X}$ is low-rank and incoherent with respect to the canonical basis, we prove that minimax risk is equivalent to $(\sqrt{m}\vee\sqrt{n})/\sqrt{I_W}$ in the high-dimensional limit $m,n\to\infty$, where $I_W$ is the Fisher information of the noise. Crucially, we develop an efficient procedure that achieves this risk, adaptively over the noise distribution (under certain regularity assumptions).
  Letting $\boldsymbol{X} = \boldsymbol{U}{\boldsymbol}\boldsymbol{V}^{\sf T}$ --where $\boldsymbol{U}\in {\mathbb R}^{m\times r}$, $\boldsymbol{V}\in{\mathbb R}^{n\times r}$ are orthogonal, and $r$ is kept fixed as $m,n\to\infty$-- we use our method to estimate $\boldsymbol{U}$, $\boldsymbol{V}$. Standard spectral methods provide non-trivial estimates of the factors $\boldsymbol{U},\boldsymbol{V}$ (weak recovery) only if the singular values of $\boldsymbol{X}$ are larger than $(mn)^{1/4}{\rm Var}(W_{11})^{1/2}$. We prove that the new approach achieves weak recovery down to the the information-theoretically optimal threshold $(mn)^{1/4}I_W^{1/2}$.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.02954v3-abstract-full').style.display = 'none'; document.getElementById('1810.02954v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 November, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 October, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1809.05884">arXiv:1809.05884</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1809.05884">pdf</a>, <a href="https://arxiv.org/format/1809.05884">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3240508.3240567">10.1145/3240508.3240567 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Label Image Classification via Knowledge Distillation from Weakly-Supervised Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yongcheng Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sheng%2C+L">Lu Sheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shao%2C+J">Jing Shao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xiang%2C+S">Shiming Xiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pan%2C+C">Chunhong Pan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1809.05884v2-abstract-short" style="display: inline;">
        Multi-label image classification is a fundamental but challenging task towards general visual understanding. Existing methods found the region-level cues (e.g., features from RoIs) can facilitate multi-label classification. Nevertheless, such methods usually require laborious object-level annotations (i.e., object labels and bounding boxes) for effective learning of the object-level visual feature&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.05884v2-abstract-full').style.display = 'inline'; document.getElementById('1809.05884v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1809.05884v2-abstract-full" style="display: none;">
        Multi-label image classification is a fundamental but challenging task towards general visual understanding. Existing methods found the region-level cues (e.g., features from RoIs) can facilitate multi-label classification. Nevertheless, such methods usually require laborious object-level annotations (i.e., object labels and bounding boxes) for effective learning of the object-level visual features. In this paper, we propose a novel and efficient deep framework to boost multi-label classification by distilling knowledge from weakly-supervised detection task without bounding box annotations. Specifically, given the image-level annotations, (1) we first develop a weakly-supervised detection (WSD) model, and then (2) construct an end-to-end multi-label image classification framework augmented by a knowledge distillation module that guides the classification model by the WSD model according to the class-level predictions for the whole image and the object-level visual features for object RoIs. The WSD model is the teacher model and the classification model is the student model. After this cross-task knowledge distillation, the performance of the classification model is significantly improved and the efficiency is maintained since the WSD model can be safely discarded in the test phase. Extensive experiments on two large-scale datasets (MS-COCO and NUS-WIDE) show that our framework achieves superior performances over the state-of-the-art methods on both performance and efficiency.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.05884v2-abstract-full').style.display = 'none'; document.getElementById('1809.05884v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 February, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 September, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted by ACM Multimedia 2018, 9 pages, 4 figures, 5 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1809.05299">arXiv:1809.05299</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1809.05299">pdf</a>, <a href="https://arxiv.org/ps/1809.05299">ps</a>, <a href="https://arxiv.org/format/1809.05299">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Systems and Control">cs.SY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An On-line Design of Physical Watermarks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+H">Hanxiao Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jiaqi Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mo%2C+Y">Yilin Mo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Johansson%2C+K+H">Karl Henrik Johansson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1809.05299v1-abstract-short" style="display: inline;">
        This paper considers the problem to design physical watermark signals to protect a control system against replay attacks. We first define the replay attack model, where an adversary replays the previous sensory data in order to fool the system. The physical watermarking scheme, which leverages a random control input as a watermark, to detect the replay attack is introduced. The optimal watermark s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.05299v1-abstract-full').style.display = 'inline'; document.getElementById('1809.05299v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1809.05299v1-abstract-full" style="display: none;">
        This paper considers the problem to design physical watermark signals to protect a control system against replay attacks. We first define the replay attack model, where an adversary replays the previous sensory data in order to fool the system. The physical watermarking scheme, which leverages a random control input as a watermark, to detect the replay attack is introduced. The optimal watermark signal design problem is then proposed as an optimization problem, which achieves the optimal trade-off between the control performance and attack detection performance. For the system with unknown parameters, we provide a procedure to asymptotically derive the optimal watermarking signal. Numerical examples are provided to illustrate the effectiveness of the proposed strategy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.05299v1-abstract-full').style.display = 'none'; document.getElementById('1809.05299v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 September, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1809.04258">arXiv:1809.04258</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1809.04258">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Ontology-Based Artificial Intelligence Model for Medicine Side-Effect Prediction: Taking Traditional Chinese Medicine as An Example
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Z">Zeheng Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+K">Kun Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cao%2C+J">Jun Cao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yao%2C+Y">Yuanzhe Yao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+L">Liang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+R">Runyu Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zhiyuan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jing Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1809.04258v1-abstract-short" style="display: inline;">
        In this work, an ontology-based model for AI-assisted medicine side-effect (SE) prediction is developed, where three main components, including the drug model, the treatment model, and the AI-assisted prediction model, of proposed model are presented. To validate the proposed model, an ANN structure is established and trained by two hundred and forty-two TCM prescriptions that are gathered and cla&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.04258v1-abstract-full').style.display = 'inline'; document.getElementById('1809.04258v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1809.04258v1-abstract-full" style="display: none;">
        In this work, an ontology-based model for AI-assisted medicine side-effect (SE) prediction is developed, where three main components, including the drug model, the treatment model, and the AI-assisted prediction model, of proposed model are presented. To validate the proposed model, an ANN structure is established and trained by two hundred and forty-two TCM prescriptions that are gathered and classified from the most famous ancient TCM book and more than one thousand SE reports, in which two ontology-based attributions, hot and cold, are simply introduced to evaluate whether the prediction will cause a SE or not. The results preliminarily reveal that it is a relationship between the ontology-based attributions and the corresponding indicator that can be learnt by AI for predicting the SE, which suggests the proposed model has a potential in AI-assisted SE prediction. However, it should be noted that, the proposed model highly depends on the sufficient clinic data, and hereby, much deeper exploration is important for enhancing the accuracy of the prediction.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.04258v1-abstract-full').style.display = 'none'; document.getElementById('1809.04258v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 September, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1809.01997">arXiv:1809.01997</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1809.01997">pdf</a>, <a href="https://arxiv.org/format/1809.01997">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dual Ask-Answer Network for Machine Reading Comprehension
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xiao%2C+H">Han Xiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+F">Feng Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jianfeng Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+J">Jingyao Zheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1809.01997v2-abstract-short" style="display: inline;">
        There are three modalities in the reading comprehension setting: question, answer and context. The task of question answering or question generation aims to infer an answer or a question when given the counterpart based on context. We present a novel two-way neural sequence transduction model that connects three modalities, allowing it to learn two tasks simultaneously and mutually benefit one ano&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.01997v2-abstract-full').style.display = 'inline'; document.getElementById('1809.01997v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1809.01997v2-abstract-full" style="display: none;">
        There are three modalities in the reading comprehension setting: question, answer and context. The task of question answering or question generation aims to infer an answer or a question when given the counterpart based on context. We present a novel two-way neural sequence transduction model that connects three modalities, allowing it to learn two tasks simultaneously and mutually benefit one another. During training, the model receives question-context-answer triplets as input and captures the cross-modal interaction via a hierarchical attention process. Unlike previous joint learning paradigms that leverage the duality of question generation and question answering at data level, we solve such dual tasks at the architecture level by mirroring the network structure and partially sharing components at different layers. This enables the knowledge to be transferred from one task to another, helping the model to find a general representation for each modality. The evaluation on four public datasets shows that our dual-learning model outperforms the mono-learning counterpart as well as the state-of-the-art joint models on both question answering and question generation tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.01997v2-abstract-full').style.display = 'none'; document.getElementById('1809.01997v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 September, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 5 figures, 4 tables. Code is available at https://github.com/hanxiao/daanet</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1809.01407">arXiv:1809.01407</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1809.01407">pdf</a>, <a href="https://arxiv.org/format/1809.01407">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Consensus-Driven Propagation in Massive Unlabeled Data for Face Recognition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhan%2C+X">Xiaohang Zhan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Ziwei Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+D">Dahua Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Loy%2C+C+C">Chen Change Loy</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1809.01407v2-abstract-short" style="display: inline;">
        Face recognition has witnessed great progress in recent years, mainly attributed to the high-capacity model designed and the abundant labeled data collected. However, it becomes more and more prohibitive to scale up the current million-level identity annotations. In this work, we show that unlabeled face data can be as effective as the labeled ones. Here, we consider a setting closely mimicking th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.01407v2-abstract-full').style.display = 'inline'; document.getElementById('1809.01407v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1809.01407v2-abstract-full" style="display: none;">
        Face recognition has witnessed great progress in recent years, mainly attributed to the high-capacity model designed and the abundant labeled data collected. However, it becomes more and more prohibitive to scale up the current million-level identity annotations. In this work, we show that unlabeled face data can be as effective as the labeled ones. Here, we consider a setting closely mimicking the real-world scenario, where the unlabeled data are collected from unconstrained environments and their identities are exclusive from the labeled ones. Our main insight is that although the class information is not available, we can still faithfully approximate these semantic relationships by constructing a relational graph in a bottom-up manner. We propose Consensus-Driven Propagation (CDP) to tackle this challenging problem with two modules, the &#34;committee&#34; and the &#34;mediator&#34;, which select positive face pairs robustly by carefully aggregating multi-view information. Extensive experiments validate the effectiveness of both modules to discard outliers and mine hard positives. With CDP, we achieve a compelling accuracy of 78.18% on MegaFace identification challenge by using only 9% of the labels, comparing to 61.78% when no unlabeled data are used and 78.52% when all labels are employed.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.01407v2-abstract-full').style.display = 'none'; document.getElementById('1809.01407v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 January, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 September, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">In ECCV 2018. More details at the project page: http://mmlab.ie.cuhk.edu.hk/projects/CDP/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1808.10250">arXiv:1808.10250</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1808.10250">pdf</a>, <a href="https://arxiv.org/format/1808.10250">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SonarSnoop: Active Acoustic Side-Channel Attacks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cheng%2C+P">Peng Cheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bagci%2C+I+E">Ibrahim Ethem Bagci</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Roedig%2C+U">Utz Roedig</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jeff Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1808.10250v1-abstract-short" style="display: inline;">
        We report the first active acoustic side-channel attack. Speakers are used to emit human inaudible acoustic signals and the echo is recorded via microphones, turning the acoustic system of a smart phone into a sonar system. The echo signal can be used to profile user interaction with the device. For example, a victim's finger movements can be inferred to steal Android phone unlock patterns. In our&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.10250v1-abstract-full').style.display = 'inline'; document.getElementById('1808.10250v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1808.10250v1-abstract-full" style="display: none;">
        We report the first active acoustic side-channel attack. Speakers are used to emit human inaudible acoustic signals and the echo is recorded via microphones, turning the acoustic system of a smart phone into a sonar system. The echo signal can be used to profile user interaction with the device. For example, a victim&#39;s finger movements can be inferred to steal Android phone unlock patterns. In our empirical study, the number of candidate unlock patterns that an attacker must try to authenticate herself to a Samsung S4 Android phone can be reduced by up to 70% using this novel acoustic side-channel. Our approach can be easily applied to other application scenarios and device types. Overall, our work highlights a new family of security threats.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.10250v1-abstract-full').style.display = 'none'; document.getElementById('1808.10250v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 August, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1808.09676">arXiv:1808.09676</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1808.09676">pdf</a>, <a href="https://arxiv.org/ps/1808.09676">ps</a>, <a href="https://arxiv.org/format/1808.09676">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Joint Doppler and Channel Estimation with Nested Arrays for Millimeter Wave Communications
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+X">Xiaohuan Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+W">Wei-Ping Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+M">Min Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jun Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1808.09676v1-abstract-short" style="display: inline;">
        Channel estimation is essential for precoding/combining in millimeter wave (mmWave) communications. However, accurate estimation is usually difficult because the receiver can only observe the low-dimensional projection of the received signals due to the hybrid architecture. We take the high speed scenario into consideration where the Doppler effect caused by fast-moving users can seriously deterio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.09676v1-abstract-full').style.display = 'inline'; document.getElementById('1808.09676v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1808.09676v1-abstract-full" style="display: none;">
        Channel estimation is essential for precoding/combining in millimeter wave (mmWave) communications. However, accurate estimation is usually difficult because the receiver can only observe the low-dimensional projection of the received signals due to the hybrid architecture. We take the high speed scenario into consideration where the Doppler effect caused by fast-moving users can seriously deteriorate the channel estimation accuracy. In this paper, we propose to incorporate the nested array into analog array architecture by using RF switch networks with an objective of reducing the complexity and power consumption of the system. Based on the covariance fitting criterion, a joint Doppler and channel estimation method is proposed without need of discretizing the angle space, and thus the model mismatch effect can be totally eliminated. We also present an algorithmic implementation by solving the dual problem of the original one in order to reduce the computational complexity. Numerical simulations are provided to demonstrate the effectiveness and superiority of our proposed method.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.09676v1-abstract-full').style.display = 'none'; document.getElementById('1808.09676v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 August, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1808.09102">arXiv:1808.09102</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1808.09102">pdf</a>, <a href="https://arxiv.org/format/1808.09102">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Localization Guided Learning for Pedestrian Attribute Recognition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+P">Pengze Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+X">Xihui Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shao%2C+J">Jing Shao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1808.09102v1-abstract-short" style="display: inline;">
        Pedestrian attribute recognition has attracted many attentions due to its wide applications in scene understanding and person analysis from surveillance videos. Existing methods try to use additional pose, part or viewpoint information to complement the global feature representation for attribute classification. However, these methods face difficulties in localizing the areas corresponding to diff&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.09102v1-abstract-full').style.display = 'inline'; document.getElementById('1808.09102v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1808.09102v1-abstract-full" style="display: none;">
        Pedestrian attribute recognition has attracted many attentions due to its wide applications in scene understanding and person analysis from surveillance videos. Existing methods try to use additional pose, part or viewpoint information to complement the global feature representation for attribute classification. However, these methods face difficulties in localizing the areas corresponding to different attributes. To address this problem, we propose a novel Localization Guided Network which assigns attribute-specific weights to local features based on the affinity between proposals pre-extracted proposals and attribute locations. The advantage of our model is that our local features are learned automatically for each attribute and emphasized by the interaction with global features. We demonstrate the effectiveness of our Localization Guided Network on two pedestrian attribute benchmarks (PA-100K and RAP). Our result surpasses the previous state-of-the-art in all five metrics on both datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.09102v1-abstract-full').style.display = 'none'; document.getElementById('1808.09102v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 August, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by BMVC 2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1808.08355">arXiv:1808.08355</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1808.08355">pdf</a>, <a href="https://arxiv.org/format/1808.08355">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Database-Agnostic Workload Management
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jain%2C+S">Shrainik Jain</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jiaqi Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cruane%2C+T">Thierry Cruane</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Howe%2C+B">Bill Howe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1808.08355v1-abstract-short" style="display: inline;">
        We present a system to support generalized SQL workload analysis and management for multi-tenant and multi-database platforms. Workload analysis applications are becoming more sophisticated to support database administration, model user behavior, audit security, and route queries, but the methods rely on specialized feature engineering, and therefore must be carefully implemented and reimplemented&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.08355v1-abstract-full').style.display = 'inline'; document.getElementById('1808.08355v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1808.08355v1-abstract-full" style="display: none;">
        We present a system to support generalized SQL workload analysis and management for multi-tenant and multi-database platforms. Workload analysis applications are becoming more sophisticated to support database administration, model user behavior, audit security, and route queries, but the methods rely on specialized feature engineering, and therefore must be carefully implemented and reimplemented for each SQL dialect, database system, and application. Meanwhile, the size and complexity of workloads are increasing as systems centralize in the cloud. We model workload analysis and management tasks as variations on query labeling, and propose a system design that can support general query labeling routines across multiple applications and database backends. The design relies on the use of learned vector embeddings for SQL queries as a replacement for application-specific syntactic features, reducing custom code and allowing the use of off-the-shelf machine learning algorithms for labeling. The key hypothesis, for which we provide evidence in this paper, is that these learned features can outperform conventional feature engineering on representative machine learning tasks. We present the design of a database-agnostic workload management and analytics service, describe potential applications, and show that separating workload representation from labeling tasks affords new capabilities and can outperform existing solutions for representative tasks, including workload sampling for index recommendation and user labeling for security audits.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.08355v1-abstract-full').style.display = 'none'; document.getElementById('1808.08355v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 August, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1808.06048">arXiv:1808.06048</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1808.06048">pdf</a>, <a href="https://arxiv.org/format/1808.06048">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Distractor-aware Siamese Networks for Visual Object Tracking
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+Z">Zheng Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Q">Qiang Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+B">Bo Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+W">Wei Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+W">Weiming Hu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1808.06048v1-abstract-short" style="display: inline;">
        Recently, Siamese networks have drawn great attention in visual tracking community because of their balanced accuracy and speed. However, features used in most Siamese tracking approaches can only discriminate foreground from the non-semantic backgrounds. The semantic backgrounds are always considered as distractors, which hinders the robustness of Siamese trackers. In this paper, we focus on lear&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.06048v1-abstract-full').style.display = 'inline'; document.getElementById('1808.06048v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1808.06048v1-abstract-full" style="display: none;">
        Recently, Siamese networks have drawn great attention in visual tracking community because of their balanced accuracy and speed. However, features used in most Siamese tracking approaches can only discriminate foreground from the non-semantic backgrounds. The semantic backgrounds are always considered as distractors, which hinders the robustness of Siamese trackers. In this paper, we focus on learning distractor-aware Siamese networks for accurate and long-term tracking. To this end, features used in traditional Siamese trackers are analyzed at first. We observe that the imbalanced distribution of training data makes the learned features less discriminative. During the off-line training phase, an effective sampling strategy is introduced to control this distribution and make the model focus on the semantic distractors. During inference, a novel distractor-aware module is designed to perform incremental learning, which can effectively transfer the general embedding to the current video domain. In addition, we extend the proposed approach for long-term tracking by introducing a simple yet effective local-to-global search region strategy. Extensive experiments on benchmarks show that our approach significantly outperforms the state-of-the-arts, yielding 9.6% relative gain in VOT2016 dataset and 35.9% relative gain in UAV20L dataset. The proposed tracker can perform at 160 FPS on short-term benchmarks and 110 FPS on long-term benchmarks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.06048v1-abstract-full').style.display = 'none'; document.getElementById('1808.06048v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 August, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ECCV 2018, main paper and supplementary material</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1808.05584">arXiv:1808.05584</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1808.05584">pdf</a>, <a href="https://arxiv.org/format/1808.05584">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        BlockQNN: Efficient Block-wise Neural Network Architecture Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhong%2C+Z">Zhao Zhong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+Z">Zichen Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Deng%2C+B">Boyang Deng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+W">Wei Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shao%2C+J">Jing Shao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+C">Cheng-Lin Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1808.05584v1-abstract-short" style="display: inline;">
        Convolutional neural networks have gained a remarkable success in computer vision. However, most usable network architectures are hand-crafted and usually require expertise and elaborate design. In this paper, we provide a block-wise network generation pipeline called BlockQNN which automatically builds high-performance networks using the Q-Learning paradigm with epsilon-greedy exploration strateg&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.05584v1-abstract-full').style.display = 'inline'; document.getElementById('1808.05584v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1808.05584v1-abstract-full" style="display: none;">
        Convolutional neural networks have gained a remarkable success in computer vision. However, most usable network architectures are hand-crafted and usually require expertise and elaborate design. In this paper, we provide a block-wise network generation pipeline called BlockQNN which automatically builds high-performance networks using the Q-Learning paradigm with epsilon-greedy exploration strategy. The optimal network block is constructed by the learning agent which is trained to choose component layers sequentially. We stack the block to construct the whole auto-generated network. To accelerate the generation process, we also propose a distributed asynchronous framework and an early stop strategy. The block-wise generation brings unique advantages: (1) it yields state-of-the-art results in comparison to the hand-crafted networks on image classification, particularly, the best network generated by BlockQNN achieves 2.35% top-1 error rate on CIFAR-10. (2) it offers tremendous reduction of the search space in designing networks, spending only 3 days with 32 GPUs. A faster version can yield a comparable result with only 1 GPU in 20 hours. (3) it has strong generalizability in that the network built on CIFAR also performs well on the larger-scale dataset. The best network achieves very competitive accuracy of 82.0% top-1 and 96.0% top-5 on ImageNet.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.05584v1-abstract-full').style.display = 'none'; document.getElementById('1808.05584v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 August, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, 18 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1805.08638">arXiv:1805.08638</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1805.08638">pdf</a>, <a href="https://arxiv.org/ps/1805.08638">ps</a>, <a href="https://arxiv.org/format/1805.08638">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Cost-aware Cascading Bandits
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+R">Ruida Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gan%2C+C">Chao Gan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jing Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+C">Cong Shen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1805.08638v1-abstract-short" style="display: inline;">
        In this paper, we propose a cost-aware cascading bandits model, a new variant of multi-armed ban- dits with cascading feedback, by considering the random cost of pulling arms. In each step, the learning agent chooses an ordered list of items and examines them sequentially, until certain stopping condition is satisfied. Our objective is then to max- imize the expected net reward in each step, i.e.,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.08638v1-abstract-full').style.display = 'inline'; document.getElementById('1805.08638v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1805.08638v1-abstract-full" style="display: none;">
        In this paper, we propose a cost-aware cascading bandits model, a new variant of multi-armed ban- dits with cascading feedback, by considering the random cost of pulling arms. In each step, the learning agent chooses an ordered list of items and examines them sequentially, until certain stopping condition is satisfied. Our objective is then to max- imize the expected net reward in each step, i.e., the reward obtained in each step minus the total cost in- curred in examining the items, by deciding the or- dered list of items, as well as when to stop examina- tion. We study both the offline and online settings, depending on whether the state and cost statistics of the items are known beforehand. For the of- fline setting, we show that the Unit Cost Ranking with Threshold 1 (UCR-T1) policy is optimal. For the online setting, we propose a Cost-aware Cas- cading Upper Confidence Bound (CC-UCB) algo- rithm, and show that the cumulative regret scales in O(log T ). We also provide a lower bound for all -consistent policies, which scales in (log T ) and matches our upper bound. The performance of the CC-UCB algorithm is evaluated with both synthetic and real-world data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.08638v1-abstract-full').style.display = 'none'; document.getElementById('1805.08638v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 May, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, 2 figures, IJCAI 2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1805.02152">arXiv:1805.02152</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1805.02152">pdf</a>, <a href="https://arxiv.org/format/1805.02152">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Quantization Mimic: Towards Very Tiny CNN for Object Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+Y">Yi Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pan%2C+X">Xinyu Pan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qin%2C+H">Hongwei Qin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ouyang%2C+W">Wanli Ouyang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1805.02152v3-abstract-short" style="display: inline;">
        In this paper, we propose a simple and general framework for training very tiny CNNs for object detection. Due to limited representation ability, it is challenging to train very tiny networks for complicated tasks like detection. To the best of our knowledge, our method, called Quantization Mimic, is the first one focusing on very tiny networks. We utilize two types of acceleration methods: mimic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.02152v3-abstract-full').style.display = 'inline'; document.getElementById('1805.02152v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1805.02152v3-abstract-full" style="display: none;">
        In this paper, we propose a simple and general framework for training very tiny CNNs for object detection. Due to limited representation ability, it is challenging to train very tiny networks for complicated tasks like detection. To the best of our knowledge, our method, called Quantization Mimic, is the first one focusing on very tiny networks. We utilize two types of acceleration methods: mimic and quantization. Mimic improves the performance of a student network by transfering knowledge from a teacher network. Quantization converts a full-precision network to a quantized one without large degradation of performance. If the teacher network is quantized, the search scope of the student network will be smaller. Using this feature of the quantization, we propose Quantization Mimic. It first quantizes the large network, then mimic a quantized small network. The quantization operation can help student network to better match the feature maps from teacher network. To evaluate our approach, we carry out experiments on various popular CNNs including VGG and Resnet, as well as different detection frameworks including Faster R-CNN and R-FCN. Experiments on Pascal VOC and WIDER FACE verify that our Quantization Mimic algorithm can be applied on various settings and outperforms state-of-the-art model acceleration methods given limited computing resouces.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.02152v3-abstract-full').style.display = 'none'; document.getElementById('1805.02152v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 September, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 May, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ECCV 2018. Version for conference submission (polish and use theoretical size in Table 1)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1804.05197">arXiv:1804.05197</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1804.05197">pdf</a>, <a href="https://arxiv.org/format/1804.05197">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Beyond Trade-off: Accelerate FCN-based Face Detector with Higher Accuracy
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+G">Guanglu Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yu Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+M">Ming Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yujie Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Leng%2C+B">Biao Leng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1804.05197v2-abstract-short" style="display: inline;">
        Fully convolutional neural network (FCN) has been dominating the game of face detection task for a few years with its congenital capability of sliding-window-searching with shared kernels, which boiled down all the redundant calculation, and most recent state-of-the-art methods such as Faster-RCNN, SSD, YOLO and FPN use FCN as their backbone. So here comes one question: Can we find a universal str&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.05197v2-abstract-full').style.display = 'inline'; document.getElementById('1804.05197v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1804.05197v2-abstract-full" style="display: none;">
        Fully convolutional neural network (FCN) has been dominating the game of face detection task for a few years with its congenital capability of sliding-window-searching with shared kernels, which boiled down all the redundant calculation, and most recent state-of-the-art methods such as Faster-RCNN, SSD, YOLO and FPN use FCN as their backbone. So here comes one question: Can we find a universal strategy to further accelerate FCN with higher accuracy, so could accelerate all the recent FCN-based methods? To analyze this, we decompose the face searching space into two orthogonal directions, `scale&#39; and `spatial&#39;. Only a few coordinates in the space expanded by the two base vectors indicate foreground. So if FCN could ignore most of the other points, the searching space and false alarm should be significantly boiled down. Based on this philosophy, a novel method named scale estimation and spatial attention proposal ($S^2AP$) is proposed to pay attention to some specific scales and valid locations in the image pyramid. Furthermore, we adopt a masked-convolution operation based on the attention result to accelerate FCN calculation. Experiments show that FCN-based method RPN can be accelerated by about $4\times$ with the help of $S^2AP$ and masked-FCN and at the same time it can also achieve the state-of-the-art on FDDB, AFW and MALF face detection benchmarks as well.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.05197v2-abstract-full').style.display = 'none'; document.getElementById('1804.05197v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 June, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 April, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by CVPR2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1804.03487">arXiv:1804.03487</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1804.03487">pdf</a>, <a href="https://arxiv.org/format/1804.03487">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploring Disentangled Feature Representation Beyond Face Identification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yu Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+F">Fangyin Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shao%2C+J">Jing Shao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sheng%2C+L">Lu Sheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xiaogang Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1804.03487v1-abstract-short" style="display: inline;">
        This paper proposes learning disentangled but complementary face features with minimal supervision by face identification. Specifically, we construct an identity Distilling and Dispelling Autoencoder (D2AE) framework that adversarially learns the identity-distilled features for identity verification and the identity-dispelled features to fool the verification system. Thanks to the design of two-st&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.03487v1-abstract-full').style.display = 'inline'; document.getElementById('1804.03487v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1804.03487v1-abstract-full" style="display: none;">
        This paper proposes learning disentangled but complementary face features with minimal supervision by face identification. Specifically, we construct an identity Distilling and Dispelling Autoencoder (D2AE) framework that adversarially learns the identity-distilled features for identity verification and the identity-dispelled features to fool the verification system. Thanks to the design of two-stream cues, the learned disentangled features represent not only the identity or attribute but the complete input image. Comprehensive evaluations further demonstrate that the proposed features not only maintain state-of-the-art identity verification performance on LFW, but also acquire competitive discriminative power for face attribute recognition on CelebA and LFWA. Moreover, the proposed system is ready to semantically control the face generation/editing based on various identities and attributes in an unsupervised manner.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.03487v1-abstract-full').style.display = 'none'; document.getElementById('1804.03487v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 April, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by CVPR 2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1803.09876">arXiv:1803.09876</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1803.09876">pdf</a>, <a href="https://arxiv.org/format/1803.09876">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        JSweep: A Patch-centric Data-driven Approach for Parallel Sweeps on Large-scale Meshes
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jie Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+Z">Zhang Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+A">Aiqing Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mo%2C+Z">Zeyao Mo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1803.09876v1-abstract-short" style="display: inline;">
        In mesh-based numerical simulations, sweep is an important computation pattern. During sweeping a mesh, computations on cells are strictly ordered by data dependencies in given directions. Due to such a serial order, parallelizing sweep is challenging, especially for unstructured and deforming structured meshes. Meanwhile, recent high-fidelity multi-physics simulations of particle transport, inclu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.09876v1-abstract-full').style.display = 'inline'; document.getElementById('1803.09876v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1803.09876v1-abstract-full" style="display: none;">
        In mesh-based numerical simulations, sweep is an important computation pattern. During sweeping a mesh, computations on cells are strictly ordered by data dependencies in given directions. Due to such a serial order, parallelizing sweep is challenging, especially for unstructured and deforming structured meshes. Meanwhile, recent high-fidelity multi-physics simulations of particle transport, including nuclear reactor and inertial confinement fusion, require {\em sweeps} on large scale meshes with billions of cells and hundreds of directions.
  In this paper, we present JSweep, a parallel data-driven computational framework integrated in the JAxMIN infrastructure. The essential of JSweep is a general patch-centric data-driven abstraction, coupled with a high performance runtime system leveraging hybrid parallelism of MPI+threads and achieving dynamic communication on contemporary multi-core clusters. Built on JSweep, we implement a representative data-driven algorithm, Sn transport, featuring optimizations of vertex clustering, multi-level priority strategy and patch-angle parallelism. Experimental evaluation with two real-world applications on structured and unstructured meshes respectively, demonstrates that JSweep can scale to tens of thousands of processor cores with reasonable parallel efficiency.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.09876v1-abstract-full').style.display = 'none'; document.getElementById('1803.09876v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 March, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 17 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68Q85
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1803.00830">arXiv:1803.00830</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1803.00830">pdf</a>, <a href="https://arxiv.org/format/1803.00830">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep Cocktail Network: Multi-source Unsupervised Domain Adaptation with Category Shift
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+R">Ruijia Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Z">Ziliang Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zuo%2C+W">Wangmeng Zuo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+L">Liang Lin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1803.00830v1-abstract-short" style="display: inline;">
        Unsupervised domain adaptation (UDA) conventionally assumes labeled source samples coming from a single underlying source distribution. Whereas in practical scenario, labeled data are typically collected from diverse sources. The multiple sources are different not only from the target but also from each other, thus, domain adaptater should not be modeled in the same way. Moreover, those sources ma&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.00830v1-abstract-full').style.display = 'inline'; document.getElementById('1803.00830v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1803.00830v1-abstract-full" style="display: none;">
        Unsupervised domain adaptation (UDA) conventionally assumes labeled source samples coming from a single underlying source distribution. Whereas in practical scenario, labeled data are typically collected from diverse sources. The multiple sources are different not only from the target but also from each other, thus, domain adaptater should not be modeled in the same way. Moreover, those sources may not completely share their categories, which further brings a new transfer challenge called category shift. In this paper, we propose a deep cocktail network (DCTN) to battle the domain and category shifts among multiple sources. Motivated by the theoretical results in \cite{mansour2009domain}, the target distribution can be represented as the weighted combination of source distributions, and, the multi-source unsupervised domain adaptation via DCTN is then performed as two alternating steps: i) It deploys multi-way adversarial learning to minimize the discrepancy between the target and each of the multiple source domains, which also obtains the source-specific perplexity scores to denote the possibilities that a target sample belongs to different source domains. ii) The multi-source category classifiers are integrated with the perplexity scores to classify target sample, and the pseudo-labeled target samples together with source samples are utilized to update the multi-source category classifier and the feature extractor. We evaluate DCTN in three domain adaptation benchmarks, which clearly demonstrate the superiority of our framework.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.00830v1-abstract-full').style.display = 'none'; document.getElementById('1803.00830v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 March, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for publication in Conference on Computer Vision and Pattern Recognition(CVPR), 2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1801.09262">arXiv:1801.09262</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1801.09262">pdf</a>, <a href="https://arxiv.org/ps/1801.09262">ps</a>, <a href="https://arxiv.org/format/1801.09262">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/MWC.2017.1600401">10.1109/MWC.2017.1600401 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Cell-less Communications in 5G Vehicular Networks Based on Vehicle-Installed Access Points
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+L">Lijun Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+T">Tao Han</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Q">Qiang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jia Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+X">Xiong Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Deng%2C+D">Dexiang Deng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1801.09262v1-abstract-short" style="display: inline;">
        The development of intelligent transportation systems raises many requirements to the current vehicular networks. For instance, to ensure secure communications between vehicles, low latency, high connectivity and high data rate are required for vehicular networks. To meet such requirements, the 5G communication systems for vehicular networks should be improved accordingly. This article proposes a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.09262v1-abstract-full').style.display = 'inline'; document.getElementById('1801.09262v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1801.09262v1-abstract-full" style="display: none;">
        The development of intelligent transportation systems raises many requirements to the current vehicular networks. For instance, to ensure secure communications between vehicles, low latency, high connectivity and high data rate are required for vehicular networks. To meet such requirements, the 5G communication systems for vehicular networks should be improved accordingly. This article proposes a communication scheme for 5G vehicular networks, in which moving access points are deployed on vehicles to facilitate the access of vehicle users. Moreover, the adjacent vehicle-installed moving access points may cooperatively communicate with the vehicle users by joint transmissions and receptions. In this way, the vehicle users communicate with one or more unspecified cooperative access points in a cell-less manner instead of being associated to a single access point. To manage such cell-less networks, local moving software-defined cloudlets are adopted to perform transmission and scheduling management. Simulation results show that the proposed scheme significantly reduces the latency, while improving the connectivity of the vehicular networks, and can be considered as a research direction for the solution to 5G vehicular networks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.09262v1-abstract-full').style.display = 'none'; document.getElementById('1801.09262v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 January, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">20 pages, 6 figures, accepted by IEEE Wireless Communications</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IEEE Wireless Communications, vol. 24, no. 6, pp. 64-71, Dec. 2017
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1801.06805">arXiv:1801.06805</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1801.06805">pdf</a>, <a href="https://arxiv.org/format/1801.06805">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Decoupled Learning for Factorial Marked Temporal Point Processes
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+W">Weichang Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junchi Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+X">Xiaokang Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zha%2C+H">Hongyuan Zha</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1801.06805v1-abstract-short" style="display: inline;">
        This paper introduces the factorial marked temporal point process model and presents efficient learning methods. In conventional (multi-dimensional) marked temporal point process models, event is often encoded by a single discrete variable i.e. a marker. In this paper, we describe the factorial marked point processes whereby time-stamped event is factored into multiple markers. Accordingly the siz&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.06805v1-abstract-full').style.display = 'inline'; document.getElementById('1801.06805v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1801.06805v1-abstract-full" style="display: none;">
        This paper introduces the factorial marked temporal point process model and presents efficient learning methods. In conventional (multi-dimensional) marked temporal point process models, event is often encoded by a single discrete variable i.e. a marker. In this paper, we describe the factorial marked point processes whereby time-stamped event is factored into multiple markers. Accordingly the size of the infectivity matrix modeling the effect between pairwise markers is in power order w.r.t. the number of the discrete marker space. We propose a decoupled learning method with two learning procedures: i) directly solving the model based on two techniques: Alternating Direction Method of Multipliers and Fast Iterative Shrinkage-Thresholding Algorithm; ii) involving a reformulation that transforms the original problem into a Logistic Regression model for more efficient learning. Moreover, a sparse group regularizer is added to identify the key profile features and event labels. Empirical results on real world datasets demonstrate the efficiency of our decoupled and reformulated method. The source code is available online.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.06805v1-abstract-full').style.display = 'none'; document.getElementById('1801.06805v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 January, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 8 figures, submitted to TNNLS, 21 Jan, 2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1801.05613">arXiv:1801.05613</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1801.05613">pdf</a>, <a href="https://arxiv.org/format/1801.05613">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Query2Vec: An Evaluation of NLP Techniques for Generalized Workload Analytics
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jain%2C+S">Shrainik Jain</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Howe%2C+B">Bill Howe</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jiaqi Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cruanes%2C+T">Thierry Cruanes</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1801.05613v2-abstract-short" style="display: inline;">
        We consider methods for learning vector representations of SQL queries to support generalized workload analytics tasks, including workload summarization for index selection and predicting queries that will trigger memory errors. We consider vector representations of both raw SQL text and optimized query plans, and evaluate these methods on synthetic and real SQL workloads. We find that general alg&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.05613v2-abstract-full').style.display = 'inline'; document.getElementById('1801.05613v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1801.05613v2-abstract-full" style="display: none;">
        We consider methods for learning vector representations of SQL queries to support generalized workload analytics tasks, including workload summarization for index selection and predicting queries that will trigger memory errors. We consider vector representations of both raw SQL text and optimized query plans, and evaluate these methods on synthetic and real SQL workloads. We find that general algorithms based on vector representations can outperform existing approaches that rely on specialized features. For index recommendation, we cluster the vector representations to compress large workloads with no loss in performance from the recommended index. For error prediction, we train a classifier over learned vectors that can automatically relate subtle syntactic patterns with specific errors raised during query execution. Surprisingly, we also find that these methods enable transfer learning, where a model trained on one SQL corpus can be applied to an unrelated corpus and still enable good performance. We find that these general approaches, when trained on a large corpus of SQL queries, provides a robust foundation for a variety of workload analysis tasks and database features, without requiring application-specific feature engineering.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.05613v2-abstract-full').style.display = 'none'; document.getElementById('1801.05613v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 February, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 January, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1801.04701">arXiv:1801.04701</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1801.04701">pdf</a>, <a href="https://arxiv.org/format/1801.04701">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        tau-FPL: Tolerance-Constrained Learning in Linear Time
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+A">Ao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+N">Nan Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pu%2C+J">Jian Pu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+J">Jun Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junchi Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zha%2C+H">Hongyuan Zha</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1801.04701v1-abstract-short" style="display: inline;">
        Learning a classifier with control on the false-positive rate plays a critical role in many machine learning applications. Existing approaches either introduce prior knowledge dependent label cost or tune parameters based on traditional classifiers, which lack consistency in methodology because they do not strictly adhere to the false-positive rate constraint. In this paper, we propose a novel sco&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.04701v1-abstract-full').style.display = 'inline'; document.getElementById('1801.04701v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1801.04701v1-abstract-full" style="display: none;">
        Learning a classifier with control on the false-positive rate plays a critical role in many machine learning applications. Existing approaches either introduce prior knowledge dependent label cost or tune parameters based on traditional classifiers, which lack consistency in methodology because they do not strictly adhere to the false-positive rate constraint. In this paper, we propose a novel scoring-thresholding approach, tau-False Positive Learning (tau-FPL) to address this problem. We show the scoring problem which takes the false-positive rate tolerance into accounts can be efficiently solved in linear time, also an out-of-bootstrap thresholding method can transform the learned ranking function into a low false-positive classifier. Both theoretical analysis and experimental results show superior performance of the proposed tau-FPL over existing approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.04701v1-abstract-full').style.display = 'none'; document.getElementById('1801.04701v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 January, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">32 pages, 3 figures. This is an extended version of our paper published in AAAI-18</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1801.01687">arXiv:1801.01687</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1801.01687">pdf</a>, <a href="https://arxiv.org/format/1801.01687">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Accelerated Training for Massive Classification via Dynamic Class Selection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xingcheng Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+L">Lei Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+D">Dahua Lin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1801.01687v1-abstract-short" style="display: inline;">
        Massive classification, a classification task defined over a vast number of classes (hundreds of thousands or even millions), has become an essential part of many real-world systems, such as face recognition. Existing methods, including the deep networks that achieved remarkable success in recent years, were mostly devised for problems with a moderate number of classes. They would meet with substa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.01687v1-abstract-full').style.display = 'inline'; document.getElementById('1801.01687v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1801.01687v1-abstract-full" style="display: none;">
        Massive classification, a classification task defined over a vast number of classes (hundreds of thousands or even millions), has become an essential part of many real-world systems, such as face recognition. Existing methods, including the deep networks that achieved remarkable success in recent years, were mostly devised for problems with a moderate number of classes. They would meet with substantial difficulties, e.g. excessive memory demand and computational cost, when applied to massive problems. We present a new method to tackle this problem. This method can efficiently and accurately identify a small number of &#34;active classes&#34; for each mini-batch, based on a set of dynamic class hierarchies constructed on the fly. We also develop an adaptive allocation scheme thereon, which leads to a better tradeoff between performance and cost. On several large-scale benchmarks, our method significantly reduces the training cost and memory demand, while maintaining competitive performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.01687v1-abstract-full').style.display = 'none'; document.getElementById('1801.01687v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 6 figures, AAAI 2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1801.01671">arXiv:1801.01671</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1801.01671">pdf</a>, <a href="https://arxiv.org/format/1801.01671">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FOTS: Fast Oriented Text Spotting with a Unified Network
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+X">Xuebo Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+D">Ding Liang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+S">Shi Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+D">Dagui Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qiao%2C+Y">Yu Qiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1801.01671v2-abstract-short" style="display: inline;">
        Incidental scene text spotting is considered one of the most difficult and valuable challenges in the document analysis community. Most existing methods treat text detection and recognition as separate tasks. In this work, we propose a unified end-to-end trainable Fast Oriented Text Spotting (FOTS) network for simultaneous detection and recognition, sharing computation and visual information among&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.01671v2-abstract-full').style.display = 'inline'; document.getElementById('1801.01671v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1801.01671v2-abstract-full" style="display: none;">
        Incidental scene text spotting is considered one of the most difficult and valuable challenges in the document analysis community. Most existing methods treat text detection and recognition as separate tasks. In this work, we propose a unified end-to-end trainable Fast Oriented Text Spotting (FOTS) network for simultaneous detection and recognition, sharing computation and visual information among the two complementary tasks. Specially, RoIRotate is introduced to share convolutional features between detection and recognition. Benefiting from convolution sharing strategy, our FOTS has little computation overhead compared to baseline text detection network, and the joint training method learns more generic features to make our method perform better than these two-stage methods. Experiments on ICDAR 2015, ICDAR 2017 MLT, and ICDAR 2013 datasets demonstrate that the proposed method outperforms state-of-the-art methods significantly, which further allows us to develop the first real-time oriented text spotting system which surpasses all previous state-of-the-art results by more than 5% on ICDAR 2015 text spotting task while keeping 22.6 fps.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.01671v2-abstract-full').style.display = 'none'; document.getElementById('1801.01671v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 January, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 January, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1712.05896">arXiv:1712.05896</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1712.05896">pdf</a>, <a href="https://arxiv.org/format/1712.05896">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Impression Network for Video Object Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hetang%2C+C">Congrui Hetang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qin%2C+H">Hongwei Qin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+S">Shaohui Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1712.05896v1-abstract-short" style="display: inline;">
        Video object detection is more challenging compared to image object detection. Previous works proved that applying object detector frame by frame is not only slow but also inaccurate. Visual clues get weakened by defocus and motion blur, causing failure on corresponding frames. Multi-frame feature fusion methods proved effective in improving the accuracy, but they dramatically sacrifice the speed.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1712.05896v1-abstract-full').style.display = 'inline'; document.getElementById('1712.05896v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1712.05896v1-abstract-full" style="display: none;">
        Video object detection is more challenging compared to image object detection. Previous works proved that applying object detector frame by frame is not only slow but also inaccurate. Visual clues get weakened by defocus and motion blur, causing failure on corresponding frames. Multi-frame feature fusion methods proved effective in improving the accuracy, but they dramatically sacrifice the speed. Feature propagation based methods proved effective in improving the speed, but they sacrifice the accuracy. So is it possible to improve speed and performance simultaneously?
  Inspired by how human utilize impression to recognize objects from blurry frames, we propose Impression Network that embodies a natural and efficient feature aggregation mechanism. In our framework, an impression feature is established by iteratively absorbing sparsely extracted frame features. The impression feature is propagated all the way down the video, helping enhance features of low-quality frames. This impression mechanism makes it possible to perform long-range multi-frame feature fusion among sparse keyframes with minimal overhead. It significantly improves per-frame detection baseline on ImageNet VID while being 3 times faster (20 fps). We hope Impression Network can provide a new perspective on video feature enhancement. Code will be made available.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1712.05896v1-abstract-full').style.display = 'none'; document.getElementById('1712.05896v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 December, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Tech Report</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1712.03351">arXiv:1712.03351</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1712.03351">pdf</a>, <a href="https://arxiv.org/format/1712.03351">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Peephole: Predicting Network Performance Before Training
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Deng%2C+B">Boyang Deng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junjie Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+D">Dahua Lin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1712.03351v1-abstract-short" style="display: inline;">
        The quest for performant networks has been a significant force that drives the advancements of deep learning in recent years. While rewarding, improving network design has never been an easy journey. The large design space combined with the tremendous cost required for network training poses a major obstacle to this endeavor. In this work, we propose a new approach to this problem, namely, predict&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1712.03351v1-abstract-full').style.display = 'inline'; document.getElementById('1712.03351v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1712.03351v1-abstract-full" style="display: none;">
        The quest for performant networks has been a significant force that drives the advancements of deep learning in recent years. While rewarding, improving network design has never been an easy journey. The large design space combined with the tremendous cost required for network training poses a major obstacle to this endeavor. In this work, we propose a new approach to this problem, namely, predicting the performance of a network before training, based on its architecture. Specifically, we develop a unified way to encode individual layers into vectors and bring them together to form an integrated description via LSTM. Taking advantage of the recurrent network&#39;s strong expressive power, this method can reliably predict the performances of various network architectures. Our empirical studies showed that it not only achieved accurate predictions but also produced consistent rankings across datasets -- a key desideratum in performance prediction.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1712.03351v1-abstract-full').style.display = 'none'; document.getElementById('1712.03351v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 December, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2017.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1712.02270">arXiv:1712.02270</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1712.02270">pdf</a>, <a href="https://arxiv.org/ps/1712.02270">ps</a>, <a href="https://arxiv.org/format/1712.02270">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Genomics">q-bio.GN</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Attention based convolutional neural network for predicting RNA-protein binding sites
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Pan%2C+X">Xiaoyong Pan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junchi Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1712.02270v1-abstract-short" style="display: inline;">
        RNA-binding proteins (RBPs) play crucial roles in many biological processes, e.g. gene regulation. Computational identification of RBP binding sites on RNAs are urgently needed. In particular, RBPs bind to RNAs by recognizing sequence motifs. Thus, fast locating those motifs on RNA sequences is crucial and time-efficient for determining whether the RNAs interact with the RBPs or not. In this study&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1712.02270v1-abstract-full').style.display = 'inline'; document.getElementById('1712.02270v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1712.02270v1-abstract-full" style="display: none;">
        RNA-binding proteins (RBPs) play crucial roles in many biological processes, e.g. gene regulation. Computational identification of RBP binding sites on RNAs are urgently needed. In particular, RBPs bind to RNAs by recognizing sequence motifs. Thus, fast locating those motifs on RNA sequences is crucial and time-efficient for determining whether the RNAs interact with the RBPs or not. In this study, we present an attention based convolutional neural network, iDeepA, to predict RNA-protein binding sites from raw RNA sequences. We first encode RNA sequences into one-hot encoding. Next, we design a deep learning model with a convolutional neural network (CNN) and an attention mechanism, which automatically search for important positions, e.g. binding motifs, to learn discriminant high-level features for predicting RBP binding sites. We evaluate iDeepA on publicly gold-standard RBP binding sites derived from CLIP-seq data. The results demonstrate iDeepA achieves comparable performance with other state-of-the-art methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1712.02270v1-abstract-full').style.display = 'none'; document.getElementById('1712.02270v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 December, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2017.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        NIPS 2017 Computational Biology Workshop
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1712.01994">arXiv:1712.01994</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1712.01994">pdf</a>, <a href="https://arxiv.org/format/1712.01994">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A High-resolution DOA Estimation Method with a Family of Nonconvex Penalties
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+X">Xiaohuan Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+W">Wei-Ping Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jun Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1712.01994v1-abstract-short" style="display: inline;">
        The low-rank matrix reconstruction (LRMR) approach is widely used in direction-of-arrival (DOA) estimation. As the rank norm penalty in an LRMR is NP-hard to compute, the nuclear norm (or the trace norm for a positive semidefinite (PSD) matrix) has been often employed as a convex relaxation of the rank norm. However, solving a nuclear norm convex problem may lead to a suboptimal solution of the or&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1712.01994v1-abstract-full').style.display = 'inline'; document.getElementById('1712.01994v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1712.01994v1-abstract-full" style="display: none;">
        The low-rank matrix reconstruction (LRMR) approach is widely used in direction-of-arrival (DOA) estimation. As the rank norm penalty in an LRMR is NP-hard to compute, the nuclear norm (or the trace norm for a positive semidefinite (PSD) matrix) has been often employed as a convex relaxation of the rank norm. However, solving a nuclear norm convex problem may lead to a suboptimal solution of the original rank norm problem. In this paper, we propose to apply a family of nonconvex penalties on the singular values of the covariance matrix as the sparsity metrics to approximate the rank norm. In particular, we formulate a nonconvex minimization problem and solve it by using a locally convergent iterative reweighted strategy in order to enhance the sparsity and resolution. The problem in each iteration is convex and hence can be solved by using the optimization toolbox. Convergence analysis shows that the new method is able to obtain a suboptimal solution. The connection between the proposed method and the sparse signal reconstruction (SSR) is explored showing that our method can be regarded as a sparsity-based method with the number of sampling grids approaching infinity. Two feasible implementation algorithms that are based on solving a duality problem and deducing a closed-form solution of the simplified problem are also provided for the convex problem at each iteration to expedite the convergence. Extensive simulation studies are conducted to show the superiority of the proposed methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1712.01994v1-abstract-full').style.display = 'none'; document.getElementById('1712.01994v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 December, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2017.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1712.01497">arXiv:1712.01497</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1712.01497">pdf</a>, <a href="https://arxiv.org/format/1712.01497">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Gridless Two-dimensional DOA Estimation With L-shaped Array Based on the Cross-covariance Matrix
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+X">Xiaohuan Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+W">Wei-Ping Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jun Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1712.01497v1-abstract-short" style="display: inline;">
        The atomic norm minimization (ANM) has been successfully incorporated into the two-dimensional (2-D) direction-of-arrival (DOA) estimation problem for super-resolution. However, its computational workload might be unaffordable when the number of snapshots is large. In this paper, we propose two gridless methods for 2-D DOA estimation with L-shaped array based on the atomic norm to improve the comp&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1712.01497v1-abstract-full').style.display = 'inline'; document.getElementById('1712.01497v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1712.01497v1-abstract-full" style="display: none;">
        The atomic norm minimization (ANM) has been successfully incorporated into the two-dimensional (2-D) direction-of-arrival (DOA) estimation problem for super-resolution. However, its computational workload might be unaffordable when the number of snapshots is large. In this paper, we propose two gridless methods for 2-D DOA estimation with L-shaped array based on the atomic norm to improve the computational efficiency. Firstly, by exploiting the cross-covariance matrix an ANM-based model has been proposed. We then prove that this model can be efficiently solved as a semi-definite programming (SDP). Secondly, a modified model has been presented to improve the estimation accuracy. It is shown that our proposed methods can be applied to both uniform and sparse L-shaped arrays and do not require any knowledge of the number of sources. Furthermore, since our methods greatly reduce the model size as compared to the conventional ANM method, and thus are much more efficient. Simulations results are provided to demonstrate the advantage of our methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1712.01497v1-abstract-full').style.display = 'none'; document.getElementById('1712.01497v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 December, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2017.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1711.09584">arXiv:1711.09584</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1711.09584">pdf</a>, <a href="https://arxiv.org/format/1711.09584">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Joint Cuts and Matching of Partitions in One Graph
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+T">Tianshu Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junchi Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+J">Jieyi Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+B">Baoxin Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1711.09584v1-abstract-short" style="display: inline;">
        As two fundamental problems, graph cuts and graph matching have been investigated over decades, resulting in vast literature in these two topics respectively. However the way of jointly applying and solving graph cuts and matching receives few attention. In this paper, we first formalize the problem of simultaneously cutting a graph into two partitions i.e. graph cuts and establishing their corres&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1711.09584v1-abstract-full').style.display = 'inline'; document.getElementById('1711.09584v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1711.09584v1-abstract-full" style="display: none;">
        As two fundamental problems, graph cuts and graph matching have been investigated over decades, resulting in vast literature in these two topics respectively. However the way of jointly applying and solving graph cuts and matching receives few attention. In this paper, we first formalize the problem of simultaneously cutting a graph into two partitions i.e. graph cuts and establishing their correspondence i.e. graph matching. Then we develop an optimization algorithm by updating matching and cutting alternatively, provided with theoretical analysis. The efficacy of our algorithm is verified on both synthetic dataset and real-world images containing similar regions or structures.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1711.09584v1-abstract-full').style.display = 'none'; document.getElementById('1711.09584v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 November, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2017.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Yan%2C+J&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Yan%2C+J&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Yan%2C+J&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Yan%2C+J&amp;start=100"
            class="pagination-link "
            aria-label="Page 3"
            aria-current="page">3
          </a>
        </li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://confluence.cornell.edu/x/giazFQ">Search v0.5 released 2018-12-20</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>

  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About arXiv</a></li>
          <li><a href="https://arxiv.org/about/people/leadership_team">Leadership Team</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li><span class="icon"><i class="fa fa-envelope"></i></span><a href="https://arxiv.org/help/contact"> Contact</a></li>
          <li><span class="icon"><i class="fa fa-twitter"></i></span><a href="https://twitter.com/arxiv"> Follow us on Twitter</a></li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help">Help</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://blogs.cornell.edu/arxiv">Blog</a></li>
          <li><a href="https://arxiv.org/help/subscribe"> Subscribe</a></li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>

<div class="columns" style="border-top: 1px solid #979797">
  <div class="column">
    <p class="help">arXiv&#174; is a registered trademark of Cornell University.</p>
  </div>
  <div class="column">
    <p class="help">If you have a disability and are having trouble accessing information
      on this website or need materials in an alternate format, contact
      <a href="mailto:web-accessibility@cornell.edu">web-accessibility@cornell.edu</a> for
       assistance.</p>
  </div>
</div>
    
  </footer>
  </body>
</html>


#####EOF#####


<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.15.2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.15.2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.15.2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.15.2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>

<script src="https://static.arxiv.org/static/base/0.15.2/js/notification.js"></script>
<!-- Piwik -->
<script type="text/javascript">
var _paq = _paq || [];
_paq.push(["setDomains", ["*.arxiv.org"]]);
_paq.push(['trackPageView']);
_paq.push(['enableLinkTracking']);
(function()
{ var u="//webanalytics.library.cornell.edu/"; _paq.push(['setTrackerUrl', u+'piwik.php']); _paq.push(['setSiteId', 538]); var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0]; g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s); }
)();
</script>
<!-- End Piwik Code -->
    <!-- Custom style sheets or other head includes -->
    
  <link rel="stylesheet" href="https://static.arxiv.org/static/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/css/search.css" />

  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14157"],  // Release search-0.5
        "customfield_11401": window.location.href
      }
    };
    </script>


  </head>
  <body>
  
  <noscript><img src="//webanalytics.library.cornell.edu/piwik.php?idsite=538&rec=1" style="border:0;" alt="" /></noscript>
  <header>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.15.2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo"><img src="https://static.arxiv.org/static/base/0.15.2/images/arxiv-logo-web.svg" alt="arXiv" aria-label="logo" width="85" /></a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>

  <main role="main" class="container">
    

    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;10 of 10 results for author: <span class="mathjax">Simon, L</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://confluence.cornell.edu/x/giazFQ">Search v0.5 released 2018-12-20</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Simon%2C+L">Search in all archives.</a>
    
    
    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Simon, L">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Simon%2C+L&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Simon, L">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      




<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.11137">arXiv:1903.11137</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.11137">pdf</a>, <a href="https://arxiv.org/format/1903.11137">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hearing your touch: A new acoustic side channel on smartphones
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shumailov%2C+I">Ilia Shumailov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Simon%2C+L">Laurent Simon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jeff Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+R">Ross Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.11137v1-abstract-short" style="display: inline;">
        We present the first acoustic side-channel attack that recovers what users type on the virtual keyboard of their touch-screen smartphone or tablet. When a user taps the screen with a finger, the tap generates a sound wave that propagates on the screen surface and in the air. We found the device's microphone(s) can recover this wave and "hear" the finger's touch, and the wave's distortions are char&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.11137v1-abstract-full').style.display = 'inline'; document.getElementById('1903.11137v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.11137v1-abstract-full" style="display: none;">
        We present the first acoustic side-channel attack that recovers what users type on the virtual keyboard of their touch-screen smartphone or tablet. When a user taps the screen with a finger, the tap generates a sound wave that propagates on the screen surface and in the air. We found the device&#39;s microphone(s) can recover this wave and &#34;hear&#34; the finger&#39;s touch, and the wave&#39;s distortions are characteristic of the tap&#39;s location on the screen. Hence, by recording audio through the built-in microphone(s), a malicious app can infer text as the user enters it on their device. We evaluate the effectiveness of the attack with 45 participants in a real-world environment on an Android tablet and an Android smartphone. For the tablet, we recover 61% of 200 4-digit PIN-codes within 20 attempts, even if the model is not trained with the victim&#39;s data. For the smartphone, we recover 9 words of size 7--13 letters with 50 attempts in a common side-channel attack benchmark. Our results suggest that it not always sufficient to rely on isolation mechanisms such as TrustZone to protect user input. We propose and discuss hardware, operating-system and application-level mechanisms to block this attack more effectively. Mobile devices may need a richer capability model, a more user-friendly notification system for sensor usage and a more thorough evaluation of the information leaked by the underlying hardware.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.11137v1-abstract-full').style.display = 'none'; document.getElementById('1903.11137v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 March, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Paper built on the MPhil thesis of Ilia Shumailov. 2017</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1901.03396">arXiv:1901.03396</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1901.03396">pdf</a>, <a href="https://arxiv.org/format/1901.03396">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Detecting Overfitting of Deep Generative Networks via Latent Recovery
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Webster%2C+R">Ryan Webster</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rabin%2C+J">Julien Rabin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Simon%2C+L">Loic Simon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jurie%2C+F">Frederic Jurie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1901.03396v1-abstract-short" style="display: inline;">
        State of the art deep generative networks are capable of producing images with such incredible realism that they can be suspected of memorizing training images. It is why it is not uncommon to include visualizations of training set nearest neighbors, to suggest generated images are not simply memorized. We demonstrate this is not sufficient and motivates the need to study memorization/overfitting&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.03396v1-abstract-full').style.display = 'inline'; document.getElementById('1901.03396v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1901.03396v1-abstract-full" style="display: none;">
        State of the art deep generative networks are capable of producing images with such incredible realism that they can be suspected of memorizing training images. It is why it is not uncommon to include visualizations of training set nearest neighbors, to suggest generated images are not simply memorized. We demonstrate this is not sufficient and motivates the need to study memorization/overfitting of deep generators with more scrutiny. This paper addresses this question by i) showing how simple losses are highly effective at reconstructing images for deep generators ii) analyzing the statistics of reconstruction errors when reconstructing training and validation images, which is the standard way to analyze overfitting in machine learning. Using this methodology, this paper shows that overfitting is not detectable in the pure GAN models proposed in the literature, in contrast with those using hybrid adversarial losses, which are amongst the most widely applied generative methods. The paper also shows that standard GAN evaluation metrics fail to capture memorization for some deep generators. Finally, the paper also shows how off-the-shelf GAN generators can be successfully applied to face inpainting and face super-resolution using the proposed reconstruction method, without hybrid adversarial losses.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.03396v1-abstract-full').style.display = 'none'; document.getElementById('1901.03396v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 January, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1806.03051">arXiv:1806.03051</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1806.03051">pdf</a>, <a href="https://arxiv.org/format/1806.03051">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep multi-scale architectures for monocular depth estimation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Moukari%2C+M">Michel Moukari</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Picard%2C+S">Sylvaine Picard</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Simon%2C+L">Loic Simon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jurie%2C+F">Frdric Jurie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1806.03051v1-abstract-short" style="display: inline;">
        This paper aims at understanding the role of multi-scale information in the estimation of depth from monocular images. More precisely, the paper investigates four different deep CNN architectures, designed to explicitly make use of multi-scale features along the network, and compare them to a state-of-the-art single-scale approach. The paper also shows that involving multi-scale features in depth&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.03051v1-abstract-full').style.display = 'inline'; document.getElementById('1806.03051v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1806.03051v1-abstract-full" style="display: none;">
        This paper aims at understanding the role of multi-scale information in the estimation of depth from monocular images. More precisely, the paper investigates four different deep CNN architectures, designed to explicitly make use of multi-scale features along the network, and compare them to a state-of-the-art single-scale approach. The paper also shows that involving multi-scale features in depth estimation not only improves the performance in terms of accuracy, but also gives qualitatively better depth maps. Experiments are done on the widely used NYU Depth dataset, on which the proposed method achieves state-of-the-art performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.03051v1-abstract-full').style.display = 'none'; document.getElementById('1806.03051v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 June, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1702.02382">arXiv:1702.02382</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1702.02382">pdf</a>, <a href="https://arxiv.org/ps/1702.02382">ps</a>, <a href="https://arxiv.org/format/1702.02382">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Adversarial Regularisation for Semi-Supervised Training of Structured Output Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kozi%C5%84ski%2C+M">Mateusz Koziski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Simon%2C+L">Loc Simon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jurie%2C+F">Frdric Jurie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1702.02382v1-abstract-short" style="display: inline;">
        We propose a method for semi-supervised training of structured-output neural networks. Inspired by the framework of Generative Adversarial Networks (GAN), we train a discriminator network to capture the notion of a quality of network output. To this end, we leverage the qualitative difference between outputs obtained on the labelled training data and unannotated data. We then use the discriminator&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1702.02382v1-abstract-full').style.display = 'inline'; document.getElementById('1702.02382v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1702.02382v1-abstract-full" style="display: none;">
        We propose a method for semi-supervised training of structured-output neural networks. Inspired by the framework of Generative Adversarial Networks (GAN), we train a discriminator network to capture the notion of a quality of network output. To this end, we leverage the qualitative difference between outputs obtained on the labelled training data and unannotated data. We then use the discriminator as a source of error signal for unlabelled data. This effectively boosts the performance of a network on a held out test set. Initial experiments in image segmentation demonstrate that the proposed framework enables achieving the same network performance as in a fully supervised scenario, while using two times less annotations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1702.02382v1-abstract-full').style.display = 'none'; document.getElementById('1702.02382v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 February, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2017.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1505.00898">arXiv:1505.00898</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1505.00898">pdf</a>, <a href="https://arxiv.org/ps/1505.00898">ps</a>, <a href="https://arxiv.org/format/1505.00898">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Dynamical Systems">math.DS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Bounds for the expected value of one-step processes
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Armbruster%2C+B">Benjamin Armbruster</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Besenyei%2C+%C3%81">dm Besenyei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Simon%2C+P+L">Pter L. Simon</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1505.00898v2-abstract-short" style="display: inline;">
        Mean-field models are often used to approximate Markov processes with large state-spaces. One-step processes, also known as birth-death processes, are an important class of such processes and are processes with state space $\{0,1,\ldots,N\}$ and where each transition is of size one. We derive explicit bounds on the expected value of such a process, bracketing it between the mean-field model and an&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1505.00898v2-abstract-full').style.display = 'inline'; document.getElementById('1505.00898v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1505.00898v2-abstract-full" style="display: none;">
        Mean-field models are often used to approximate Markov processes with large state-spaces. One-step processes, also known as birth-death processes, are an important class of such processes and are processes with state space $\{0,1,\ldots,N\}$ and where each transition is of size one. We derive explicit bounds on the expected value of such a process, bracketing it between the mean-field model and another simple ODE. Our bounds require that the Markov transition rates are density dependent polynomials that satisfy a sign condition. We illustrate the tightness of our bounds on the SIS epidemic process and the voter model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1505.00898v2-abstract-full').style.display = 'none'; document.getElementById('1505.00898v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 December, 2015; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 May, 2015;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2015.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, 4 figures, revised</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          60J75; 34C11; 92D30
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1412.7448">arXiv:1412.7448</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1412.7448">pdf</a>, <a href="https://arxiv.org/format/1412.7448">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Systemization of Pluggable Transports for Censorship Resistance
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Khattak%2C+S">Sheharbano Khattak</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Simon%2C+L">Laurent Simon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Murdoch%2C+S+J">Steven J. Murdoch</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1412.7448v2-abstract-short" style="display: inline;">
        An increasing number of countries implement Internet censorship at different scales and for a variety of reasons. In particular, the link between the censored client and entry point to the uncensored network is a frequent target of censorship due to the ease with which a nation-state censor can control it. A number of censorship resistance systems have been developed thus far to help circumvent bl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1412.7448v2-abstract-full').style.display = 'inline'; document.getElementById('1412.7448v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1412.7448v2-abstract-full" style="display: none;">
        An increasing number of countries implement Internet censorship at different scales and for a variety of reasons. In particular, the link between the censored client and entry point to the uncensored network is a frequent target of censorship due to the ease with which a nation-state censor can control it. A number of censorship resistance systems have been developed thus far to help circumvent blocking on this link, which we refer to as link circumvention systems (LCs). The variety and profusion of attack vectors available to a censor has led to an arms race, leading to a dramatic speed of evolution of LCs. Despite their inherent complexity and the breadth of work in this area, there is no systematic way to evaluate link circumvention systems and compare them against each other. In this paper, we (i) sketch an attack model to comprehensively explore a censor&#39;s capabilities, (ii) present an abstract model of a LC, a system that helps a censored client communicate with a server over the Internet while resisting censorship, (iii) describe an evaluation stack that underscores a layered approach to evaluate LCs, and (iv) systemize and evaluate existing censorship resistance systems that provide link circumvention. We highlight open challenges in the evaluation and development of LCs and discuss possible mitigations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1412.7448v2-abstract-full').style.display = 'none'; document.getElementById('1412.7448v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 July, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 December, 2014;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2014.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Content from this paper was published in Proceedings on Privacy Enhancing Technologies (PoPETS), Volume 2016, Issue 4 (July 2016) as &#34;SoK: Making Sense of Censorship Resistance Systems&#34; by Sheharbano Khattak, Tariq Elahi, Laurent Simon, Colleen M. Swanson, Steven J. Murdoch and Ian Goldberg (DOI 10.1515/popets-2016-0028)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1407.2221">arXiv:1407.2221</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1407.2221">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sonic interaction with a virtual orchestra of factory machinery
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Simon%2C+L">Laurent Simon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nouviale%2C+F">Florian Nouviale</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gaugne%2C+R">Ronan Gaugne</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gouranton%2C+V">Valrie Gouranton</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1407.2221v1-abstract-short" style="display: inline;">
        This paper presents an immersive application where users receive sound and visual feedbacks on their interactions with a virtual environment. In this application, the users play the part of conductors of an orchestra of factory machines since each of their actions on interaction devices triggers a pair of visual and audio responses. Audio stimuli were spatialized around the listener. The applicati&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1407.2221v1-abstract-full').style.display = 'inline'; document.getElementById('1407.2221v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1407.2221v1-abstract-full" style="display: none;">
        This paper presents an immersive application where users receive sound and visual feedbacks on their interactions with a virtual environment. In this application, the users play the part of conductors of an orchestra of factory machines since each of their actions on interaction devices triggers a pair of visual and audio responses. Audio stimuli were spatialized around the listener. The application was exhibited during the 2013 Science and Music day and designed to be used in a large immersive system with head tracking, shutter glasses and a 10.2 loudspeaker configuration.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1407.2221v1-abstract-full').style.display = 'none'; document.getElementById('1407.2221v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 July, 2014; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2014.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Sonic Interaction for Virtual Environments, Minneapolis : United States (2014)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1112.6178">arXiv:1112.6178</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1112.6178">pdf</a>, <a href="https://arxiv.org/ps/1112.6178">ps</a>, <a href="https://arxiv.org/format/1112.6178">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A general framework for online audio source separation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Simon%2C+L+S+R">Laurent S. R. Simon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vincent%2C+E">Emmanuel Vincent</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1112.6178v1-abstract-short" style="display: inline;">
        We consider the problem of online audio source separation. Existing algorithms adopt either a sliding block approach or a stochastic gradient approach, which is faster but less accurate. Also, they rely either on spatial cues or on spectral cues and cannot separate certain mixtures. In this paper, we design a general online audio source separation framework that combines both approaches and both t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1112.6178v1-abstract-full').style.display = 'inline'; document.getElementById('1112.6178v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1112.6178v1-abstract-full" style="display: none;">
        We consider the problem of online audio source separation. Existing algorithms adopt either a sliding block approach or a stochastic gradient approach, which is faster but less accurate. Also, they rely either on spatial cues or on spectral cues and cannot separate certain mixtures. In this paper, we design a general online audio source separation framework that combines both approaches and both types of cues. The model parameters are estimated in the Maximum Likelihood (ML) sense using a Generalised Expectation Maximisation (GEM) algorithm with multiplicative updates. The separation performance is evaluated as a function of the block size and the step size and compared to that of an offline algorithm.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1112.6178v1-abstract-full').style.display = 'none'; document.getElementById('1112.6178v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 December, 2011; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2011.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">International conference on Latente Variable Analysis and Signal Separation (2012)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1109.5716">arXiv:1109.5716</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1109.5716">pdf</a>, <a href="https://arxiv.org/ps/1109.5716">ps</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1613/jair.1785">10.1613/jair.1785 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Distributed Reasoning in a Peer-to-Peer Setting: Application to the Semantic Web
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Adjiman%2C+P">P. Adjiman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chatalic%2C+P">P. Chatalic</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Goasdoue%2C+F">F. Goasdoue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rousset%2C+M+C">M. C. Rousset</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Simon%2C+L">L. Simon</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1109.5716v1-abstract-short" style="display: inline;">
        In a peer-to-peer inference system, each peer can reason locally but can also solicit some of its acquaintances, which are peers sharing part of its vocabulary. In this paper, we consider peer-to-peer inference systems in which the local theory of each peer is a set of propositional clauses defined upon a local vocabulary. An important characteristic of peer-to-peer inference systems is that the g&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1109.5716v1-abstract-full').style.display = 'inline'; document.getElementById('1109.5716v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1109.5716v1-abstract-full" style="display: none;">
        In a peer-to-peer inference system, each peer can reason locally but can also solicit some of its acquaintances, which are peers sharing part of its vocabulary. In this paper, we consider peer-to-peer inference systems in which the local theory of each peer is a set of propositional clauses defined upon a local vocabulary. An important characteristic of peer-to-peer inference systems is that the global theory (the union of all peer theories) is not known (as opposed to partition-based reasoning systems). The main contribution of this paper is to provide the first consequence finding algorithm in a peer-to-peer setting: DeCA. It is anytime and computes consequences gradually from the solicited peer to peers that are more and more distant. We exhibit a sufficient condition on the acquaintance graph of the peer-to-peer inference system for guaranteeing the completeness of this algorithm. Another important contribution is to apply this general distributed reasoning setting to the setting of the Semantic Web through the Somewhere semantic peer-to-peer data management system. The last contribution of this paper is to provide an experimental analysis of the scalability of the peer-to-peer infrastructure that we propose, on large networks of 1000 peers. 
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1109.5716v1-abstract-full').style.display = 'none'; document.getElementById('1109.5716v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 September, 2011; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2011.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Journal Of Artificial Intelligence Research, Volume 25, pages 269-314, 2006
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/0710.4823">arXiv:0710.4823</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/0710.4823">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Coprocessor for Accelerating Visual Information Processing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Stechele%2C+W">W. Stechele</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Carcel%2C+L+A">L. Alvado Carcel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Herrmann%2C+S">S. Herrmann</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Simon%2C+J+L">J. Lidon Simon</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="0710.4823v1-abstract-short" style="display: inline;">
        Visual information processing will play an increasingly important role in future electronics systems. In many applications, e.g. video surveillance cameras, data throughput of microprocessors is not sufficient and power consumption is too high. Instruction profiling on a typical test algorithm has shown that pixel address calculations are the dominant operations to be optimized. Therefore Addres&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('0710.4823v1-abstract-full').style.display = 'inline'; document.getElementById('0710.4823v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="0710.4823v1-abstract-full" style="display: none;">
          Visual information processing will play an increasingly important role in future electronics systems. In many applications, e.g. video surveillance cameras, data throughput of microprocessors is not sufficient and power consumption is too high. Instruction profiling on a typical test algorithm has shown that pixel address calculations are the dominant operations to be optimized. Therefore AddressLib, a structured scheme for pixel addressing was developed, that can be accelerated by AddressEngine, a coprocessor for visual information processing. In this paper, the architectural design of AddressEngine is described, which in the first step supports a subset of the AddressLib. Dataflow and memory organization are optimized during architectural design. AddressEngine was implemented in a FPGA and was tested with MPEG-7 Global Motion Estimation algorithm. Results on processing speed and circuit complexity are given and compared to a pure software implementation. The next step will be the support for the full AddressLib, including segment addressing. An outlook on further investigations on dynamic reconfiguration capabilities is given.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('0710.4823v1-abstract-full').style.display = 'none'; document.getElementById('0710.4823v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 October, 2007; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2007.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted on behalf of EDAA (http://www.edaa.com/)</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Dans Design, Automation and Test in Europe | Designers&#39;Forum - DATE&#39;05, Munich : Allemagne (2005)
      </p>
    
  </li>

</ol>


  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://confluence.cornell.edu/x/giazFQ">Search v0.5 released 2018-12-20</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>

  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About arXiv</a></li>
          <li><a href="https://arxiv.org/about/people/leadership_team">Leadership Team</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li><span class="icon"><i class="fa fa-envelope"></i></span><a href="https://arxiv.org/help/contact"> Contact</a></li>
          <li><span class="icon"><i class="fa fa-twitter"></i></span><a href="https://twitter.com/arxiv"> Follow us on Twitter</a></li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help">Help</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://blogs.cornell.edu/arxiv">Blog</a></li>
          <li><a href="https://arxiv.org/help/subscribe"> Subscribe</a></li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>

<div class="columns" style="border-top: 1px solid #979797">
  <div class="column">
    <p class="help">arXiv&#174; is a registered trademark of Cornell University.</p>
  </div>
  <div class="column">
    <p class="help">If you have a disability and are having trouble accessing information
      on this website or need materials in an alternate format, contact
      <a href="mailto:web-accessibility@cornell.edu">web-accessibility@cornell.edu</a> for
       assistance.</p>
  </div>
</div>
    
  </footer>
  </body>
</html>


#####EOF#####





#####EOF#####





#####EOF#####


<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.15.2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.15.2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.15.2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.15.2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>

<script src="https://static.arxiv.org/static/base/0.15.2/js/notification.js"></script>
<!-- Piwik -->
<script type="text/javascript">
var _paq = _paq || [];
_paq.push(["setDomains", ["*.arxiv.org"]]);
_paq.push(['trackPageView']);
_paq.push(['enableLinkTracking']);
(function()
{ var u="//webanalytics.library.cornell.edu/"; _paq.push(['setTrackerUrl', u+'piwik.php']); _paq.push(['setSiteId', 538]); var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0]; g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s); }
)();
</script>
<!-- End Piwik Code -->
    <!-- Custom style sheets or other head includes -->
    
  <link rel="stylesheet" href="https://static.arxiv.org/static/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/css/search.css" />

  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14157"],  // Release search-0.5
        "customfield_11401": window.location.href
      }
    };
    </script>


  </head>
  <body>
  
  <noscript><img src="//webanalytics.library.cornell.edu/piwik.php?idsite=538&rec=1" style="border:0;" alt="" /></noscript>
  <header>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.15.2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo"><img src="https://static.arxiv.org/static/base/0.15.2/images/arxiv-logo-web.svg" alt="arXiv" aria-label="logo" width="85" /></a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>

  <main role="main" class="container">
    

    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;18 of 18 results for author: <span class="mathjax">Anderson, R</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://confluence.cornell.edu/x/giazFQ">Search v0.5 released 2018-12-20</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Anderson%2C+R">Search in all archives.</a>
    
    
    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Anderson, R">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Anderson%2C+R&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Anderson, R">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      




<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.11137">arXiv:1903.11137</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.11137">pdf</a>, <a href="https://arxiv.org/format/1903.11137">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hearing your touch: A new acoustic side channel on smartphones
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shumailov%2C+I">Ilia Shumailov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Simon%2C+L">Laurent Simon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Jeff Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+R">Ross Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.11137v1-abstract-short" style="display: inline;">
        We present the first acoustic side-channel attack that recovers what users type on the virtual keyboard of their touch-screen smartphone or tablet. When a user taps the screen with a finger, the tap generates a sound wave that propagates on the screen surface and in the air. We found the device's microphone(s) can recover this wave and "hear" the finger's touch, and the wave's distortions are char&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.11137v1-abstract-full').style.display = 'inline'; document.getElementById('1903.11137v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.11137v1-abstract-full" style="display: none;">
        We present the first acoustic side-channel attack that recovers what users type on the virtual keyboard of their touch-screen smartphone or tablet. When a user taps the screen with a finger, the tap generates a sound wave that propagates on the screen surface and in the air. We found the device&#39;s microphone(s) can recover this wave and &#34;hear&#34; the finger&#39;s touch, and the wave&#39;s distortions are characteristic of the tap&#39;s location on the screen. Hence, by recording audio through the built-in microphone(s), a malicious app can infer text as the user enters it on their device. We evaluate the effectiveness of the attack with 45 participants in a real-world environment on an Android tablet and an Android smartphone. For the tablet, we recover 61% of 200 4-digit PIN-codes within 20 attempts, even if the model is not trained with the victim&#39;s data. For the smartphone, we recover 9 words of size 7--13 letters with 50 attempts in a common side-channel attack benchmark. Our results suggest that it not always sufficient to rely on isolation mechanisms such as TrustZone to protect user input. We propose and discuss hardware, operating-system and application-level mechanisms to block this attack more effectively. Mobile devices may need a richer capability model, a more user-friendly notification system for sensor usage and a more thorough evaluation of the information leaked by the underlying hardware.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.11137v1-abstract-full').style.display = 'none'; document.getElementById('1903.11137v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 March, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Paper built on the MPhil thesis of Ilia Shumailov. 2017</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1901.08121">arXiv:1901.08121</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1901.08121">pdf</a>, <a href="https://arxiv.org/format/1901.08121">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sitatapatra: Blocking the Transfer of Adversarial Samples
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shumailov%2C+I">Ilia Shumailov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+X">Xitong Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yiren Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mullins%2C+R">Robert Mullins</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+R">Ross Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+C">Cheng-Zhong Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1901.08121v1-abstract-short" style="display: inline;">
        Convolutional Neural Networks (CNNs) are widely used to solve classification tasks in computer vision. However, they can be tricked into misclassifying specially crafted `adversarial' samples -- and samples built to trick one model often work alarmingly well against other models trained on the same task. In this paper we introduce Sitatapatra, a system designed to block the transfer of adversarial&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.08121v1-abstract-full').style.display = 'inline'; document.getElementById('1901.08121v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1901.08121v1-abstract-full" style="display: none;">
        Convolutional Neural Networks (CNNs) are widely used to solve classification tasks in computer vision. However, they can be tricked into misclassifying specially crafted `adversarial&#39; samples -- and samples built to trick one model often work alarmingly well against other models trained on the same task. In this paper we introduce Sitatapatra, a system designed to block the transfer of adversarial samples. It diversifies neural networks using a key, as in cryptography, and provides a mechanism for detecting attacks. What&#39;s more, when adversarial samples are detected they can typically be traced back to the individual device that was used to develop them. The run-time overheads are minimal permitting the use of Sitatapatra on constrained systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.08121v1-abstract-full').style.display = 'none'; document.getElementById('1901.08121v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 January, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1901.01769">arXiv:1901.01769</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1901.01769">pdf</a>, <a href="https://arxiv.org/format/1901.01769">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Tendrils of Crime: Visualizing the Diffusion of Stolen Bitcoins
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ahmed%2C+M">Mansoor Ahmed</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shumailov%2C+I">Ilia Shumailov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+R">Ross Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1901.01769v1-abstract-short" style="display: inline;">
        The first six months of 2018 have seen cryptocurrency thefts of $761 million, and the technology is also the latest and greatest tool for money laundering. This increase in crime has caused both researchers and law enforcement to look for ways to trace criminal proceeds. Although tracing algorithms have improved recently, they still yield an enormous amount of data of which very few datapoints are&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.01769v1-abstract-full').style.display = 'inline'; document.getElementById('1901.01769v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1901.01769v1-abstract-full" style="display: none;">
        The first six months of 2018 have seen cryptocurrency thefts of $761 million, and the technology is also the latest and greatest tool for money laundering. This increase in crime has caused both researchers and law enforcement to look for ways to trace criminal proceeds. Although tracing algorithms have improved recently, they still yield an enormous amount of data of which very few datapoints are relevant or interesting to investigators, let alone ordinary bitcoin owners interested in provenance. In this work we describe efforts to visualize relevant data on a blockchain. To accomplish this we come up with a graphical model to represent the stolen coins and then implement this using a variety of visualization techniques.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.01769v1-abstract-full').style.display = 'none'; document.getElementById('1901.01769v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 January, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at The Fifth International Workshop on Graphical Models for Security, hosted at FLoC 2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.08359">arXiv:1811.08359</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.08359">pdf</a>, <a href="https://arxiv.org/ps/1811.08359">ps</a>, <a href="https://arxiv.org/format/1811.08359">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Strong mixed-integer programming formulations for trained neural networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+R">Ross Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huchette%2C+J">Joey Huchette</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tjandraatmadja%2C+C">Christian Tjandraatmadja</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vielma%2C+J+P">Juan Pablo Vielma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.08359v2-abstract-short" style="display: inline;">
        We present an ideal mixed-integer programming (MIP) formulation for a rectified linear unit (ReLU) appearing in a trained neural network. Our formulation requires a single binary variable and no additional continuous variables beyond the input and output variables of the ReLU. We contrast it with an ideal "extended" formulation with a linear number of additional continuous variables, derived throu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.08359v2-abstract-full').style.display = 'inline'; document.getElementById('1811.08359v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.08359v2-abstract-full" style="display: none;">
        We present an ideal mixed-integer programming (MIP) formulation for a rectified linear unit (ReLU) appearing in a trained neural network. Our formulation requires a single binary variable and no additional continuous variables beyond the input and output variables of the ReLU. We contrast it with an ideal &#34;extended&#34; formulation with a linear number of additional continuous variables, derived through standard techniques. An apparent drawback of our formulation is that it requires an exponential number of inequality constraints, but we provide a routine to separate the inequalities in linear time. We also prove that these exponentially-many constraints are facet-defining under mild conditions. Finally, we study network verification problems and observe that dynamically separating from the exponential inequalities 1) is much more computationally efficient and scalable than the extended formulation, 2) decreases the solve time of a state-of-the-art MIP solver by a factor of 7 on smaller instances, and 3) nearly matches the dual bounds of a state-of-the-art MIP solver on harder instances, after just a few rounds of separation and in orders of magnitude less time.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.08359v2-abstract-full').style.display = 'none'; document.getElementById('1811.08359v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 February, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 November, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Extended abstract of arXiv:1811.01988 [math.OC]</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.07375">arXiv:1811.07375</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.07375">pdf</a>, <a href="https://arxiv.org/format/1811.07375">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Taboo Trap: Behavioural Detection of Adversarial Samples
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shumailov%2C+I">Ilia Shumailov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yiren Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mullins%2C+R">Robert Mullins</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+R">Ross Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.07375v1-abstract-short" style="display: inline;">
        Deep Neural Networks (DNNs) have become a powerful tool for a wide range of problems. Yet recent work has shown an increasing variety of adversarial samples that can fool them. Most existing detection mechanisms impose significant costs, either by using additional classifiers to spot adversarial samples, or by requiring the DNN to be restructured. In this paper, we introduce a novel defence. We tr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.07375v1-abstract-full').style.display = 'inline'; document.getElementById('1811.07375v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.07375v1-abstract-full" style="display: none;">
        Deep Neural Networks (DNNs) have become a powerful tool for a wide range of problems. Yet recent work has shown an increasing variety of adversarial samples that can fool them. Most existing detection mechanisms impose significant costs, either by using additional classifiers to spot adversarial samples, or by requiring the DNN to be restructured. In this paper, we introduce a novel defence. We train our DNN so that, as long as it is working as intended on the kind of inputs we expect, its behavior is constrained, in that a set of behaviors are taboo. If it is exposed to adversarial samples, they will often cause a taboo behavior, which we can detect. As an analogy, we can imagine that we are teaching our robot good manners; if it&#39;s ever rude, we know it&#39;s come under some bad influence. This defence mechanism is very simple and, although it involves a modest increase in training, has almost zero computation overhead at runtime -- making it particularly suitable for use in embedded systems. Taboos can be both subtle and diverse. Just as humans&#39; choice of language can convey a lot of information about location, affiliation, class and much else that can be opaque to outsiders but that enables members of the same group to recognise each other, so also taboo choice can encode and hide information. We can use this to make adversarial attacks much harder. It is a well-established design principle that the security of a system should not depend on the obscurity of its design, but of some variable (the key) which can differ between implementations and be changed as necessary. We explain how taboos can be used to equip a classifier with just such a key, and to tune the keying mechanism to adversaries of various capabilities. We evaluate the performance of a prototype against a wide range of attacks and show how our simple defense can work well in practice.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.07375v1-abstract-full').style.display = 'none'; document.getElementById('1811.07375v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 November, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.00208">arXiv:1810.00208</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.00208">pdf</a>, <a href="https://arxiv.org/format/1810.00208">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        To compress or not to compress: Understanding the Interactions between Adversarial Attacks and Neural Network Compression
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yiren Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shumailov%2C+I">Ilia Shumailov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mullins%2C+R">Robert Mullins</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+R">Ross Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.00208v1-abstract-short" style="display: inline;">
        As deep neural networks (DNNs) become widely used, pruned and quantised models are becoming ubiquitous on edge devices; such compressed DNNs are popular for lowering computational requirements. Meanwhile, recent studies show that adversarial samples can be effective at making DNNs misclassify. We, therefore, investigate the extent to which adversarial samples are transferable between uncompressed&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.00208v1-abstract-full').style.display = 'inline'; document.getElementById('1810.00208v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.00208v1-abstract-full" style="display: none;">
        As deep neural networks (DNNs) become widely used, pruned and quantised models are becoming ubiquitous on edge devices; such compressed DNNs are popular for lowering computational requirements. Meanwhile, recent studies show that adversarial samples can be effective at making DNNs misclassify. We, therefore, investigate the extent to which adversarial samples are transferable between uncompressed and compressed DNNs. We find that adversarial samples remain transferable for both pruned and quantised models. For pruning, the adversarial samples generated from heavily pruned models remain effective on uncompressed models. For quantisation, we find the transferability of adversarial samples is highly sensitive to integer precision.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.00208v1-abstract-full').style.display = 'none'; document.getElementById('1810.00208v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 September, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to SysML 2019</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1809.04064">arXiv:1809.04064</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1809.04064">pdf</a>, <a href="https://arxiv.org/format/1809.04064">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Numerical Analysis">cs.NA</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SNS: A Solution-based Nonlinear Subspace method for time-dependent model order reduction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Choi%2C+Y">Youngsoo Choi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Coombs%2C+D">Deshawn Coombs</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+R">Robert Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1809.04064v3-abstract-short" style="display: inline;">
        Several reduced order models have been successfully developed for nonlinear dynamical systems. To achieve a considerable speed-up, a hyper-reduction step is needed to reduce the computational complexity due to nonlinear terms. Many hyper-reduction techniques require the construction of nonlinear term basis, which introduces a computationally expensive offline phase. A novel way of constructing non&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.04064v3-abstract-full').style.display = 'inline'; document.getElementById('1809.04064v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1809.04064v3-abstract-full" style="display: none;">
        Several reduced order models have been successfully developed for nonlinear dynamical systems. To achieve a considerable speed-up, a hyper-reduction step is needed to reduce the computational complexity due to nonlinear terms. Many hyper-reduction techniques require the construction of nonlinear term basis, which introduces a computationally expensive offline phase. A novel way of constructing nonlinear term basis within the hyper-reduction process is introduced. In contrast to the traditional hyper-reduction techniques where the collection of nonlinear term snapshots is required, the SNS method avoids collecting the nonlinear term snapshots. Instead, it uses the solution snapshots that are used for building a solution basis, which enables avoiding an extra data compression of nonlinear term snapshots. As a result, the SNS method provides a more efficient offline strategy than the traditional model order reduction techniques, such as the DEIM, GNAT, and ST-GNAT methods. The SNS method is theoretically justified by the conforming subspace condition and the subspace inclusion relation. It is especially useful for ST-GNAT that has shown promising results, such as a good accuracy with a considerable online speed-up for hyperbolic problems in a recent paper, because ST-GNAT involves an expensive offline cost related to collecting nonlinear term snapshots. Error analysis shows that the oblique projection error bound of the SNS method depends on the condition number of the volume matrix generated from a discretization of a specific numerical scheme. Numerical results support that the accuracy of the solution from the SNS method is comparable to the traditional methods and a considerable speed-up (i.e., a factor of two to a hundred) is achieved in the offline phase.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.04064v3-abstract-full').style.display = 'none'; document.getElementById('1809.04064v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 February, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 September, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">26 pages, 16 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          15A23; 35K05; 35N20; 35L65; 65D25; 65D30; 65F15; 65L05; 65L06; 65L60; 65M22
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1806.08641">arXiv:1806.08641</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1806.08641">pdf</a>, <a href="https://arxiv.org/format/1806.08641">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Compact Deep Neural Networks for Computationally Efficient Gesture Classification From Electromyography Signals
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hartwell%2C+A">Adam Hartwell</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kadirkamanathan%2C+V">Visakan Kadirkamanathan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+S+R">Sean R Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1806.08641v2-abstract-short" style="display: inline;">
        Machine learning classifiers using surface electromyography are important for human-machine interfacing and device control. Conventional classifiers such as support vector machines (SVMs) use manually extracted features based on e.g. wavelets. These features tend to be fixed and non-person specific, which is a key limitation due to high person-to-person variability of myography signals. Deep neura&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.08641v2-abstract-full').style.display = 'inline'; document.getElementById('1806.08641v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1806.08641v2-abstract-full" style="display: none;">
        Machine learning classifiers using surface electromyography are important for human-machine interfacing and device control. Conventional classifiers such as support vector machines (SVMs) use manually extracted features based on e.g. wavelets. These features tend to be fixed and non-person specific, which is a key limitation due to high person-to-person variability of myography signals. Deep neural networks, by contrast, can automatically extract person specific features - an important advantage. However, deep neural networks typically have the drawback of large numbers of parameters, requiring large training data sets and powerful hardware not suited to embedded systems. This paper solves these problems by introducing a compact deep neural network architecture that is much smaller than existing counterparts. The performance of the compact deep net is benchmarked against an SVM and compared to other contemporary architectures across 10 human subjects, comparing Myo and Delsys Trigno electrode sets. The accuracy of the compact deep net was found to be 84.2 +/- 0.06% versus 70.5 +/- 0.07% for the SVM on the Myo, and 80.3+/- 0.07% versus 67.8 +/- 0.09% for the Delsys system, demonstrating the superior effectiveness of the proposed compact network, which had just 5,889 parameters - orders of magnitude less than some contemporary alternatives in this domain while maintaining better performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.08641v2-abstract-full').style.display = 'none'; document.getElementById('1806.08641v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 July, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 June, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">IEEE BioRob 2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1806.04226">arXiv:1806.04226</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1806.04226">pdf</a>, <a href="https://arxiv.org/format/1806.04226">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Physical Representation-based Predicate Optimization for a Visual Analytics Database
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+M+R">Michael R. Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cafarella%2C+M">Michael Cafarella</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ros%2C+G">German Ros</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wenisch%2C+T+F">Thomas F. Wenisch</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1806.04226v3-abstract-short" style="display: inline;">
        Querying the content of images, video, and other non-textual data sources requires expensive content extraction methods. Modern extraction techniques are based on deep convolutional neural networks (CNNs) and can classify objects within images with astounding accuracy. Unfortunately, these methods are slow: processing a single image can take about 10 milliseconds on modern GPU-based hardware. As m&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.04226v3-abstract-full').style.display = 'inline'; document.getElementById('1806.04226v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1806.04226v3-abstract-full" style="display: none;">
        Querying the content of images, video, and other non-textual data sources requires expensive content extraction methods. Modern extraction techniques are based on deep convolutional neural networks (CNNs) and can classify objects within images with astounding accuracy. Unfortunately, these methods are slow: processing a single image can take about 10 milliseconds on modern GPU-based hardware. As massive video libraries become ubiquitous, running a content-based query over millions of video frames is prohibitive.
  One promising approach to reduce the runtime cost of queries of visual content is to use a hierarchical model, such as a cascade, where simple cases are handled by an inexpensive classifier. Prior work has sought to design cascades that optimize the computational cost of inference by, for example, using smaller CNNs. However, we observe that there are critical factors besides the inference time that dramatically impact the overall query time. Notably, by treating the physical representation of the input image as part of our query optimization---that is, by including image transforms, such as resolution scaling or color-depth reduction, within the cascade---we can optimize data handling costs and enable drastically more efficient classifier cascades.
  In this paper, we propose Tahoma, which generates and evaluates many potential classifier cascades that jointly optimize the CNN architecture and input data representation. Our experiments on a subset of ImageNet show that Tahoma&#39;s input transformations speed up cascades by up to 35 times. We also find up to a 98x speedup over the ResNet50 classifier with no loss in accuracy, and a 280x speedup if some accuracy is sacrificed.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.04226v3-abstract-full').style.display = 'none'; document.getElementById('1806.04226v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 February, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 June, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Camera-ready version of the paper submitted to ICDE 2019, In Proceedings of the 35th IEEE International Conference on Data Engineering (ICDE 2019)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1709.08119">arXiv:1709.08119</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1709.08119">pdf</a>, <a href="https://arxiv.org/format/1709.08119">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Combinatorics">math.CO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Discrete Mathematics">cs.DM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Analogies between the crossing number and the tangle crossing number
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+R">Robin Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bai%2C+S">Shuliang Bai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Barrera-Cruz%2C+F">Fidel Barrera-Cruz</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Czabarka%2C+%C3%89">va Czabarka</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Da+Lozzo%2C+G">Giordano Da Lozzo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hobson%2C+N+L+F">Natalie L. F. Hobson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+J+C+-">Jephian C. -H. Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mohr%2C+A">Austin Mohr</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Smith%2C+H+C">Heather C. Smith</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sz%C3%A9kely%2C+L+A">Lszl A. Szkely</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Whitlatch%2C+H">Hays Whitlatch</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1709.08119v1-abstract-short" style="display: inline;">
        Tanglegrams are special graphs that consist of a pair of rooted binary trees with the same number of leaves, and a perfect matching between the two leaf-sets. These objects are of use in phylogenetics and are represented with straightline drawings where the leaves of the two plane binary trees are on two parallel lines and only the matching edges can cross. The tangle crossing number of a tanglegr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1709.08119v1-abstract-full').style.display = 'inline'; document.getElementById('1709.08119v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1709.08119v1-abstract-full" style="display: none;">
        Tanglegrams are special graphs that consist of a pair of rooted binary trees with the same number of leaves, and a perfect matching between the two leaf-sets. These objects are of use in phylogenetics and are represented with straightline drawings where the leaves of the two plane binary trees are on two parallel lines and only the matching edges can cross. The tangle crossing number of a tanglegram is the minimum crossing number over all such drawings and is related to biologically relevant quantities, such as the number of times a parasite switched hosts.
  Our main results for tanglegrams which parallel known theorems for crossing numbers are as follows. The removal of a single matching edge in a tanglegram with $n$ leaves decreases the tangle crossing number by at most $n-3$, and this is sharp. Additionally, if $(n)$ is the maximum tangle crossing number of a tanglegram with $n$ leaves, we prove $\frac{1}{2}\binom{n}{2}(1-o(1))\le(n)&lt;\frac{1}{2}\binom{n}{2}$. Further, we provide an algorithm for computing non-trivial lower bounds on the tangle crossing number in $O(n^4)$ time. This lower bound may be tight, even for tanglegrams with tangle crossing number $(n^2)$.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1709.08119v1-abstract-full').style.display = 'none'; document.getElementById('1709.08119v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 September, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 6 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          Primary: 05C10; Secondary: 05C62; 05C05; 92B10
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1701.03415">arXiv:1701.03415</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1701.03415">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/TWC.2004.826328">10.1109/TWC.2004.826328 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        In-Building Wideband Partition Loss Measurements at 2.5 GHz and 60 GHz
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+C+R">Christopher R. Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rappaort%2C+T+S">Theodore S. Rappaort</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1701.03415v1-abstract-short" style="display: inline;">
        This paper contains measured data and empirical models for 2.5 & 60 GHz in-building propagation path loss and multipath delay spread. Path loss measurements were recorded using a broadband sliding correlator channel sounder which recorded over 39,000 Power Delay Profiles (PDPs) in 22 separate locations in a modern office building. Transmitters and receivers were separated by distances ranging from&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1701.03415v1-abstract-full').style.display = 'inline'; document.getElementById('1701.03415v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1701.03415v1-abstract-full" style="display: none;">
        This paper contains measured data and empirical models for 2.5 &amp; 60 GHz in-building propagation path loss and multipath delay spread. Path loss measurements were recorded using a broadband sliding correlator channel sounder which recorded over 39,000 Power Delay Profiles (PDPs) in 22 separate locations in a modern office building. Transmitters and receivers were separated by distances ranging from 3.5 to 27.4 meters, and were separated by a variety of obstructions, in order to create realistic environments for future single-cell-per-room wireless networks. Path loss data is coupled with site-specific information to provide insight into channel characteristics. These measurements and models may aid in the development of future in-building wireless networks in the unlicensed 2.4 GHz and 60 GHz bands.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1701.03415v1-abstract-full').style.display = 'none'; document.getElementById('1701.03415v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 December, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">http://ieeexplore.ieee.org/document/1296643/</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IEEE Transactions on Wireless Communications, vol. 3, no. 3, pp. 922-928, May 2004
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1701.00691">arXiv:1701.00691</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1701.00691">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/JSTSP.2013.2286774">10.1109/JSTSP.2013.2286774 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Radio Tomography for Roadside Surveillance
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+C+R">Christopher R. Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Martin%2C+R+K">Richard K. Martin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Walker%2C+T+O">T. Owens Walker</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Thomas%2C+R+W">Ryan W. Thomas</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1701.00691v1-abstract-short" style="display: inline;">
        Radio tomographic imaging (RTI) has recently been proposed for tracking object location via radio waves without requiring the objects to transmit or receive radio signals. The position is extracted by inferring which voxels are obstructing a subset of radio links in a dense wireless sensor network. This paper proposes a variety of modeling and algorithmic improvements to RTI for the scenario of ro&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1701.00691v1-abstract-full').style.display = 'inline'; document.getElementById('1701.00691v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1701.00691v1-abstract-full" style="display: none;">
        Radio tomographic imaging (RTI) has recently been proposed for tracking object location via radio waves without requiring the objects to transmit or receive radio signals. The position is extracted by inferring which voxels are obstructing a subset of radio links in a dense wireless sensor network. This paper proposes a variety of modeling and algorithmic improvements to RTI for the scenario of roadside surveillance. These include the use of a more physically motivated weight matrix, a method for mitigating negative (aphysical) data due to noisy observations, and a method for combining frames of a moving vehicle into a single image. The proposed approaches are used to show improvement in both imaging (useful for human-in-the-loop target recognition) and automatic target recognition in a measured data set.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1701.00691v1-abstract-full').style.display = 'none'; document.getElementById('1701.00691v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 December, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">http://ieeexplore.ieee.org/document/6644288/</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        C. R. Anderson, R. K. Martin, T. O. Walker and R. W. Thomas, &#34;Radio Tomography for Roadside Surveillance,&#34; in IEEE Journal of Selected Topics in Signal Processing, vol. 8, no. 1, pp. 66-79, Feb. 2014
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1612.03971">arXiv:1612.03971</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1612.03971">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Systems and Control">cs.SY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Instrumentation and Methods for Astrophysics">astro-ph.IM</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/TIM.2016.2599458">10.1109/TIM.2016.2599458 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An In Situ Measurement System for Characterizing Orbital Debris
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tsao%2C+M+A">Michael A. Tsao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ngo%2C+H+T">Hau T. Ngo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Corsaro%2C+R+D">Robert D. Corsaro</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+C+R">Christopher R. Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1612.03971v1-abstract-short" style="display: inline;">
        This paper presents the development of an in situ measurement system known as the Debris Resistive Acoustic Grid Orbital Navy/NASA Sensor (DRAGONS). The DRAGONS system is designed to detect impacts caused by particles ranging from 50 micrometers to 1 mm at both low-earth and geostationary orbits. DRAGONS utilizes a combination of low-cost sensor technologies to facilitate accurate measurements and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1612.03971v1-abstract-full').style.display = 'inline'; document.getElementById('1612.03971v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1612.03971v1-abstract-full" style="display: none;">
        This paper presents the development of an in situ measurement system known as the Debris Resistive Acoustic Grid Orbital Navy/NASA Sensor (DRAGONS). The DRAGONS system is designed to detect impacts caused by particles ranging from 50 micrometers to 1 mm at both low-earth and geostationary orbits. DRAGONS utilizes a combination of low-cost sensor technologies to facilitate accurate measurements and approximations of the size, velocity, and angle of impacting micrometeoroids and orbital debris (MMOD). Two thin layers of kapton sheets with resistive traces are used to detect the changes in resistance that are directly proportional to the impacting force caused by the fast traveling particles. Four polyvinylidene fluoride-based sensors are positioned in the back of each kapton sheet to measure acoustic strain caused by an impact. The electronic hardware module that controls all operations employs a low-power, modular, and compact design that enables it to be installed as a low-resource load on a host satellite. Laboratory results demonstrate that in addition to having the ability to detect an impact event, the DRAGONS system can determine impact location, speed, and angle of impact with a mean error of 1.4 cm, 0.2 km/s, and 5. The DRAGONS system could be deployed as an add-on subsystem of a payload to enable a real-time, in-depth study of the properties of MMOD.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1612.03971v1-abstract-full').style.display = 'none'; document.getElementById('1612.03971v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 December, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IEEE Transactions on Instrumentation and Measurement, Vol. 65, No. 12, pp. 2758-2772, December 2016
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1605.01081">arXiv:1605.01081</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1605.01081">pdf</a>, <a href="https://arxiv.org/format/1605.01081">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/LWC.2016.2560179">10.1109/LWC.2016.2560179 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Co-existence of TD-LTE and Radar over 3.5 GHz Band: An Experimental Study
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Reed%2C+J+H">Jeffrey H. Reed</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Clegg%2C+A+W">Andrew W. Clegg</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Padaki%2C+A+V">Aditya V. Padaki</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+T">Taeyoung Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nealy%2C+R">Randall Nealy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dietrich%2C+C">Carl Dietrich</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+C+R">Christopher R. Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mearns%2C+D+M">D. Michael Mearns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1605.01081v1-abstract-short" style="display: inline;">
        This paper presents a pioneering study based on a series of experiments on the operation of commercial Time-Division Long-Term Evolution (TD-LTE) systems in the presence of pulsed interfering signals in the 3550-3650 MHz band. TD-LTE operations were carried out in channels overlapping and adjacent to the high power SPN-43 radar with various frequency offsets between the two systems to evaluate the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.01081v1-abstract-full').style.display = 'inline'; document.getElementById('1605.01081v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1605.01081v1-abstract-full" style="display: none;">
        This paper presents a pioneering study based on a series of experiments on the operation of commercial Time-Division Long-Term Evolution (TD-LTE) systems in the presence of pulsed interfering signals in the 3550-3650 MHz band. TD-LTE operations were carried out in channels overlapping and adjacent to the high power SPN-43 radar with various frequency offsets between the two systems to evaluate the susceptibility of LTE to a high power interfering signal. Our results demonstrate that LTE communication using low antenna heights was not adversely affected by the pulsed interfering signal operating on adjacent frequencies irrespective of the distance of interfering transmitter. Performance was degraded only for very close distances (1-2 km) of overlapping frequencies of interfering transmitter.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.01081v1-abstract-full').style.display = 'none'; document.getElementById('1605.01081v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 May, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for publication in IEEE Wireless Communications Letters</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1304.6613">arXiv:1304.6613</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1304.6613">pdf</a>, <a href="https://arxiv.org/format/1304.6613">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Tissues and Organs">q-bio.TO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1371/journal.pone.0071465">10.1371/journal.pone.0071465 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Ovarian volume throughout life: a validated normative model
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kelsey%2C+T+W">Thomas W. Kelsey</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dodwell%2C+S+K">Sarah K. Dodwell</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wilkinson%2C+A+G">A. Graham Wilkinson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Greve%2C+T">Tine Greve</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Andersen%2C+C+Y">Claus Y. Andersen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+R+A">Richard A. Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wallace%2C+W+H+B">W. Hamish B. Wallace</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1304.6613v1-abstract-short" style="display: inline;">
        The measurement of ovarian volume has been shown to be a useful indirect indicator of the ovarian reserve in women of reproductive age, in the diagnosis and management of a number of disorders of puberty and adult reproductive function, and is under investigation as a screening tool for ovarian cancer. To date there is no normative model of ovarian volume throughout life. By searching the publishe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1304.6613v1-abstract-full').style.display = 'inline'; document.getElementById('1304.6613v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1304.6613v1-abstract-full" style="display: none;">
        The measurement of ovarian volume has been shown to be a useful indirect indicator of the ovarian reserve in women of reproductive age, in the diagnosis and management of a number of disorders of puberty and adult reproductive function, and is under investigation as a screening tool for ovarian cancer. To date there is no normative model of ovarian volume throughout life. By searching the published literature for ovarian volume in healthy females, and using our own data from multiple sources (combined n = 59,994) we have generated and robustly validated the first model of ovarian volume from conception to 82 years of age. This model shows that 69% of the variation in ovarian volume is due to age alone. We have shown that in the average case ovarian volume rises from 0.7 mL (95% CI 0.4 -- 1.1 mL) at 2 years of age to a peak of 7.7 mL (95% CI 6.5 -- 9.2 mL) at 20 years of age with a subsequent decline to about 2.8mL (95% CI 2.7 -- 2.9 mL) at the menopause and smaller volumes thereafter. Our model allows us to generate normal values and ranges for ovarian volume throughout life. This is the first validated normative model of ovarian volume from conception to old age; it will be of use in the diagnosis and management of a number of diverse gynaecological and reproductive conditions in females from birth to menopause and beyond.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1304.6613v1-abstract-full').style.display = 'none'; document.getElementById('1304.6613v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 April, 2013; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2013.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, 7 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          92B05
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          G.1.2
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1301.7351">arXiv:1301.7351</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1301.7351">pdf</a>, <a href="https://arxiv.org/format/1301.7351">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Quantum Physics">quant-ph</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Mathematical Physics">math-ph</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Why quantum computing is hard - and quantum cryptography is not provably secure
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+R">Ross Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Brady%2C+R">Robert Brady</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1301.7351v1-abstract-short" style="display: inline;">
        Despite high hopes for quantum computation in the 1990s, progress in the past decade has been slow; we still cannot perform computation with more than about three qubits and are no closer to solving problems of real interest than a decade ago. Separately, recent experiments in fluid mechanics have demonstrated the emergence of a full range of quantum phenomena from completely classical motion. We&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1301.7351v1-abstract-full').style.display = 'inline'; document.getElementById('1301.7351v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1301.7351v1-abstract-full" style="display: none;">
        Despite high hopes for quantum computation in the 1990s, progress in the past decade has been slow; we still cannot perform computation with more than about three qubits and are no closer to solving problems of real interest than a decade ago. Separately, recent experiments in fluid mechanics have demonstrated the emergence of a full range of quantum phenomena from completely classical motion. We present two specific hypotheses. First, Kuramoto theory may give a basis for geometrical thinking about entanglement. Second, we consider a recent soliton model of the electron, in which the quantum-mechanical wave function is a phase modulation of a carrier wave. Both models are consistent with one another and with observation. Both models suggest how entanglement and decoherence may be related to device geometry. Both models predict that it will be difficult to maintain phase coherence of more than three qubits in the plane, or four qubits in a three-dimensional structure. The soliton model also shows that the experimental work which appeared to demonstrate a violation of Bell&#39;s inequalities might not actually do so; regardless of whether it is a correct description of the world, it exposes a flaw in the logic of the Bell tests. Thus the case for the security of EPR-based quantum cryptography has just not been made. We propose experiments in quantum computation to test this. Finally, we examine two possible interpretations of such soliton models: one is consistent with the transactional interpretation of quantum mechanics, while the other is an entirely classical model in which we do not have to abandon the idea of a single world where action is local and causal.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1301.7351v1-abstract-full').style.display = 'none'; document.getElementById('1301.7351v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 January, 2013; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2013.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1301.3934">arXiv:1301.3934</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1301.3934">pdf</a>, <a href="https://arxiv.org/format/1301.3934">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Tissues and Organs">q-bio.TO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Intrinsic cell factors that influence tumourigenicity in cancer stem cells - towards hallmarks of cancer stem cells
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Scott%2C+J+G">Jacob G. Scott</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chinnaiyan%2C+P">Prakash Chinnaiyan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A+R+A">Alexander R. A. Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hjelmeland%2C+A">Anita Hjelmeland</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Basanta%2C+D">David Basanta</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1301.3934v3-abstract-short" style="display: inline;">
        Since the discovery of a cancer initiating side population in solid tumours, studies focussing on the role of so-called cancer stem cells in cancer initiation and progression have abounded. The biological interrogation of these cells has yielded volumes of information about their behaviour, but there has, as of yet, not been many actionable generalised theoretical conclusions. To address this poin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1301.3934v3-abstract-full').style.display = 'inline'; document.getElementById('1301.3934v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1301.3934v3-abstract-full" style="display: none;">
        Since the discovery of a cancer initiating side population in solid tumours, studies focussing on the role of so-called cancer stem cells in cancer initiation and progression have abounded. The biological interrogation of these cells has yielded volumes of information about their behaviour, but there has, as of yet, not been many actionable generalised theoretical conclusions. To address this point, we have created a hybrid, discrete/continuous computational cellular automaton model of a generalised stem-cell driven tissue and explored the phenotypic traits inherent in the inciting cell and the resultant tissue growth. We identify the regions in phenotype parameter space where these initiating cells are able to cause a disruption in homeostasis, leading to tissue overgrowth and tumour formation. As our parameters and model are non-specific, they could apply to any tissue cancer stem-cell and do not assume specific genetic mutations. In this way, our model suggests that targeting these phenotypic traits could represent generalizable strategies across cancer types and represents a first attempt to identify the hallmarks of cancer stem cells.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1301.3934v3-abstract-full').style.display = 'none'; document.getElementById('1301.3934v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 August, 2013; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 January, 2013;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2013.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 4 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1209.2531">arXiv:1209.2531</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1209.2531">pdf</a>, <a href="https://arxiv.org/format/1209.2531">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Chip and Skim: cloning EMV cards with the pre-play attack
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bond%2C+M">Mike Bond</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choudary%2C+O">Omar Choudary</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Murdoch%2C+S+J">Steven J. Murdoch</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Skorobogatov%2C+S">Sergei Skorobogatov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+R">Ross Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1209.2531v1-abstract-short" style="display: inline;">
        EMV, also known as "Chip and PIN", is the leading system for card payments worldwide. It is used throughout Europe and much of Asia, and is starting to be introduced in North America too. Payment cards contain a chip so they can execute an authentication protocol. This protocol requires point-of-sale (POS) terminals or ATMs to generate a nonce, called the unpredictable number, for each transaction&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1209.2531v1-abstract-full').style.display = 'inline'; document.getElementById('1209.2531v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1209.2531v1-abstract-full" style="display: none;">
        EMV, also known as &#34;Chip and PIN&#34;, is the leading system for card payments worldwide. It is used throughout Europe and much of Asia, and is starting to be introduced in North America too. Payment cards contain a chip so they can execute an authentication protocol. This protocol requires point-of-sale (POS) terminals or ATMs to generate a nonce, called the unpredictable number, for each transaction to ensure it is fresh. We have discovered that some EMV implementers have merely used counters, timestamps or home-grown algorithms to supply this number. This exposes them to a &#34;pre-play&#34; attack which is indistinguishable from card cloning from the standpoint of the logs available to the card-issuing bank, and can be carried out even if it is impossible to clone a card physically (in the sense of extracting the key material and loading it into another card). Card cloning is the very type of fraud that EMV was supposed to prevent. We describe how we detected the vulnerability, a survey methodology we developed to chart the scope of the weakness, evidence from ATM and terminal experiments in the field, and our implementation of proof-of-concept attacks. We found flaws in widely-used ATMs from the largest manufacturers. We can now explain at least some of the increasing number of frauds in which victims are refused refunds by banks which claim that EMV cards cannot be cloned and that a customer involved in a dispute must therefore be mistaken or complicit. Pre-play attacks may also be carried out by malware in an ATM or POS terminal, or by a man-in-the-middle between the terminal and the acquirer. We explore the design and implementation mistakes that enabled the flaw to evade detection until now: shortcomings of the EMV specification, of the EMV kernel certification process, of implementation testing, formal analysis, or monitoring customer complaints. Finally we discuss countermeasures.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1209.2531v1-abstract-full').style.display = 'none'; document.getElementById('1209.2531v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 September, 2012; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2012.
      
    </p>
    

    

    
  </li>

</ol>


  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://confluence.cornell.edu/x/giazFQ">Search v0.5 released 2018-12-20</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>

  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About arXiv</a></li>
          <li><a href="https://arxiv.org/about/people/leadership_team">Leadership Team</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li><span class="icon"><i class="fa fa-envelope"></i></span><a href="https://arxiv.org/help/contact"> Contact</a></li>
          <li><span class="icon"><i class="fa fa-twitter"></i></span><a href="https://twitter.com/arxiv"> Follow us on Twitter</a></li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help">Help</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://blogs.cornell.edu/arxiv">Blog</a></li>
          <li><a href="https://arxiv.org/help/subscribe"> Subscribe</a></li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>

<div class="columns" style="border-top: 1px solid #979797">
  <div class="column">
    <p class="help">arXiv&#174; is a registered trademark of Cornell University.</p>
  </div>
  <div class="column">
    <p class="help">If you have a disability and are having trouble accessing information
      on this website or need materials in an alternate format, contact
      <a href="mailto:web-accessibility@cornell.edu">web-accessibility@cornell.edu</a> for
       assistance.</p>
  </div>
</div>
    
  </footer>
  </body>
</html>


#####EOF#####


<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.15.2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.15.2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.15.2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.15.2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Leadership Team | arXiv e-print repository</title>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.15.2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>

<script src="https://static.arxiv.org/static/base/0.15.2/js/notification.js"></script>
<!-- Piwik -->
<script type="text/javascript">
var _paq = _paq || [];
_paq.push(["setDomains", ["*.arxiv.org"]]);
_paq.push(['trackPageView']);
_paq.push(['enableLinkTracking']);
(function()
{ var u="//webanalytics.library.cornell.edu/"; _paq.push(['setTrackerUrl', u+'piwik.php']); _paq.push(['setSiteId', 538]); var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0]; g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s); }
)();
</script>
<!-- End Piwik Code -->
    <!-- Custom style sheets or other head includes -->
    
  <link rel="stylesheet" href="https://static.arxiv.org/_marxdown/static/arxiv.marxdown/0.1/docs/css/docs.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/_marxdown/static/arxiv.marxdown/0.1/docs/css/codehilite.css" />
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-2rfjj7/b/29/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&amp;collectorId=9f4c36dd"></script>

  <script type="text/javascript">
  window.ATL_JQ_PAGE_PROPS =  {
    "triggerFunction": function(showCollectorDialog) {
      //Requires that jQuery is available!
      $("#feedback-button").click(function(e) {
        e.preventDefault();
        showCollectorDialog();
      });
    },
    fieldValues: {
      "components": ["12154"],
      "versions": ["14219"],
      "customfield_11401": window.location.href
    }
  };
  </script>


  </head>
  <body>
  
  <noscript><img src="//webanalytics.library.cornell.edu/piwik.php?idsite=538&rec=1" style="border:0;" alt="" /></noscript>
  <header>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.15.2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo"><img src="https://static.arxiv.org/static/base/0.15.2/images/arxiv-logo-web.svg" alt="arXiv" aria-label="logo" width="85" /></a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>

  <main role="main" class="container">
    

    


  


<div >
  

<nav class="breadcrumb" aria-label="breadcrumbs">
  <ul>
    <li>
      <a href="https://arxiv.org/">
        arXiv
      </a>
    </li>
    <li>
      <a href="/about/">
        About
      </a>
    </li>
    
      
      <li>
        <a href="/about/people">
          Who We Are
        </a>
      </li>
      
    
    <li class="is-active">
      <a href="/about/people/leadership_team" aria-current="page">
        Leadership Team
      </a>
    </li>
  </ul>
</nav>

</div>
<div >
  
    
<h1 id="arxiv-leadership-team">arXiv Leadership Team</h1>
<p><div class="columns">

    <article class="column is-half">
      <div class="media">
        <div class="media-left is-hidden-mobile">
          <p class="image is-96x96"><img style="border-radius: 50%" src="https://static.arxiv.org/static/arxiv.marxdown/0.1/about/images/oya.jpg" alt="" /></p>
        </div>
        <div class="media-content">
          <p class="image is-48x48 is-hidden-tablet is-pulled-left" style=" margin-right: .5em; margin-top: 1.5em;"><img style="border-radius: 50%;" src="https://static.arxiv.org/static/arxiv.marxdown/0.1/about/images/oya.jpg" alt="" /></p>
          <h2 class="title">Oya Y. Rieger</h2>
          <p class="subtitle">Program Director</p>
          <p>Oya has been leading the arXiv operation and spearheaded the development of the governance and sustainability model since 2010. She has provided leadership in several national and international scholarly communication and digital preservation initiatives and holds a Ph.D. in Human-Computer Interaction (Cornell University).
</p>
          <ul class="is-marginless">
          
            <li class="orcid" style="list-style: none"><span class="icon is-small" style="vertical-align: middle"><img src="https://static.arxiv.org/static/arxiv.marxdown/0.1/about/images/orcid_32x32.png" alt="" /></span> <a href="https://orcid.org/0000-0001-6175-5157">https://orcid.org/0000-0001-6175-5157</a></li>
          
          
            <li class="twit" style="list-style: none"><span class="icon is-small"><i class="fa fa-twitter has-text-link" role="presentation"></i></span><a href="https://twitter.com/OyaRieger">@OyaRieger</a></li>
          
          </ul>
        </div>
      </div>

    </article>


    <article class="column is-half">
      <div class="media">
        <div class="media-left is-hidden-mobile">
          <p class="image is-96x96"><img style="border-radius: 50%" src="https://static.arxiv.org/static/arxiv.marxdown/0.1/about/images/steinn.jpg" alt="" /></p>
        </div>
        <div class="media-content">
          <p class="image is-48x48 is-hidden-tablet is-pulled-left" style=" margin-right: .5em; margin-top: 1.5em;"><img style="border-radius: 50%;" src="https://static.arxiv.org/static/arxiv.marxdown/0.1/about/images/steinn.jpg" alt="" /></p>
          <h2 class="title">Steinn Sigurdsson</h2>
          <p class="subtitle">Scientific Director</p>
          <p>Steinn is a Professor of Astrophysics at Penn State University.  He holds a doctorate in theoretical physics from the California Institute of Technology. His research interests include astrophysics and related areas, ranging from cosmology, large scale dynamics and black holes, to formation and evolution of planets, and the prospects for discovering non-terrestrial life.
</p>
          <ul class="is-marginless">
          
            <li class="orcid" style="list-style: none"><span class="icon is-small" style="vertical-align: middle"><img src="https://static.arxiv.org/static/arxiv.marxdown/0.1/about/images/orcid_32x32.png" alt="" /></span> <a href="https://orcid.org/0000-0002-8187-1144">https://orcid.org/0000-0002-8187-1144</a></li>
          
          
            <li class="twit" style="list-style: none"><span class="icon is-small"><i class="fa fa-twitter has-text-link" role="presentation"></i></span><a href="https://twitter.com/steinly0">@steinly0</a></li>
          
          </ul>
        </div>
      </div>

    </article>

</div>
<div class="columns">

    <article class="column is-half">
      <div class="media">
        <div class="media-left is-hidden-mobile">
          <p class="image is-96x96"><img style="border-radius: 50%" src="https://static.arxiv.org/static/arxiv.marxdown/0.1/about/images/martin.jpg" alt="" /></p>
        </div>
        <div class="media-content">
          <p class="image is-48x48 is-hidden-tablet is-pulled-left" style=" margin-right: .5em; margin-top: 1.5em;"><img style="border-radius: 50%;" src="https://static.arxiv.org/static/arxiv.marxdown/0.1/about/images/martin.jpg" alt="" /></p>
          <h2 class="title">Martin Lessmeister</h2>
          <p class="subtitle">IT Lead</p>
          <p>Martin is responsible for overseeing the day-to-day technical operations of the arXiv services and is supervisor to the members of the development team. He works closely with our Lead Software Architect in planning and executing the migration of arXivs legacy software system to the next generation architecture. His background is in web development with a focus on distributed systems, with an M.Eng. in Computer Science (Cornell University).
</p>
          <ul class="is-marginless">
          
          
          </ul>
        </div>
      </div>

    </article>


    <article class="column is-half">
      <div class="media">
        <div class="media-left is-hidden-mobile">
          <p class="image is-96x96"><img style="border-radius: 50%" src="https://static.arxiv.org/static/arxiv.marxdown/0.1/about/images/erick.jpg" alt="" /></p>
        </div>
        <div class="media-content">
          <p class="image is-48x48 is-hidden-tablet is-pulled-left" style=" margin-right: .5em; margin-top: 1.5em;"><img style="border-radius: 50%;" src="https://static.arxiv.org/static/arxiv.marxdown/0.1/about/images/erick.jpg" alt="" /></p>
          <h2 class="title">Erick Peirson</h2>
          <p class="subtitle">Lead Software Architect</p>
          <p>Erick is responsible for high-level technical decisions, planning, and collaboration related to the arXiv software system. His main focus is the arXiv-NG project, which moves the arXiv.org software system into a modern, cloud-native architectural paradigm. Ericks background is in software development for information systems and computational research, and he also holds a PhD in History &amp; Philosophy of Science.
</p>
          <ul class="is-marginless">
          
            <li class="orcid" style="list-style: none"><span class="icon is-small" style="vertical-align: middle"><img src="https://static.arxiv.org/static/arxiv.marxdown/0.1/about/images/orcid_32x32.png" alt="" /></span> <a href="https://orcid.org/0000-0002-0564-9939">https://orcid.org/0000-0002-0564-9939</a></li>
          
          
            <li class="twit" style="list-style: none"><span class="icon is-small"><i class="fa fa-twitter has-text-link" role="presentation"></i></span><a href="https://twitter.com/undercaffeinatd">@undercaffeinatd</a></li>
          
          </ul>
        </div>
      </div>

    </article>

</div>
<div class="columns">

    <article class="column is-half">
      <div class="media">
        <div class="media-left is-hidden-mobile">
          <p class="image is-96x96"><img style="border-radius: 50%" src="https://static.arxiv.org/static/arxiv.marxdown/0.1/about/images/jim.jpg" alt="" /></p>
        </div>
        <div class="media-content">
          <p class="image is-48x48 is-hidden-tablet is-pulled-left" style=" margin-right: .5em; margin-top: 1.5em;"><img style="border-radius: 50%;" src="https://static.arxiv.org/static/arxiv.marxdown/0.1/about/images/jim.jpg" alt="" /></p>
          <h2 class="title">Jim Entwood</h2>
          <p class="subtitle">Operations Manager</p>
          <p>Jim coordinates the efforts of the volunteer moderators and arXiv administrators on the daily flow of papers and user support, and works with the Scientific Director to develop and improve arXiv&#39;s operations policies. His background is in volunteer management and website development for research groups with a Masters in Leadership Studies.
</p>
          <ul class="is-marginless">
          
          
          </ul>
        </div>
      </div>

    </article>


    <article class="column is-half">
      <div class="media">
        <div class="media-left is-hidden-mobile">
          <p class="image is-96x96"><img style="border-radius: 50%" src="https://static.arxiv.org/static/arxiv.marxdown/0.1/about/images/janelle.jpg" alt="" /></p>
        </div>
        <div class="media-content">
          <p class="image is-48x48 is-hidden-tablet is-pulled-left" style=" margin-right: .5em; margin-top: 1.5em;"><img style="border-radius: 50%;" src="https://static.arxiv.org/static/arxiv.marxdown/0.1/about/images/janelle.jpg" alt="" /></p>
          <h2 class="title">Janelle Morano</h2>
          <p class="subtitle">Community Engagement and Development Coordinator</p>
          <p>Janelle is responsible for the outreach and marketing program and communication strategies. She engages educational institutions in arXivs membership program and implements fundraising strategies, including grant writing and giving campaigns. Her background is in animal communication and ecology, with an M.S. in biology.
</p>
          <ul class="is-marginless">
          
            <li class="orcid" style="list-style: none"><span class="icon is-small" style="vertical-align: middle"><img src="https://static.arxiv.org/static/arxiv.marxdown/0.1/about/images/orcid_32x32.png" alt="" /></span> <a href="https://orcid.org/0000-0001-5950-3313">https://orcid.org/0000-0001-5950-3313</a></li>
          
          
            <li class="twit" style="list-style: none"><span class="icon is-small"><i class="fa fa-twitter has-text-link" role="presentation"></i></span><a href="https://twitter.com/janellelmorano">@janellelmorano</a></li>
          
          </ul>
        </div>
      </div>

    </article>

</div></p>

  
</div>

<aside class="has-text-centered" style="padding-top: 3rem;">
  
  <p class="is-size-7">"Leadership Team" revision <a href="https://github.com/arXiv/arxiv-docs/releases/tag/0.2.8">0.2.8</a>. Last modified <a href="https://github.com/arXiv/arxiv-docs/tree/52e89bb1/about/people/leadership_team.md">2019-02-12</a>.</p> <button class="button is-small" id="feedback-button">Feedback?</button>
  
</aside>


  </main>

  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About arXiv</a></li>
          <li><a href="https://arxiv.org/about/people/leadership_team">Leadership Team</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li><span class="icon"><i class="fa fa-envelope"></i></span><a href="https://arxiv.org/help/contact"> Contact</a></li>
          <li><span class="icon"><i class="fa fa-twitter"></i></span><a href="https://twitter.com/arxiv"> Follow us on Twitter</a></li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help">Help</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://blogs.cornell.edu/arxiv">Blog</a></li>
          <li><a href="https://arxiv.org/help/subscribe"> Subscribe</a></li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>

<div class="columns" style="border-top: 1px solid #979797">
  <div class="column">
    <p class="help">arXiv&#174; is a registered trademark of Cornell University.</p>
  </div>
  <div class="column">
    <p class="help">If you have a disability and are having trouble accessing information
      on this website or need materials in an alternate format, contact
      <a href="mailto:web-accessibility@cornell.edu">web-accessibility@cornell.edu</a> for
       assistance.</p>
  </div>
</div>
    
  </footer>
  </body>
</html>


#####EOF#####


<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">

<head>
  <title>[0803.0476] Fast unfolding of communities in large networks</title>
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/css/arXiv.css?v=20190307" />
  
  <!-- Piwik -->
  <script type="text/javascript">
    var _paq = _paq || [];
    _paq.push(["setDomains", ["*.arxiv.org"]]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
      var u = "//webanalytics.library.cornell.edu/";
      _paq.push(['setTrackerUrl', u + 'piwik.php']);
      _paq.push(['setSiteId', 538]);
      var d = document,
        g = d.createElement('script'),
        s = d.getElementsByTagName('script')[0];
      g.type = 'text/javascript';
      g.async = true;
      g.defer = true;
      g.src = u + 'piwik.js';
      s.parentNode.insertBefore(g, s);
    })();
  </script>
  <!-- End Piwik Code -->
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/zca7yc/b/13/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=7a8da419"></script>
<script type="text/javascript">window.ATL_JQ_PAGE_PROPS =  {
  "triggerFunction": function(showCollectorDialog) {
    //Requires that jQuery is available!
    jQuery("#feedback-button").click(function(e) {
      e.preventDefault();
      showCollectorDialog();
    });
  },
  fieldValues: {
    "components": ["15700"],  // Browse component.
    "versions": ["14132"],  // Release browse-0.1
    "customfield_11401": window.location.href
  }
  };
</script><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" media="screen" type="text/css" href="/bibex/bibex.css?20181010"/>
  <script src="//static.arxiv.org/js/mathjaxToggle.min.js" type="text/javascript"></script>
  <meta name="citation_title" content="Fast unfolding of communities in large networks"/>
  <meta name="citation_author" content="Blondel, Vincent D."/>
  <meta name="citation_author" content="Guillaume, Jean-Loup"/>
  <meta name="citation_author" content="Lambiotte, Renaud"/>
  <meta name="citation_author" content="Lefebvre, Etienne"/>
  <meta name="citation_doi" content="10.1088/1742-5468/2008/10/P10008"/>
  <meta name="citation_date" content="2008/03/04"/>
  <meta name="citation_online_date" content="2008/07/25"/>
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/0803.0476"/>
  <meta name="citation_arxiv_id" content="0803.0476"/><meta name="twitter:site" content="@arxiv"/>
    <meta property="twitter:title" content="Fast unfolding of communities in large networks"/>
    <meta property="twitter:description" content="We propose a simple method to extract the community structure of large
networks. Our method is a heuristic method that is based on modularity
optimization. It is shown to outperform all other..."/>
    <meta property="og:site_name" content="arXiv.org"/>
    <meta property="og:title" content="Fast unfolding of communities in large networks"/>
    <meta property="og:url" content="https://arxiv.org/abs/0803.0476v2"/>
    <meta property="og:description" content="We propose a simple method to extract the community structure of large
networks. Our method is a heuristic method that is based on modularity
optimization. It is shown to outperform all other known community detection
method in terms of computation time. Moreover, the quality of the communities
detected is very good, as measured by the so-called modularity. This is shown
first by identifying language communities in a Belgian mobile phone network of
2.6 million customers and by analyzing a web graph of 118 million nodes and
more than one billion links. The accuracy of our algorithm is also verified on
ad-hoc modular networks. ."/>
</head>

<body  class="with-cu-identity">
  <noscript><img src="//webanalytics.library.cornell.edu/piwik.php?idsite=538&amp;rec=1" style="border:0;" alt="" /></noscript>
  <div id="cu-identity">
    <div id="cu-logo">
      <a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
    </div>
    <div id="support-ack">
      <a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br/>the Simons Foundation and member institutions.</a>
    </div>
  </div>

  <div id="header" >
    
  <h1><a href="/">arXiv.org</a> &gt; <a href="/list/physics/recent">physics</a> &gt; arXiv:0803.0476</h1>
  <div id="search">
    <form id="search-arxiv" method="get" action="https://arxiv.org/search">

      <div class="wrapper-search-arxiv">
        <input class="keyword-field" type="text" name="query" placeholder="Search or Article ID" />

        <div class="filter-field">
          <select name="searchtype">
            <option value="all">All fields</option>
            <option value="title">Title</option>
            <option value="author">Author(s)</option>
            <option value="abstract">Abstract</option>
            <option value="comments">Comments</option>
            <option value="journal_ref">Journal reference</option>
            <option value="acm_class">ACM classification</option>
            <option value="msc_class">MSC classification</option>
            <option value="report_num">Report number</option>
            <option value="paper_id">arXiv identifier</option>
            <option value="doi">DOI</option>
            <option value="orcid">ORCID</option>
            <option value="author_id">arXiv author ID</option>
            <option value="help">Help pages</option>
            <option value="full_text">Full text</option>
          </select>
        </div>
        <input class="btn-search-arxiv" value="" type="submit">
        <div class="links">(<a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced search</a>)</div>
      </div>
    </form>
  </div>

  </div>

  <div id="content">
    <!--
rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/0803.0476"
        dc:identifier="/abs/0803.0476"
        dc:title="Fast unfolding of communities in large networks"
        trackback:ping="/trackback/0803.0476" />
    </rdf:RDF>
-->
<div id="abs">
  <div class="extra-services">
    <div class="full-text">
      <span class="descriptor">Full-text links:</span>
      <h2>Download:</h2>
      <ul>
  <li><a href="/pdf/0803.0476" accesskey="f">PDF</a></li>
  <li><a href="/ps/0803.0476">PostScript</a></li>
  <li><a href="/format/0803.0476">Other formats</a></li></ul>
      <div class="abs-license">(<a href="http://arxiv.org/licenses/nonexclusive-distrib/1.0/" title="Rights to this article">license</a>)</div>
    </div>
    <!--end full-text-->
    <div class="browse">
    <h3>Current browse context:</h3>
  <div class="current">physics.soc-ph</div>

  <div class="prevnext">

  <span class="arrow">
    <a href="/prevnext?site=arxiv.org&amp;id=0803.0476&amp;function=prev&amp;context=physics.soc-ph"
       accesskey="p" title="previous in physics.soc-ph (accesskey p)">&lt;&nbsp;prev</a>
  </span>&nbsp;|&nbsp;

  
  <span class="arrow">
    <a href="/prevnext?site=arxiv.org&amp;id=0803.0476&amp;function=next&amp;context=physics.soc-ph" accesskey="n"
       title="next in physics.soc-ph (accesskey n)">next&nbsp;&gt;</a>
  </span><br/>
  </div><div class="list">
    <a href="/list/physics.soc-ph/new">new</a>&nbsp;|
    <a href="/list/physics.soc-ph/recent">recent</a>&nbsp;|
    <a href="/list/physics.soc-ph/0803">0803</a>
  </div><h3>Change to browse by:</h3>
  <div class="switch">
    
      <a href="/abs/0803.0476?context=cond-mat">cond-mat</a>
      
    <br/>
    
      <span class="subclass"><a href="/abs/0803.0476?context=cond-mat.stat-mech">cond-mat.stat-mech</a></span>
      
    <br/>
    
      <a href="/abs/0803.0476?context=cs">cs</a>
      
    <br/>
    
      <span class="subclass"><a href="/abs/0803.0476?context=cs.CY">cs.CY</a></span>
      
    <br/>
    
      <span class="subclass"><a href="/abs/0803.0476?context=cs.DS">cs.DS</a></span>
      
    <br/>
    
      <a href="/abs/0803.0476?context=physics">physics</a>
      
    <br/>
    
  </div>
  
    </div>

    <div class="extra-ref-cite">
      <h3>References &amp; Citations</h3>
      <ul>
        
        <li><a href="https://ui.adsabs.harvard.edu/#abs/arXiv:0803.0476">NASA ADS</a></li>
      </ul>
    </div>

    
    <div class="extra-general">
        <div class="what-is-this">
            <h3><a href="/tb/0803.0476"> 1 blog link</a></h3> (<a href="https://arxiv.org/help/trackback">what is this?</a>)
        </div>
    </div>
    <div class="bookmarks">
  <div class="what-is-this"><h3>Bookmark</h3> (<a href="https://arxiv.org/help/social_bookmarking">what is this?</a>)</div><a href="/ct?url=http%3A%2F%2Fwww.citeulike.org%2Fposturl%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F0803.0476&amp;v=9d47a368"
     title="Bookmark on CiteULike">
    <img src="//static.arxiv.org/icons/social/citeulike.png"
         alt="CiteULike logo" />
  </a>
  <a href="/ct?url=http%3A%2F%2Fwww.bibsonomy.org%2FBibtexHandler%3FrequTask%3Dupload%26url%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F0803.0476%26description%3DFast+unfolding+of+communities+in+large+networks&amp;v=fcaebb56"
     title="Bookmark on BibSonomy">
    <img src="//static.arxiv.org/icons/social/bibsonomy.png"
         alt="BibSonomy logo"/>
  </a>
  <a href="/ct?url=https%3A%2F%2Fwww.mendeley.com%2Fimport%2F%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F0803.0476&amp;v=bcf747c2"
     title="Bookmark on Mendeley">
    <img src="//static.arxiv.org/icons/social/mendeley.png"
         alt="Mendeley logo"/>
  </a>
  <a href="/ct?url=https%3A%2F%2Freddit.com%2Fsubmit%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F0803.0476%26title%3DFast+unfolding+of+communities+in+large+networks&amp;v=4dcfc6a2"
     title="Bookmark on Reddit">
    <img src="//static.arxiv.org/icons/social/reddit.png"
         alt="Reddit logo"/>
  </a>
  <a href="/ct?url=http%3A%2F%2Fsciencewise.info%2Fbookmarks%2Fadd%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F0803.0476&amp;v=fe2720e8"
     title="Bookmark on ScienceWISE">
    <img src="//static.arxiv.org/icons/social/sciencewise.png"
         alt="ScienceWISE logo"/>
  </a>
</div>
  </div>
  <!--end extra-services-->

  <div class="leftcolumn">
    <div class="subheader">
      <h1>Physics > Physics and Society</h1>
    </div>
    <h1 class="title mathjax"><span class="descriptor">Title:</span>Fast unfolding of communities in large networks</h1>
    <div class="authors"><span class="descriptor">Authors:</span><a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Blondel%2C+V+D">Vincent D. Blondel</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Guillaume%2C+J">Jean-Loup Guillaume</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Lambiotte%2C+R">Renaud Lambiotte</a>, <a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Lefebvre%2C+E">Etienne Lefebvre</a>
    </div>

    <div class="dateline">(Submitted on 4 Mar 2008 (<a href="/abs/0803.0476v1">v1</a>), last revised 25 Jul 2008 (this version, v2))</div>

    
    <blockquote class="abstract mathjax"><span class="descriptor">Abstract:</span>  We propose a simple method to extract the community structure of large
networks. Our method is a heuristic method that is based on modularity
optimization. It is shown to outperform all other known community detection
method in terms of computation time. Moreover, the quality of the communities
detected is very good, as measured by the so-called modularity. This is shown
first by identifying language communities in a Belgian mobile phone network of
2.6 million customers and by analyzing a web graph of 118 million nodes and
more than one billion links. The accuracy of our algorithm is also verified on
ad-hoc modular networks. .
</blockquote>
    <!--CONTEXT-->
    <div class="metatable">
      <table summary="Additional metadata">
        <tr>
          <td class="tablecell label">Comments:</td>
          <td class="tablecell comments mathjax">6 pages, 5 figures, 1 table; new version with new figures in order to clarify our method, where we look more carefully at the role played by the ordering of the nodes and where we compare our method with that of Wakita and Tsurumi</td>
        </tr>
        <tr>
          <td class="tablecell label">Subjects:</td>
          <td class="tablecell subjects"><span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Statistical Mechanics (cond-mat.stat-mech); Computers and Society (cs.CY); Data Structures and Algorithms (cs.DS)</td>
        </tr>
        <tr>
          <td class="tablecell label">Journal&nbsp;reference:</td>
          <td class="tablecell jref">J. Stat. Mech. (2008) P10008</td>
        </tr>
        
        <tr>
          <td class="tablecell label"><abbr title="Digital Object Identifier">DOI</abbr>:</td>
          <td class="tablecell msc_classes"><a href="/ct?url=https%3A%2F%2Fdx.doi.org%2F10.1088%252F1742-5468%252F2008%252F10%252FP10008&v=e1f91257">10.1088/1742-5468/2008/10/P10008</a></td>
        </tr>
        <tr>
          <td class="tablecell label">Cite as:</td>
          <td class="tablecell arxivid"><a href="https://arxiv.org/abs/0803.0476">arXiv:0803.0476</a> [physics.soc-ph]</td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxividv">(or <span class="arxivid">
              <a href="https://arxiv.org/abs/0803.0476v2">arXiv:0803.0476v2</a> [physics.soc-ph]</span> for this version)
          </td>
        </tr>
      </table>
    </div>
    <div class="submission-history">
      <h2>Submission history</h2> From: Renaud Lambiotte [<a href="/show-email/c80a2ec4/0803.0476">view email</a>]
      <br/>
  <b><a href="/abs/0803.0476v1">[v1]</a></b>
  Tue, 4 Mar 2008 15:29:44 UTC (741 KB)<br/><b>[v2]</b>
Fri, 25 Jul 2008 09:52:42 UTC (1,477 KB)<br/></div>
  </div>
  <!--end leftcolumn-->
  <div class="endorsers"><a href="/auth/show-endorsers/0803.0476">Which authors of this paper are endorsers?</a> | <a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="https://arxiv.org/help/mathjax">What is MathJax?</a>)
    <span class="help" style="display: inline-block; font-style: normal; float: right; margin-top: 0; margin-right: 1em;"><a href="https://confluence.cornell.edu/x/MjmLFQ">Browse v0.1 released 2018-10-22</a>&nbsp;&nbsp;<button class="button is-small" id="feedback-button">Feedback?</button></span>
  </div>
  <script type="text/javascript" language="javascript">mathjaxToggle();</script>
  <script src="/bibex/bibex.js?20181010" type="text/javascript" defer></script>
  
</div>

  </div>

  <footer style="clear: both;">
    <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
      <!-- Macro-Column 1 -->
      <div class="column" style="padding: 0;">
        <div class="columns">
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="https://arxiv.org/about">About arXiv</a></li>
              <li><a href="https://arxiv.org/about/people/leadership_team">Leadership Team</a></li>
            </ul>
          </div>
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><span class="icon"><i class="fa fa-envelope"></i></span><a href="https://arxiv.org/help/contact"> Contact Us</a></li>
              <li><span class="icon"><i class="fa fa-twitter"></i></span><a href="https://twitter.com/arxiv"> Follow us on Twitter</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- End Macro-Column 1 -->
      <!-- Macro-Column 2 -->
      <div class="column" style="padding: 0;">
        <div class="columns">
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="https://arxiv.org/help">Help</a></li>
              <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
            </ul>
          </div>
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="https://blogs.cornell.edu/arxiv">Blog</a></li>
              <li><a href="https://arxiv.org/help/subscribe"> Subscribe</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- End Macro-Column 2 -->
    </div>

    <div class="columns" style="border-top: 1px solid #979797; margin: -0.75em;">
      <div class="column">
        <p class="help" style="margin-bottom: 0;">arXiv&#174; is a registered trademark of Cornell University.</p>
      </div>
      <div class="column">
        <p class="help" style="margin-bottom: 0;">If you have a disability and are having trouble accessing information on this website or need materials in an alternate format,
        contact <a href="mailto:web-accessibility@cornell.edu">web-accessibility@cornell.edu</a> for assistance.</p>
      </div>
    </div>
  </footer>

</body>

</html>


#####EOF#####


